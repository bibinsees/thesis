{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"./saved_models/simclr\"  # Change this to your desired checkpoint path\n",
    "\n",
    "# Custom Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.tiff')]\n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer) and the other two layers (augmentations)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "        other_indices = [i for i in range(3) if i != sharpest_layer_index]\n",
    "        augmentation1 = image[other_indices[0]]\n",
    "        augmentation2 = image[other_indices[1]]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        aug1 = torch.tensor(augmentation1, dtype=torch.float32).unsqueeze(0)\n",
    "        aug2 = torch.tensor(augmentation2, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "        aug1 = self.resize_transform(aug1)\n",
    "        aug2 = self.resize_transform(aug2)\n",
    "\n",
    "        return aug1, aug2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "image_dir = \"/home/k54739/.tiff_experiment_unsupervised_data/combined\"\n",
    "dataset = ImageDataset(image_dir=image_dir)\n",
    "batch_size = 16\n",
    "\n",
    "# Function to split dataset with explicit percentage\n",
    "def split_dataset(dataset, val_percentage):\n",
    "    val_size = int(len(dataset) * val_percentage)\n",
    "    train_size = len(dataset) - val_size\n",
    "    return random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Split the dataset with 20% for validation\n",
    "val_percentage = 0.2\n",
    "train_dataset, val_dataset = split_dataset(dataset, val_percentage)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check an example to ensure correct conversion\n",
    "for (aug1, aug2) in train_loader:\n",
    "    print(aug1.shape, aug2.shape, aug1.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | convnet | ResNet | 11.5 M | train\n",
      "-------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "45.994    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19/19 [00:34<00:00,  0.55it/s, v_num=38, train_loss_step=1.43e-6, train_acc_top1_step=1.000, train_acc_top5_step=1.000, train_acc_mean_pos_step=1.000, val_loss=0.00486, val_acc_top1=1.000, val_acc_top5=1.000, val_acc_mean_pos=1.000, train_loss_epoch=0.00335, train_acc_top1_epoch=1.000, train_acc_top5_epoch=1.000, train_acc_mean_pos_epoch=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19/19 [00:34<00:00,  0.55it/s, v_num=38, train_loss_step=1.43e-6, train_acc_top1_step=1.000, train_acc_top5_step=1.000, train_acc_mean_pos_step=1.000, val_loss=0.00486, val_acc_top1=1.000, val_acc_top5=1.000, val_acc_mean_pos=1.000, train_loss_epoch=0.00335, train_acc_top1_epoch=1.000, train_acc_top5_epoch=1.000, train_acc_mean_pos_epoch=1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k54739/miniconda3/envs/thesis/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "# Define the SimCLR model class (same as in the tutorial)\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=50):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "\n",
    "        # Load the pretrained ResNet-18 model\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # Replace the existing convolutional layer with one that accepts a single channel\n",
    "        #in_channels = 1\n",
    "        '''self.convnet.conv1 = nn.Conv2d(in_channels, self.convnet.conv1.out_channels, \n",
    "                                        kernel_size=self.convnet.conv1.kernel_size, \n",
    "                                        stride=self.convnet.conv1.stride, \n",
    "                                        padding=self.convnet.conv1.padding, \n",
    "                                        bias=self.convnet.conv1.bias)'''\n",
    "                                        \n",
    "        weight = self.convnet.conv1.weight.clone()\n",
    "        self.convnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.convnet.conv1.weight.data = weight.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Modify the fully connected layer\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4*hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        aug1, aug2 = batch\n",
    "        imgs = torch.cat((aug1, aug2), dim=0).to(self.device)  # Concatenate along the batch dimension\n",
    "\n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs).to(self.device)\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = nn.functional.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Logging loss\n",
    "        self.log(mode + '_loss', nll, on_epoch=True, prog_bar=True)\n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:, None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)], dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode + '_acc_top1', (sim_argsort == 0).float().mean(), on_epoch=True, prog_bar=True)\n",
    "        self.log(mode + '_acc_top5', (sim_argsort < 5).float().mean(), on_epoch=True, prog_bar=True)\n",
    "        self.log(mode + '_acc_mean_pos', 1 + sim_argsort.float().mean(), on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a callback to plot the training/validation loss and accuracy\n",
    "class PlotLossAccuracyCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc_top1 = []\n",
    "        self.val_acc_top1 = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if 'train_loss' in trainer.callback_metrics:\n",
    "            self.train_losses.append(trainer.callback_metrics['train_loss'].item())\n",
    "        if 'train_acc_top1' in trainer.callback_metrics:\n",
    "            self.train_acc_top1.append(trainer.callback_metrics['train_acc_top1'].item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if 'val_loss' in trainer.callback_metrics:\n",
    "            self.val_losses.append(trainer.callback_metrics['val_loss'].item())\n",
    "        if 'val_acc_top1' in trainer.callback_metrics:\n",
    "            self.val_acc_top1.append(trainer.callback_metrics['val_acc_top1'].item())\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        # Ensure the lists are the same length\n",
    "        min_len = min(len(self.train_losses), len(self.val_losses), len(self.train_acc_top1), len(self.val_acc_top1))\n",
    "        self.train_losses = self.train_losses[:min_len]\n",
    "        self.val_losses = self.val_losses[:min_len]\n",
    "        self.train_acc_top1 = self.train_acc_top1[:min_len]\n",
    "        self.val_acc_top1 = self.val_acc_top1[:min_len]\n",
    "\n",
    "        # Plotting the loss curves\n",
    "        epochs = range(1, min_len + 1)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "\n",
    "        # Plotting the accuracy curves\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_acc_top1, label='Training Accuracy Top-1')\n",
    "        plt.plot(epochs, self.val_acc_top1, label='Validation Accuracy Top-1')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Accuracy Top-1')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_simclr(batch_size, max_epochs=50, **kwargs):\n",
    "    plot_callback = PlotLossAccuracyCallback()\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         log_every_n_steps=10,  # Set this value to a lower number to see logs more frequently\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch'), plot_callback])\n",
    "\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = SimCLR.load_from_checkpoint(pretrained_filename)  # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)  # Load best checkpoint after training\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the SimCLR model\n",
    "simclr_model = train_simclr(batch_size=16, hidden_dim=128, lr=5e-4, temperature=0.07, weight_decay=1e-4, max_epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['untreated', 'single dose', 'drug_screening']\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith('.tiff')]\n",
    "            self.image_files.extend(files)\n",
    "            self.labels.extend([idx] * len(files))\n",
    "        \n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return anchor, label\n",
    "\n",
    "# Directories for labeled data\n",
    "train_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/train\"\n",
    "test_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/test\"\n",
    "/home/k54739/Data_supervised\n",
    "\n",
    "# Create the labeled datasets\n",
    "train_labeled_dataset = LabeledImageDataset(train_image_dir)\n",
    "test_labeled_dataset = LabeledImageDataset(test_image_dir)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "test_loader_labeled = DataLoader(test_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction Using Pretrained SimCLR Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)\n",
    "\n",
    "# Extract features for train and test datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_loader_labeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')\n",
    "\n",
    "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results = train_logreg(batch_size=16, train_feats_data=train_feats_simclr, test_feats_data=test_feats_simclr, \n",
    "                                     model_suffix=\"SimCLR\", feature_dim=128, num_classes=3, lr=5e-4, weight_decay=1e-4, max_epochs=2)\n",
    "\n",
    "print(f\"Train Accuracy: {results['train']}, Test Accuracy: {results['test']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added validation dataset and treat  test dataset and val set seperately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Custom Dataset for Labeled Data\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['untreated', 'single dose', 'drug_screening']\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith('.tiff')]\n",
    "            self.image_files.extend(files)\n",
    "            self.labels.extend([idx] * len(files))\n",
    "        \n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return anchor, label\n",
    "\n",
    "# Directories for labeled data\n",
    "train_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/train\"\n",
    "test_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/test\"\n",
    "\n",
    "# Create the labeled datasets\n",
    "labeled_dataset = LabeledImageDataset(train_image_dir)\n",
    "\n",
    "# Split the labeled dataset into train and validation sets\n",
    "val_percentage = 0.2\n",
    "train_labeled_dataset, val_labeled_dataset = split_dataset(labeled_dataset, val_percentage)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "val_loader_labeled = DataLoader(val_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "test_loader_labeled = DataLoader(LabeledImageDataset(test_image_dir), batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "# Feature Extraction Using Pretrained SimCLR Model\n",
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)\n",
    "\n",
    "# Extract features for train, validation, and test datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "val_feats_simclr = prepare_data_features(simclr_model, val_loader_labeled)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_loader_labeled)\n",
    "\n",
    "# Define and Train the Logistic Regression Model\n",
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')\n",
    "\n",
    "def train_logreg(batch_size, train_feats_data, val_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"val\": val_result, \"test\": test_result}\n",
    "    return model, result\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results = train_logreg(batch_size=batch_size, train_feats_data=train_feats_simclr, \n",
    "                                     val_feats_data=val_feats_simclr, test_feats_data=test_feats_simclr, \n",
    "                                     model_suffix='simclr', feature_dim=train_feats_simclr.tensors[0].shape[1], \n",
    "                                     num_classes=3, lr=1e-4, weight_decay=1e-4, max_epochs=100)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "considered test set as val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['untreated', 'single dose', 'drug_screening']\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith('.tiff')]\n",
    "            self.image_files.extend(files)\n",
    "            self.labels.extend([idx] * len(files))\n",
    "        \n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return anchor, label\n",
    "\n",
    "# Directories for labeled data\n",
    "train_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/train\"\n",
    "val_image_dir = \"/home/k54739/.tiff_experiment_supervised_data/test\"  # Renamed test directory to val\n",
    "\n",
    "# Create the labeled datasets\n",
    "train_labeled_dataset = LabeledImageDataset(train_image_dir)\n",
    "val_labeled_dataset = LabeledImageDataset(val_image_dir)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "val_loader_labeled = DataLoader(val_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)\n",
    "\n",
    "# Extract features for train and val datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "val_feats_simclr = prepare_data_features(simclr_model, val_loader_labeled)\n",
    "\n",
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(mode + '_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "def train_logreg(batch_size, train_feats_data, val_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    csv_logger = CSVLogger(save_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"))\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         logger=csv_logger)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=False, pin_memory=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                            drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    result = {\"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result, csv_logger\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results, csv_logger = train_logreg(batch_size=16, train_feats_data=train_feats_simclr, val_feats_data=val_feats_simclr, \n",
    "                                                 model_suffix=\"SimCLR\", feature_dim=128, num_classes=3, lr=5e-4, weight_decay=1e-4, max_epochs=2)\n",
    "\n",
    "print(f\"Validation Accuracy: {results['val']}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "metrics = csv_logger.metrics\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics['epoch'], metrics['train_loss'], label='Training Loss')\n",
    "plt.plot(metrics['epoch'], metrics['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics['epoch'], metrics['train_acc'], label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train split corrected according to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return anchor, label\n",
    "\n",
    "def load_and_split_data(root_dir, test_size=0.2):\n",
    "    classes = ['untreated', 'single_dose', 'drug_screened']\n",
    "    image_files = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith('.tiff')]\n",
    "        image_files.extend(files)\n",
    "        labels.extend([idx] * len(files))\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "        image_files, labels, test_size=test_size, stratify=labels, random_state=42)\n",
    "\n",
    "    return train_files, test_files, train_labels, test_labels\n",
    "\n",
    "# Directories for labeled data\n",
    "image_dir = \"/home/k54739/Data_supervised\"\n",
    "\n",
    "# Load and split the data\n",
    "train_files, test_files, train_labels, test_labels = load_and_split_data(image_dir, test_size=0.2)\n",
    "\n",
    "# Create the labeled datasets\n",
    "train_labeled_dataset = LabeledImageDataset(train_files, train_labels)\n",
    "test_labeled_dataset = LabeledImageDataset(test_files, test_labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "test_loader_labeled = DataLoader(test_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256]) torch.Size([16]) torch.float32\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for anchor,label in train_loader_labeled:\n",
    "    print(anchor.shape, label.shape, anchor.dtype)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        \n",
    "        print(f\"Batch features shape: {batch_feats.shape}\")\n",
    "        print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    print(f\"Features shape after concatenation: {feats.shape}\")\n",
    "    print(f\"Labels shape after concatenation: {labels.shape}\")\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.30s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features for train and test datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_loader_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying original code inbeteen variation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Mapping from representation h to classes\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params | Mode \n",
      "-----------------------------------------\n",
      "0 | model | Linear | 387    | train\n",
      "-----------------------------------------\n",
      "387       Trainable params\n",
      "0         Non-trainable params\n",
      "387       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k54739/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (12x512 and 128x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the logistic regression model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m logreg_model, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_logreg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_feats_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_feats_simclr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_feats_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_feats_simclr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmodel_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSimCLR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m, in \u001b[0;36mtrain_logreg\u001b[0;34m(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m     pl\u001b[38;5;241m.\u001b[39mseed_everything(\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# To be reproducible\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     model \u001b[38;5;241m=\u001b[39m LogisticRegression\u001b[38;5;241m.\u001b[39mload_from_checkpoint(trainer\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Test best model on train and validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 32\u001b[0m, in \u001b[0;36mLogisticRegression.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mLogisticRegression._calculate_loss\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     19\u001b[0m     feats, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m---> 20\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(preds, labels)\n\u001b[1;32m     22\u001b[0m     acc \u001b[38;5;241m=\u001b[39m (preds\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x512 and 128x3)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results = train_logreg(batch_size=16, train_feats_data=train_feats_simclr, test_feats_data=test_feats_simclr, \n",
    "                                     model_suffix=\"SimCLR\", feature_dim=128, num_classes=3, lr=5e-4, weight_decay=1e-4, max_epochs=2)\n",
    "\n",
    "print(f\"Train Accuracy: {results['train']}, Test Accuracy: {results['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variation code for val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(mode + '_acc', acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "def train_logreg(batch_size, train_feats_data, val_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    csv_logger = CSVLogger(save_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"))\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         logger=csv_logger)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=False, pin_memory=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                            drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    result = {\"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result, csv_logger\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results, csv_logger = train_logreg(batch_size=16, train_feats_data=train_feats_simclr, val_feats_data=test_feats_simclr, \n",
    "                                                 model_suffix=\"SimCLR\", feature_dim=128, num_classes=3, lr=5e-4, weight_decay=1e-4, max_epochs=2)\n",
    "\n",
    "print(f\"Validation Accuracy: {results['val']}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "metrics = csv_logger.metrics\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics['epoch'], metrics['train_loss'], label='Training Loss')\n",
    "plt.plot(metrics['epoch'], metrics['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metrics['epoch'], metrics['train_acc'], label='Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | convnet | ResNet | 11.5 M | train\n",
      "-------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "45.994    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7feb37cfc900>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/k54739/miniconda3/envs/thesis/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/19 [00:00<?, ?it/s]                            "
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/k54739'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 231\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Train the SimCLR model\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m simclr_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_simclr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m#LOGISTIC REGRESSION\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLabeledImageDataset\u001b[39;00m(Dataset):\n",
      "Cell \u001b[0;32mIn[33], line 226\u001b[0m, in \u001b[0;36mtrain_simclr\u001b[0;34m(batch_size, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     model \u001b[38;5;241m=\u001b[39m SimCLR(max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    225\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, train_loader, val_loader)\n\u001b[0;32m--> 226\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSimCLR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load best checkpoint after training\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1586\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \n\u001b[1;32m   1585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1586\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m map_location \u001b[38;5;241m=\u001b[39m map_location \u001b[38;5;129;01mor\u001b[39;00m _default_map_location\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     67\u001b[0m     checkpoint, checkpoint_path\u001b[38;5;241m=\u001b[39m(checkpoint_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(checkpoint_path, (\u001b[38;5;28mstr\u001b[39m, Path)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/lightning_fabric/utilities/cloud_io.py:56\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mstr\u001b[39m(path_or_url),\n\u001b[1;32m     53\u001b[0m         map_location\u001b[38;5;241m=\u001b[39mmap_location,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m fs \u001b[38;5;241m=\u001b[39m get_filesystem(path_or_url)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mmap_location)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/spec.py:1303\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1303\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/implementations/local.py:191\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/implementations/local.py:355\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[0;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.12/site-packages/fsspec/implementations/local.py:360\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m--> 360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[1;32m    362\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/k54739'"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"./saved_models/simclr\"  # Change this to your desired checkpoint path\n",
    "\n",
    "# Custom Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.tiff')]\n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer) and the other two layers (augmentations)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "        other_indices = [i for i in range(3) if i != sharpest_layer_index]\n",
    "        augmentation1 = image[other_indices[0]]\n",
    "        augmentation2 = image[other_indices[1]]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        aug1 = torch.tensor(augmentation1, dtype=torch.float32).unsqueeze(0)\n",
    "        aug2 = torch.tensor(augmentation2, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "        aug1 = self.resize_transform(aug1)\n",
    "        aug2 = self.resize_transform(aug2)\n",
    "\n",
    "        return aug1, aug2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create the dataset\n",
    "image_dir = \"/home/k54739/.tiff_experiment_unsupervised_data/combined\"\n",
    "dataset = ImageDataset(image_dir=image_dir)\n",
    "batch_size = 16\n",
    "\n",
    "# Function to split dataset with explicit percentage\n",
    "def split_dataset(dataset, val_percentage):\n",
    "    val_size = int(len(dataset) * val_percentage)\n",
    "    train_size = len(dataset) - val_size\n",
    "    return random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Split the dataset with 20% for validation\n",
    "val_percentage = 0.2\n",
    "train_dataset, val_dataset = split_dataset(dataset, val_percentage)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "\n",
    "# Define the SimCLR model class (same as in the tutorial)\n",
    "class SimCLR(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=50):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "\n",
    "        # Load the pretrained ResNet-18 model\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # Replace the existing convolutional layer with one that accepts a single channel\n",
    "        #in_channels = 1\n",
    "        '''self.convnet.conv1 = nn.Conv2d(in_channels, self.convnet.conv1.out_channels, \n",
    "                                        kernel_size=self.convnet.conv1.kernel_size, \n",
    "                                        stride=self.convnet.conv1.stride, \n",
    "                                        padding=self.convnet.conv1.padding, \n",
    "                                        bias=self.convnet.conv1.bias)'''\n",
    "                                        \n",
    "        weight = self.convnet.conv1.weight.clone()\n",
    "        self.convnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.convnet.conv1.weight.data = weight.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Modify the fully connected layer\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4*hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs, eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        aug1, aug2 = batch\n",
    "        imgs = torch.cat((aug1, aug2), dim=0).to(self.device)  # Concatenate along the batch dimension\n",
    "\n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs).to(self.device)\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = nn.functional.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Logging loss\n",
    "        self.log(mode + '_loss', nll, on_epoch=True, prog_bar=True)\n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:, None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)], dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode + '_acc_top1', (sim_argsort == 0).float().mean(), on_epoch=True, prog_bar=True)\n",
    "        self.log(mode + '_acc_top5', (sim_argsort < 5).float().mean(), on_epoch=True, prog_bar=True)\n",
    "        self.log(mode + '_acc_mean_pos', 1 + sim_argsort.float().mean(), on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a callback to plot the training/validation loss and accuracy\n",
    "class PlotLossAccuracyCallback(pl.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc_top1 = []\n",
    "        self.val_acc_top1 = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if 'train_loss' in trainer.callback_metrics:\n",
    "            self.train_losses.append(trainer.callback_metrics['train_loss'].item())\n",
    "        if 'train_acc_top1' in trainer.callback_metrics:\n",
    "            self.train_acc_top1.append(trainer.callback_metrics['train_acc_top1'].item())\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        if 'val_loss' in trainer.callback_metrics:\n",
    "            self.val_losses.append(trainer.callback_metrics['val_loss'].item())\n",
    "        if 'val_acc_top1' in trainer.callback_metrics:\n",
    "            self.val_acc_top1.append(trainer.callback_metrics['val_acc_top1'].item())\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        # Ensure the lists are the same length\n",
    "        min_len = min(len(self.train_losses), len(self.val_losses), len(self.train_acc_top1), len(self.val_acc_top1))\n",
    "        self.train_losses = self.train_losses[:min_len]\n",
    "        self.val_losses = self.val_losses[:min_len]\n",
    "        self.train_acc_top1 = self.train_acc_top1[:min_len]\n",
    "        self.val_acc_top1 = self.val_acc_top1[:min_len]\n",
    "\n",
    "        # Plotting the loss curves\n",
    "        epochs = range(1, min_len + 1)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, label='Training Loss')\n",
    "        plt.plot(epochs, self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "\n",
    "        # Plotting the accuracy curves\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_acc_top1, label='Training Accuracy Top-1')\n",
    "        plt.plot(epochs, self.val_acc_top1, label='Validation Accuracy Top-1')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Accuracy Top-1')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_simclr(batch_size, max_epochs=50, **kwargs):\n",
    "    plot_callback = PlotLossAccuracyCallback()\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         log_every_n_steps=10,  # Set this value to a lower number to see logs more frequently\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch'), plot_callback])\n",
    "\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = SimCLR.load_from_checkpoint(pretrained_filename)  # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)  # Load best checkpoint after training\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the SimCLR model\n",
    "simclr_model = train_simclr(batch_size=16, hidden_dim=128, lr=5e-4, temperature=0.07, weight_decay=1e-4, max_epochs=5)\n",
    "\n",
    "\n",
    "\n",
    "#LOGISTIC REGRESSION\n",
    "\n",
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((256, 256))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        anchor = self.resize_transform(anchor)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return anchor, label\n",
    "\n",
    "def load_and_split_data(root_dir, test_size=0.2):\n",
    "    classes = ['untreated', 'single_dose', 'drug_screened']\n",
    "    image_files = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith('.tiff')]\n",
    "        image_files.extend(files)\n",
    "        labels.extend([idx] * len(files))\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "        image_files, labels, test_size=test_size, stratify=labels, random_state=42)\n",
    "\n",
    "    return train_files, test_files, train_labels, test_labels\n",
    "\n",
    "# Directories for labeled data\n",
    "image_dir = \"/home/k54739/Data_supervised\"\n",
    "\n",
    "# Load and split the data\n",
    "train_files, test_files, train_labels, test_labels = load_and_split_data(image_dir, test_size=0.2)\n",
    "\n",
    "# Create the labeled datasets\n",
    "train_labeled_dataset = LabeledImageDataset(train_files, train_labels)\n",
    "test_labeled_dataset = LabeledImageDataset(test_files, test_labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "test_loader_labeled = DataLoader(test_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=os.cpu_count())\n",
    "\n",
    "for anchor,label in train_loader_labeled:\n",
    "    print(anchor.shape, label.shape, anchor.dtype)\n",
    "    break\n",
    "\n",
    "#got output for above code:torch.Size([16, 1, 256, 256]) torch.Size([16]) torch.float32 \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)\n",
    "\n",
    "\n",
    "# Extract features for train and test datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_loader_labeled)\n",
    "\n",
    "\n",
    "class LogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Mapping from representation h to classes\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs * 0.6),\n",
    "                                                                  int(self.hparams.max_epochs * 0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = nn.functional.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')\n",
    "        \n",
    "        \n",
    "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=True,\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, pin_memory=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducible\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Train the logistic regression model\n",
    "logreg_model, results = train_logreg(batch_size=16, train_feats_data=train_feats_simclr, test_feats_data=test_feats_simclr, \n",
    "                                     model_suffix=\"SimCLR\", feature_dim=128, num_classes=3, lr=5e-4, weight_decay=1e-4, max_epochs=2)\n",
    "\n",
    "print(f\"Train Accuracy: {results['train']}, Test Accuracy: {results['test']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
