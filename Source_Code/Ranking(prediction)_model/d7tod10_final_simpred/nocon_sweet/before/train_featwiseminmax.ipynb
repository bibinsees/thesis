{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Day 7 Features Shape: torch.Size([2860, 512])\n",
      "Loaded Day 10 Features Shape: torch.Size([2860, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_21168\\1410609442.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_cond7 = torch.load('condall7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_21168\\1410609442.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_cond10 = torch.load('condall10.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the saved day 7 features\n",
    "combined_cond7 = torch.load('condall7.pt')\n",
    "\n",
    "# Load the saved day 10 features\n",
    "combined_cond10 = torch.load('condall10.pt')\n",
    "# Verify the shapes\n",
    "print(f\"Loaded Day 7 Features Shape: {combined_cond7.shape}\")\n",
    "print(f\"Loaded Day 10 Features Shape: {combined_cond10.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Day 7 Features Shape: torch.Size([646, 512])\n",
      "Loaded Day 10 Features Shape: torch.Size([646, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\1356550951.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_ex7 = torch.load('combined_ex7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\1356550951.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_ex10 = torch.load('combined_ex10.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the saved day 7 features\n",
    "combined_ex7 = torch.load('combined_ex7.pt')\n",
    "\n",
    "# Load the saved day 10 features\n",
    "combined_ex10 = torch.load('combined_ex10.pt')\n",
    "# Verify the shapes\n",
    "print(f\"Loaded Day 7 Features Shape: {combined_ex7.shape}\")\n",
    "print(f\"Loaded Day 10 Features Shape: {combined_ex10.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Day 7 Features Shape: torch.Size([650, 512])\n",
      "Loaded Day 10 Features Shape: torch.Size([650, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\3329391658.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_cond7 = torch.load('combined_cond7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\3329391658.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_cond10 = torch.load('combined_cond10.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the saved day 7 features\n",
    "combined_cond7 = torch.load('combined_cond7.pt')\n",
    "\n",
    "# Load the saved day 10 features\n",
    "combined_cond10 = torch.load('combined_cond10.pt')\n",
    "# Verify the shapes\n",
    "print(f\"Loaded Day 7 Features Shape: {combined_cond7.shape}\")\n",
    "print(f\"Loaded Day 10 Features Shape: {combined_cond10.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Day 7 Features Shape: torch.Size([638, 512])\n",
      "Loaded Day 10 Features Shape: torch.Size([638, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\1607657786.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_sd7 = torch.load('combined_sd7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_6704\\1607657786.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined_sd10 = torch.load('combined_sd10.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load the saved day 7 features\n",
    "combined_sd7 = torch.load('combined_sd7.pt')\n",
    "\n",
    "# Load the saved day 10 features\n",
    "combined_sd10 = torch.load('combined_sd10.pt')\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Loaded Day 7 Features Shape: {combined_sd7.shape}\")\n",
    "print(f\"Loaded Day 10 Features Shape: {combined_sd10.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day7_feats = combined_cond7 \n",
    "train_day10_feats = combined_cond10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day7_feats = combined_sd7 \n",
    "train_day10_feats = combined_sd10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day7_feats = combined_ex7 \n",
    "train_day10_feats = combined_ex10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day7_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_day7_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_day7_feats[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (X): Mean=1.0100, Std=1.2786\n",
      "Target (Y): Mean=0.8409, Std=1.1767\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms for input (X) and target (Y)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Flatten tensors to 1D for easier visualization\n",
    "train_day7_feats_flat = train_day7_feats.flatten().cpu().numpy()\n",
    "train_day10_feats_flat = train_day10_feats.flatten().cpu().numpy()\n",
    "\n",
    "# Input (X) histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_day7_feats_flat, bins=50, color='blue', alpha=0.7, label='Input (X)')\n",
    "plt.title('Distribution of Input (X)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Target (Y) histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_day10_feats_flat, bins=50, color='green', alpha=0.7, label='Target (Y)')\n",
    "plt.title('Distribution of Target (Y)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Input (X): Mean={train_day7_feats.mean():.4f}, Std={train_day7_feats.std():.4f}\")\n",
    "print(f\"Target (Y): Mean={train_day10_feats.mean():.4f}, Std={train_day10_feats.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch tensor to NumPy array\n",
    "train_day7_feats_numpy = train_day7_feats.numpy()\n",
    "train_day10_feats_numpy = train_day10_feats.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2860, 512)\n",
      "(512,)\n",
      "(512,)\n",
      "(2860, 512)\n",
      "(512,)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize separate MinMaxScaler for day7 and day10\n",
    "scaler_day7 = MinMaxScaler()\n",
    "scaler_day10 = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the train_day7_feats\n",
    "train_day7_feats_minmaxed = scaler_day7.fit_transform(train_day7_feats_numpy)\n",
    "print(train_day7_feats_minmaxed.shape)\n",
    "# Get min and max values for day7\n",
    "min_values_day7 = scaler_day7.data_min_\n",
    "max_values_day7 = scaler_day7.data_max_\n",
    "\n",
    "print(min_values_day7.shape)\n",
    "print(max_values_day7.shape)\n",
    "#print(\"Min values for day7:\", min_values_day7)\n",
    "#print(\"Max values for day7:\", max_values_day7)\n",
    "\n",
    "# Fit and transform the train_day10_feats (target)\n",
    "train_day10_feats_minmaxed = scaler_day10.fit_transform(train_day10_feats_numpy)\n",
    "print(train_day10_feats_minmaxed.shape)\n",
    "# Get min and max values for day10\n",
    "min_values_day10 = scaler_day10.data_min_\n",
    "max_values_day10 = scaler_day10.data_max_\n",
    "\n",
    "print(min_values_day10.shape)\n",
    "print(max_values_day10.shape)\n",
    "#print(\"Min values for day10:\", min_values_day10)\n",
    "#print(\"Max values for day10:\", max_values_day10)\n",
    "\n",
    "# Convert the scaled data back to PyTorch tensors (optional)\n",
    "train_day7_feats_minmaxed = torch.tensor(train_day7_feats_minmaxed)\n",
    "train_day10_feats_minmaxed = torch.tensor(train_day10_feats_minmaxed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scalers to disk\n",
    "joblib.dump(scaler_day7, 'ex7.pkl')\n",
    "joblib.dump(scaler_day10, 'ex10.pkl')\n",
    "\n",
    "print(\"Scalers saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scalers to disk\n",
    "joblib.dump(scaler_day7, 'sd7.pkl')\n",
    "joblib.dump(scaler_day10, 'sd10.pkl')\n",
    "\n",
    "print(\"Scalers saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers saved!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scalers to disk\n",
    "joblib.dump(scaler_day7, 'condall7.pkl')\n",
    "joblib.dump(scaler_day10, 'condall10.pkl')\n",
    "\n",
    "print(\"Scalers saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_day7_feats_minmaxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Flatten the tensors for easier visualization\n",
    "train_day7_feats_minmaxed_flat = train_day7_feats_minmaxed.flatten().cpu().numpy()\n",
    "train_day10_feats_minmaxed_flat = train_day10_feats_minmaxed.flatten().cpu().numpy()\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Input (X) histogram after Min-Max scaling\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_day7_feats_minmaxed_flat, bins=50, color='blue', alpha=0.7, label='Min-Max Scaled Input (X)')\n",
    "plt.title('Min-Max Scaled Distribution of Input (X)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Target (Y) histogram after Min-Max scaling\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(train_day10_feats_minmaxed_flat, bins=50, color='green', alpha=0.7, label='Min-Max Scaled Target (Y)')\n",
    "plt.title('Min-Max Scaled Distribution of Target (Y)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=20, output_size=20):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.BatchNorm1d(4),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z\n",
    "\n",
    "# Example usage\n",
    "model = FeaturePredictor()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    # Corrected indentation for forward method\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super(FeaturePredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeaturePredictor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "optimizer_class = torch.optim.Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after the last validation loss improvement.\n",
    "            delta (float): Minimum change in the validation loss to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.no_improvement_epochs = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.no_improvement_epochs = 0\n",
    "        else:\n",
    "            self.no_improvement_epochs += 1\n",
    "            if self.no_improvement_epochs >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "def cross_validate_with_early_stopping(\n",
    "    model_class, dataset, criterion, optimizer_class, num_epochs=50, n_splits=10, patience=10, device='cuda'\n",
    "):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    histories = []\n",
    "    best_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "\n",
    "        # Create subsets for this fold\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Initialize the model, optimizer, and early stopping for this fold\n",
    "        model = model_class().to(device)\n",
    "        optimizer = optimizer_class(model.parameters(), lr=1e-4)\n",
    "        early_stopping = EarlyStopping(patience=patience)\n",
    "\n",
    "        # Training and validation loop\n",
    "        history = {'train_loss': [], 'val_loss': []}\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch_inputs, batch_targets in train_loader:\n",
    "                batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch_inputs, batch_targets in val_loader:\n",
    "                    batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
    "                    outputs = model(batch_inputs)\n",
    "                    loss = criterion(outputs, batch_targets)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            history['val_loss'].append(val_loss)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Fold {fold + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Check early stopping\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch + 1} for fold {fold + 1}\")\n",
    "                break\n",
    "\n",
    "        # Save the best model for this fold\n",
    "        best_models.append(model.state_dict())\n",
    "        histories.append(history)\n",
    "\n",
    "    return histories, best_models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch [1/1000] Fold 1, Train Loss: 0.1445, Val Loss: 0.0733\n",
      "Epoch [2/1000] Fold 1, Train Loss: 0.0764, Val Loss: 0.0463\n",
      "Epoch [3/1000] Fold 1, Train Loss: 0.0522, Val Loss: 0.0360\n",
      "Epoch [4/1000] Fold 1, Train Loss: 0.0412, Val Loss: 0.0316\n",
      "Epoch [5/1000] Fold 1, Train Loss: 0.0362, Val Loss: 0.0291\n",
      "Epoch [6/1000] Fold 1, Train Loss: 0.0333, Val Loss: 0.0278\n",
      "Epoch [7/1000] Fold 1, Train Loss: 0.0314, Val Loss: 0.0268\n",
      "Epoch [8/1000] Fold 1, Train Loss: 0.0302, Val Loss: 0.0262\n",
      "Epoch [9/1000] Fold 1, Train Loss: 0.0297, Val Loss: 0.0257\n",
      "Epoch [10/1000] Fold 1, Train Loss: 0.0285, Val Loss: 0.0251\n",
      "Epoch [11/1000] Fold 1, Train Loss: 0.0283, Val Loss: 0.0247\n",
      "Epoch [12/1000] Fold 1, Train Loss: 0.0277, Val Loss: 0.0243\n",
      "Epoch [13/1000] Fold 1, Train Loss: 0.0268, Val Loss: 0.0238\n",
      "Epoch [14/1000] Fold 1, Train Loss: 0.0263, Val Loss: 0.0233\n",
      "Epoch [15/1000] Fold 1, Train Loss: 0.0265, Val Loss: 0.0231\n",
      "Epoch [16/1000] Fold 1, Train Loss: 0.0258, Val Loss: 0.0231\n",
      "Epoch [17/1000] Fold 1, Train Loss: 0.0255, Val Loss: 0.0224\n",
      "Epoch [18/1000] Fold 1, Train Loss: 0.0252, Val Loss: 0.0222\n",
      "Epoch [19/1000] Fold 1, Train Loss: 0.0251, Val Loss: 0.0217\n",
      "Epoch [20/1000] Fold 1, Train Loss: 0.0247, Val Loss: 0.0215\n",
      "Epoch [21/1000] Fold 1, Train Loss: 0.0243, Val Loss: 0.0214\n",
      "Epoch [22/1000] Fold 1, Train Loss: 0.0244, Val Loss: 0.0210\n",
      "Epoch [23/1000] Fold 1, Train Loss: 0.0238, Val Loss: 0.0206\n",
      "Epoch [24/1000] Fold 1, Train Loss: 0.0238, Val Loss: 0.0207\n",
      "Epoch [25/1000] Fold 1, Train Loss: 0.0237, Val Loss: 0.0204\n",
      "Epoch [26/1000] Fold 1, Train Loss: 0.0236, Val Loss: 0.0202\n",
      "Epoch [27/1000] Fold 1, Train Loss: 0.0231, Val Loss: 0.0201\n",
      "Epoch [28/1000] Fold 1, Train Loss: 0.0230, Val Loss: 0.0197\n",
      "Epoch [29/1000] Fold 1, Train Loss: 0.0225, Val Loss: 0.0196\n",
      "Epoch [30/1000] Fold 1, Train Loss: 0.0227, Val Loss: 0.0193\n",
      "Epoch [31/1000] Fold 1, Train Loss: 0.0226, Val Loss: 0.0190\n",
      "Epoch [32/1000] Fold 1, Train Loss: 0.0220, Val Loss: 0.0189\n",
      "Epoch [33/1000] Fold 1, Train Loss: 0.0218, Val Loss: 0.0187\n",
      "Epoch [34/1000] Fold 1, Train Loss: 0.0217, Val Loss: 0.0185\n",
      "Epoch [35/1000] Fold 1, Train Loss: 0.0214, Val Loss: 0.0183\n",
      "Epoch [36/1000] Fold 1, Train Loss: 0.0215, Val Loss: 0.0183\n",
      "Epoch [37/1000] Fold 1, Train Loss: 0.0213, Val Loss: 0.0180\n",
      "Epoch [38/1000] Fold 1, Train Loss: 0.0210, Val Loss: 0.0177\n",
      "Epoch [39/1000] Fold 1, Train Loss: 0.0210, Val Loss: 0.0177\n",
      "Epoch [40/1000] Fold 1, Train Loss: 0.0209, Val Loss: 0.0174\n",
      "Epoch [41/1000] Fold 1, Train Loss: 0.0206, Val Loss: 0.0172\n",
      "Epoch [42/1000] Fold 1, Train Loss: 0.0207, Val Loss: 0.0170\n",
      "Epoch [43/1000] Fold 1, Train Loss: 0.0205, Val Loss: 0.0171\n",
      "Epoch [44/1000] Fold 1, Train Loss: 0.0201, Val Loss: 0.0167\n",
      "Epoch [45/1000] Fold 1, Train Loss: 0.0201, Val Loss: 0.0167\n",
      "Epoch [46/1000] Fold 1, Train Loss: 0.0198, Val Loss: 0.0164\n",
      "Epoch [47/1000] Fold 1, Train Loss: 0.0200, Val Loss: 0.0164\n",
      "Epoch [48/1000] Fold 1, Train Loss: 0.0199, Val Loss: 0.0165\n",
      "Epoch [49/1000] Fold 1, Train Loss: 0.0198, Val Loss: 0.0163\n",
      "Epoch [50/1000] Fold 1, Train Loss: 0.0198, Val Loss: 0.0161\n",
      "Epoch [51/1000] Fold 1, Train Loss: 0.0195, Val Loss: 0.0160\n",
      "Epoch [52/1000] Fold 1, Train Loss: 0.0193, Val Loss: 0.0161\n",
      "Epoch [53/1000] Fold 1, Train Loss: 0.0193, Val Loss: 0.0159\n",
      "Epoch [54/1000] Fold 1, Train Loss: 0.0192, Val Loss: 0.0158\n",
      "Epoch [55/1000] Fold 1, Train Loss: 0.0191, Val Loss: 0.0158\n",
      "Epoch [56/1000] Fold 1, Train Loss: 0.0188, Val Loss: 0.0155\n",
      "Epoch [57/1000] Fold 1, Train Loss: 0.0189, Val Loss: 0.0153\n",
      "Epoch [58/1000] Fold 1, Train Loss: 0.0192, Val Loss: 0.0151\n",
      "Epoch [59/1000] Fold 1, Train Loss: 0.0185, Val Loss: 0.0152\n",
      "Epoch [60/1000] Fold 1, Train Loss: 0.0186, Val Loss: 0.0151\n",
      "Epoch [61/1000] Fold 1, Train Loss: 0.0185, Val Loss: 0.0150\n",
      "Epoch [62/1000] Fold 1, Train Loss: 0.0184, Val Loss: 0.0147\n",
      "Epoch [63/1000] Fold 1, Train Loss: 0.0180, Val Loss: 0.0147\n",
      "Epoch [64/1000] Fold 1, Train Loss: 0.0180, Val Loss: 0.0146\n",
      "Epoch [65/1000] Fold 1, Train Loss: 0.0182, Val Loss: 0.0146\n",
      "Epoch [66/1000] Fold 1, Train Loss: 0.0182, Val Loss: 0.0146\n",
      "Epoch [67/1000] Fold 1, Train Loss: 0.0178, Val Loss: 0.0144\n",
      "Epoch [68/1000] Fold 1, Train Loss: 0.0180, Val Loss: 0.0143\n",
      "Epoch [69/1000] Fold 1, Train Loss: 0.0177, Val Loss: 0.0141\n",
      "Epoch [70/1000] Fold 1, Train Loss: 0.0179, Val Loss: 0.0144\n",
      "Epoch [71/1000] Fold 1, Train Loss: 0.0172, Val Loss: 0.0140\n",
      "Epoch [72/1000] Fold 1, Train Loss: 0.0180, Val Loss: 0.0140\n",
      "Epoch [73/1000] Fold 1, Train Loss: 0.0176, Val Loss: 0.0139\n",
      "Epoch [74/1000] Fold 1, Train Loss: 0.0175, Val Loss: 0.0139\n",
      "Epoch [75/1000] Fold 1, Train Loss: 0.0176, Val Loss: 0.0136\n",
      "Epoch [76/1000] Fold 1, Train Loss: 0.0171, Val Loss: 0.0137\n",
      "Epoch [77/1000] Fold 1, Train Loss: 0.0173, Val Loss: 0.0137\n",
      "Epoch [78/1000] Fold 1, Train Loss: 0.0172, Val Loss: 0.0135\n",
      "Epoch [79/1000] Fold 1, Train Loss: 0.0170, Val Loss: 0.0137\n",
      "Epoch [80/1000] Fold 1, Train Loss: 0.0172, Val Loss: 0.0136\n",
      "Epoch [81/1000] Fold 1, Train Loss: 0.0170, Val Loss: 0.0134\n",
      "Epoch [82/1000] Fold 1, Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Epoch [83/1000] Fold 1, Train Loss: 0.0170, Val Loss: 0.0133\n",
      "Epoch [84/1000] Fold 1, Train Loss: 0.0168, Val Loss: 0.0132\n",
      "Epoch [85/1000] Fold 1, Train Loss: 0.0168, Val Loss: 0.0130\n",
      "Epoch [86/1000] Fold 1, Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Epoch [87/1000] Fold 1, Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Epoch [88/1000] Fold 1, Train Loss: 0.0162, Val Loss: 0.0129\n",
      "Epoch [89/1000] Fold 1, Train Loss: 0.0168, Val Loss: 0.0131\n",
      "Epoch [90/1000] Fold 1, Train Loss: 0.0165, Val Loss: 0.0131\n",
      "Epoch [91/1000] Fold 1, Train Loss: 0.0162, Val Loss: 0.0131\n",
      "Epoch [92/1000] Fold 1, Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Epoch [93/1000] Fold 1, Train Loss: 0.0162, Val Loss: 0.0129\n",
      "Epoch [94/1000] Fold 1, Train Loss: 0.0165, Val Loss: 0.0127\n",
      "Epoch [95/1000] Fold 1, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Epoch [96/1000] Fold 1, Train Loss: 0.0164, Val Loss: 0.0128\n",
      "Epoch [97/1000] Fold 1, Train Loss: 0.0165, Val Loss: 0.0126\n",
      "Epoch [98/1000] Fold 1, Train Loss: 0.0160, Val Loss: 0.0128\n",
      "Epoch [99/1000] Fold 1, Train Loss: 0.0163, Val Loss: 0.0128\n",
      "Epoch [100/1000] Fold 1, Train Loss: 0.0162, Val Loss: 0.0125\n",
      "Epoch [101/1000] Fold 1, Train Loss: 0.0159, Val Loss: 0.0125\n",
      "Epoch [102/1000] Fold 1, Train Loss: 0.0160, Val Loss: 0.0125\n",
      "Epoch [103/1000] Fold 1, Train Loss: 0.0158, Val Loss: 0.0125\n",
      "Epoch [104/1000] Fold 1, Train Loss: 0.0158, Val Loss: 0.0126\n",
      "Epoch [105/1000] Fold 1, Train Loss: 0.0158, Val Loss: 0.0126\n",
      "Epoch [106/1000] Fold 1, Train Loss: 0.0161, Val Loss: 0.0126\n",
      "Epoch [107/1000] Fold 1, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Epoch [108/1000] Fold 1, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [109/1000] Fold 1, Train Loss: 0.0158, Val Loss: 0.0123\n",
      "Epoch [110/1000] Fold 1, Train Loss: 0.0158, Val Loss: 0.0122\n",
      "Epoch [111/1000] Fold 1, Train Loss: 0.0156, Val Loss: 0.0125\n",
      "Epoch [112/1000] Fold 1, Train Loss: 0.0159, Val Loss: 0.0124\n",
      "Epoch [113/1000] Fold 1, Train Loss: 0.0157, Val Loss: 0.0124\n",
      "Epoch [114/1000] Fold 1, Train Loss: 0.0152, Val Loss: 0.0122\n",
      "Epoch [115/1000] Fold 1, Train Loss: 0.0154, Val Loss: 0.0121\n",
      "Epoch [116/1000] Fold 1, Train Loss: 0.0153, Val Loss: 0.0123\n",
      "Epoch [117/1000] Fold 1, Train Loss: 0.0153, Val Loss: 0.0122\n",
      "Epoch [118/1000] Fold 1, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [119/1000] Fold 1, Train Loss: 0.0154, Val Loss: 0.0122\n",
      "Epoch [120/1000] Fold 1, Train Loss: 0.0153, Val Loss: 0.0119\n",
      "Epoch [121/1000] Fold 1, Train Loss: 0.0154, Val Loss: 0.0119\n",
      "Epoch [122/1000] Fold 1, Train Loss: 0.0153, Val Loss: 0.0119\n",
      "Epoch [123/1000] Fold 1, Train Loss: 0.0152, Val Loss: 0.0120\n",
      "Epoch [124/1000] Fold 1, Train Loss: 0.0150, Val Loss: 0.0121\n",
      "Epoch [125/1000] Fold 1, Train Loss: 0.0152, Val Loss: 0.0119\n",
      "Epoch [126/1000] Fold 1, Train Loss: 0.0151, Val Loss: 0.0120\n",
      "Epoch [127/1000] Fold 1, Train Loss: 0.0153, Val Loss: 0.0117\n",
      "Epoch [128/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0119\n",
      "Epoch [129/1000] Fold 1, Train Loss: 0.0150, Val Loss: 0.0116\n",
      "Epoch [130/1000] Fold 1, Train Loss: 0.0150, Val Loss: 0.0117\n",
      "Epoch [131/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0118\n",
      "Epoch [132/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0118\n",
      "Epoch [133/1000] Fold 1, Train Loss: 0.0150, Val Loss: 0.0117\n",
      "Epoch [134/1000] Fold 1, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [135/1000] Fold 1, Train Loss: 0.0150, Val Loss: 0.0116\n",
      "Epoch [136/1000] Fold 1, Train Loss: 0.0148, Val Loss: 0.0116\n",
      "Epoch [137/1000] Fold 1, Train Loss: 0.0147, Val Loss: 0.0116\n",
      "Epoch [138/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0117\n",
      "Epoch [139/1000] Fold 1, Train Loss: 0.0146, Val Loss: 0.0115\n",
      "Epoch [140/1000] Fold 1, Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [141/1000] Fold 1, Train Loss: 0.0148, Val Loss: 0.0115\n",
      "Epoch [142/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0116\n",
      "Epoch [143/1000] Fold 1, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [144/1000] Fold 1, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [145/1000] Fold 1, Train Loss: 0.0147, Val Loss: 0.0115\n",
      "Epoch [146/1000] Fold 1, Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [147/1000] Fold 1, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [148/1000] Fold 1, Train Loss: 0.0149, Val Loss: 0.0115\n",
      "Epoch [149/1000] Fold 1, Train Loss: 0.0145, Val Loss: 0.0114\n",
      "Epoch [150/1000] Fold 1, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [151/1000] Fold 1, Train Loss: 0.0147, Val Loss: 0.0113\n",
      "Epoch [152/1000] Fold 1, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [153/1000] Fold 1, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [154/1000] Fold 1, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [155/1000] Fold 1, Train Loss: 0.0146, Val Loss: 0.0112\n",
      "Epoch [156/1000] Fold 1, Train Loss: 0.0143, Val Loss: 0.0112\n",
      "Epoch [157/1000] Fold 1, Train Loss: 0.0142, Val Loss: 0.0112\n",
      "Epoch [158/1000] Fold 1, Train Loss: 0.0142, Val Loss: 0.0110\n",
      "Epoch [159/1000] Fold 1, Train Loss: 0.0141, Val Loss: 0.0109\n",
      "Epoch [160/1000] Fold 1, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [161/1000] Fold 1, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [162/1000] Fold 1, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [163/1000] Fold 1, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [164/1000] Fold 1, Train Loss: 0.0143, Val Loss: 0.0112\n",
      "Epoch [165/1000] Fold 1, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [166/1000] Fold 1, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [167/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0111\n",
      "Epoch [168/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [169/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0111\n",
      "Epoch [170/1000] Fold 1, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [171/1000] Fold 1, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [172/1000] Fold 1, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [173/1000] Fold 1, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [174/1000] Fold 1, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [175/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [176/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [177/1000] Fold 1, Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [178/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [179/1000] Fold 1, Train Loss: 0.0138, Val Loss: 0.0109\n",
      "Epoch [180/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [181/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [182/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [183/1000] Fold 1, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [184/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [185/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [186/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [187/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [188/1000] Fold 1, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [189/1000] Fold 1, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [190/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [191/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0110\n",
      "Epoch [192/1000] Fold 1, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [193/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [194/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [195/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0108\n",
      "Epoch [196/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [197/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [198/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0106\n",
      "Epoch [199/1000] Fold 1, Train Loss: 0.0137, Val Loss: 0.0106\n",
      "Epoch [200/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [201/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [202/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [203/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [204/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [205/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [206/1000] Fold 1, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [207/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [208/1000] Fold 1, Train Loss: 0.0135, Val Loss: 0.0105\n",
      "Epoch [209/1000] Fold 1, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [210/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [211/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [212/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [213/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [214/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0105\n",
      "Epoch [215/1000] Fold 1, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [216/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [217/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [218/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [219/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0106\n",
      "Epoch [220/1000] Fold 1, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [221/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [222/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [223/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [224/1000] Fold 1, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [225/1000] Fold 1, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [226/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [227/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [228/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [229/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [230/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [231/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [232/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [233/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [234/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [235/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [236/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [237/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [238/1000] Fold 1, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [239/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [240/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [241/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [242/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [243/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [244/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [245/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [246/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [247/1000] Fold 1, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [248/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [249/1000] Fold 1, Train Loss: 0.0132, Val Loss: 0.0102\n",
      "Epoch [250/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [251/1000] Fold 1, Train Loss: 0.0129, Val Loss: 0.0100\n",
      "Epoch [252/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [253/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [254/1000] Fold 1, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [255/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [256/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [257/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [258/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [259/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [260/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [261/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [262/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [263/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [264/1000] Fold 1, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [265/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [266/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [267/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [268/1000] Fold 1, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [269/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [270/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [271/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [272/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [273/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [274/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [275/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [276/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [277/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [278/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [279/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [280/1000] Fold 1, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [281/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [282/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [283/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [284/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [285/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [286/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [287/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [288/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [289/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [290/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [291/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [292/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [293/1000] Fold 1, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [294/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [295/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [296/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [297/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [298/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [299/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [300/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [301/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [302/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [303/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [304/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [305/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [306/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [307/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [308/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [309/1000] Fold 1, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [310/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [311/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [312/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [313/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [314/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [315/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [316/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [317/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [318/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [319/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [320/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [321/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [322/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [323/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [324/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [325/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [326/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [327/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [328/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [329/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [330/1000] Fold 1, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [331/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [332/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [333/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [334/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [335/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [336/1000] Fold 1, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [337/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [338/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [339/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [340/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [341/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [342/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [343/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [344/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [345/1000] Fold 1, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [346/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [347/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [348/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [349/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [350/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [351/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [352/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [353/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [354/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [355/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [356/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [357/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [358/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [359/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [360/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [361/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [362/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [363/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [364/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [365/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [366/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [367/1000] Fold 1, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [368/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [369/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [370/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [371/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [372/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [373/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [374/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [375/1000] Fold 1, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [376/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [377/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [378/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [379/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [380/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [381/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [382/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [383/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [384/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [385/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [386/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [387/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [388/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [389/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [390/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [391/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [392/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [393/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [394/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [395/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [396/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [397/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [398/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [399/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [400/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [401/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [402/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [403/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [404/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [405/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [406/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [407/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [408/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [409/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [410/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [411/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [412/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [413/1000] Fold 1, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [414/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [415/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [416/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [417/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [418/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [419/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [420/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [421/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [422/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [423/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [424/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [425/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [426/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [427/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [428/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [429/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [430/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [431/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [432/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [433/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [434/1000] Fold 1, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [435/1000] Fold 1, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [436/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [437/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [438/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [439/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [440/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [441/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [442/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [443/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [444/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [445/1000] Fold 1, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [446/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [447/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [448/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [449/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [450/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [451/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [452/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [453/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [454/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [455/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [456/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [457/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [458/1000] Fold 1, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [459/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [460/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [461/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [462/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [463/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [464/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [465/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [466/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [467/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [468/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [469/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [470/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [471/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [472/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [473/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [474/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [475/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [476/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [477/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [478/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [479/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [480/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [481/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [482/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [483/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [484/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [485/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [486/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [487/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [488/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [489/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [490/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [491/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [492/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [493/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [494/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [495/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [496/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [497/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [498/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [499/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [500/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [501/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [502/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [503/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [504/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [505/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [506/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [507/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [508/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [509/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [510/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [511/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [512/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [513/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [514/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [515/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [516/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [517/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [518/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [519/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [520/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [521/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [522/1000] Fold 1, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [523/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [524/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [525/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [526/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [527/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [528/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [529/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [530/1000] Fold 1, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [531/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [532/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [533/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [534/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [535/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [536/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [537/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [538/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [539/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [540/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [541/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [542/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [543/1000] Fold 1, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [544/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [545/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [546/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [547/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [548/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [549/1000] Fold 1, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [550/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [551/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [552/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [553/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [554/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [555/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [556/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [557/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [558/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [559/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [560/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [561/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [562/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [563/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [564/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [565/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [566/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [567/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [568/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [569/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [570/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [571/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [572/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [573/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [574/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [575/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [576/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [577/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [578/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [579/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [580/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [581/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [582/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [583/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [584/1000] Fold 1, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [585/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [586/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [587/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [588/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [589/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [590/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [591/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [592/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [593/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [594/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [595/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [596/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [597/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [598/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [599/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [600/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [601/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [602/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [603/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [604/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [605/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [606/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [607/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [608/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [609/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [610/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [611/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [612/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [613/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [614/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [615/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [616/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [617/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [618/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [619/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [620/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [621/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [622/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [623/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [624/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [625/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [626/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [627/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [628/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [629/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [630/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [631/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [632/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [633/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [634/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [635/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [636/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [637/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [638/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [639/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [640/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [641/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [642/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [643/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [644/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [645/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [646/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [647/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [648/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [649/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [650/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [651/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [652/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [653/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [654/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [655/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [656/1000] Fold 1, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [657/1000] Fold 1, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [658/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [659/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [660/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [661/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [662/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [663/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [664/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [665/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [666/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [667/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [668/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [669/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [670/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [671/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [672/1000] Fold 1, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [673/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [674/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [675/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [676/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [677/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [678/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [679/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [680/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [681/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [682/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [683/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [684/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [685/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [686/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [687/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [688/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [689/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [690/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [691/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [692/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [693/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [694/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [695/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [696/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [697/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [698/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [699/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [700/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [701/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [702/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [703/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [704/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [705/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [706/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [707/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [708/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [709/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [710/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [711/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [712/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [713/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [714/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [715/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [716/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [717/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [718/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [719/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [720/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [721/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [722/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [723/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [724/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [725/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [726/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [727/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [728/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [729/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [730/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [731/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [732/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [733/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [734/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [735/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [736/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [737/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [738/1000] Fold 1, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [739/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [740/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [741/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [742/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [743/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [744/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [745/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [746/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [747/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [748/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [749/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [750/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [751/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [752/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [753/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [754/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [755/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [756/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [757/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [758/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [759/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [760/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [761/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [762/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [763/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [764/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [765/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [766/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [767/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [768/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [769/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [770/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [771/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [772/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [773/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [774/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [775/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [776/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [777/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [778/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [779/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [780/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [781/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [782/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [783/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [784/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [785/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [786/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [787/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [788/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [789/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [790/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [791/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [792/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [793/1000] Fold 1, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [794/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [795/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [796/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [797/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [798/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [799/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [800/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [801/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [802/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [803/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [804/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [805/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [806/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [807/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [808/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [809/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [810/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [811/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [812/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [813/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [814/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [815/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [816/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [817/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [818/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [819/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [820/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [821/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [822/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [823/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [824/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [825/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [826/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [827/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [828/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [829/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [830/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [831/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [832/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [833/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [834/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [835/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [836/1000] Fold 1, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [837/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [838/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [839/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [840/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [841/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [842/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [843/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [844/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [845/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [846/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [847/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [848/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [849/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [850/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [851/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [852/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [853/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [854/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [855/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [856/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [857/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [858/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [859/1000] Fold 1, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [860/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [861/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [862/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [863/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [864/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [865/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [866/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [867/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [868/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [869/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [870/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [871/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [872/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [873/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [874/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [875/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [876/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [877/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [878/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [879/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [880/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [881/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [882/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [883/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [884/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [885/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [886/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [887/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [888/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [889/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [890/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [891/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [892/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [893/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [894/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [895/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [896/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [897/1000] Fold 1, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [898/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [899/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [900/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [901/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [902/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [903/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [904/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [905/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [906/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [907/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [908/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [909/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [910/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [911/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [912/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [913/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [914/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [915/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [916/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [917/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [918/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [919/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [920/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [921/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [922/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [923/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [924/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [925/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [926/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [927/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [928/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [929/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [930/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [931/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [932/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [933/1000] Fold 1, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [934/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [935/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [936/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [937/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [938/1000] Fold 1, Train Loss: 0.0095, Val Loss: 0.0090\n",
      "Epoch [939/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [940/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [941/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [942/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [943/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [944/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [945/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [946/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [947/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [948/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [949/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0089\n",
      "Epoch [950/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [951/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [952/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [953/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [954/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [955/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [956/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [957/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [958/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [959/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [960/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [961/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [962/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [963/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [964/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [965/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [966/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [967/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [968/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [969/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [970/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [971/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [972/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [973/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [974/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [975/1000] Fold 1, Train Loss: 0.0095, Val Loss: 0.0086\n",
      "Epoch [976/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [977/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [978/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [979/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [980/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [981/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [982/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [983/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [984/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [985/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [986/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [987/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [988/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [989/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [990/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [991/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [992/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [993/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [994/1000] Fold 1, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [995/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [996/1000] Fold 1, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [997/1000] Fold 1, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [998/1000] Fold 1, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [999/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [1000/1000] Fold 1, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Fold 2/10\n",
      "Epoch [1/1000] Fold 2, Train Loss: 0.1446, Val Loss: 0.0732\n",
      "Epoch [2/1000] Fold 2, Train Loss: 0.0765, Val Loss: 0.0476\n",
      "Epoch [3/1000] Fold 2, Train Loss: 0.0529, Val Loss: 0.0368\n",
      "Epoch [4/1000] Fold 2, Train Loss: 0.0427, Val Loss: 0.0325\n",
      "Epoch [5/1000] Fold 2, Train Loss: 0.0380, Val Loss: 0.0302\n",
      "Epoch [6/1000] Fold 2, Train Loss: 0.0349, Val Loss: 0.0289\n",
      "Epoch [7/1000] Fold 2, Train Loss: 0.0327, Val Loss: 0.0280\n",
      "Epoch [8/1000] Fold 2, Train Loss: 0.0314, Val Loss: 0.0270\n",
      "Epoch [9/1000] Fold 2, Train Loss: 0.0304, Val Loss: 0.0263\n",
      "Epoch [10/1000] Fold 2, Train Loss: 0.0292, Val Loss: 0.0258\n",
      "Epoch [11/1000] Fold 2, Train Loss: 0.0287, Val Loss: 0.0253\n",
      "Epoch [12/1000] Fold 2, Train Loss: 0.0283, Val Loss: 0.0250\n",
      "Epoch [13/1000] Fold 2, Train Loss: 0.0278, Val Loss: 0.0245\n",
      "Epoch [14/1000] Fold 2, Train Loss: 0.0273, Val Loss: 0.0240\n",
      "Epoch [15/1000] Fold 2, Train Loss: 0.0271, Val Loss: 0.0238\n",
      "Epoch [16/1000] Fold 2, Train Loss: 0.0264, Val Loss: 0.0233\n",
      "Epoch [17/1000] Fold 2, Train Loss: 0.0261, Val Loss: 0.0230\n",
      "Epoch [18/1000] Fold 2, Train Loss: 0.0256, Val Loss: 0.0225\n",
      "Epoch [19/1000] Fold 2, Train Loss: 0.0255, Val Loss: 0.0222\n",
      "Epoch [20/1000] Fold 2, Train Loss: 0.0247, Val Loss: 0.0219\n",
      "Epoch [21/1000] Fold 2, Train Loss: 0.0249, Val Loss: 0.0216\n",
      "Epoch [22/1000] Fold 2, Train Loss: 0.0242, Val Loss: 0.0215\n",
      "Epoch [23/1000] Fold 2, Train Loss: 0.0244, Val Loss: 0.0212\n",
      "Epoch [24/1000] Fold 2, Train Loss: 0.0240, Val Loss: 0.0209\n",
      "Epoch [25/1000] Fold 2, Train Loss: 0.0237, Val Loss: 0.0205\n",
      "Epoch [26/1000] Fold 2, Train Loss: 0.0236, Val Loss: 0.0204\n",
      "Epoch [27/1000] Fold 2, Train Loss: 0.0235, Val Loss: 0.0202\n",
      "Epoch [28/1000] Fold 2, Train Loss: 0.0230, Val Loss: 0.0198\n",
      "Epoch [29/1000] Fold 2, Train Loss: 0.0224, Val Loss: 0.0198\n",
      "Epoch [30/1000] Fold 2, Train Loss: 0.0224, Val Loss: 0.0195\n",
      "Epoch [31/1000] Fold 2, Train Loss: 0.0222, Val Loss: 0.0194\n",
      "Epoch [32/1000] Fold 2, Train Loss: 0.0222, Val Loss: 0.0192\n",
      "Epoch [33/1000] Fold 2, Train Loss: 0.0222, Val Loss: 0.0190\n",
      "Epoch [34/1000] Fold 2, Train Loss: 0.0218, Val Loss: 0.0188\n",
      "Epoch [35/1000] Fold 2, Train Loss: 0.0218, Val Loss: 0.0184\n",
      "Epoch [36/1000] Fold 2, Train Loss: 0.0213, Val Loss: 0.0183\n",
      "Epoch [37/1000] Fold 2, Train Loss: 0.0214, Val Loss: 0.0181\n",
      "Epoch [38/1000] Fold 2, Train Loss: 0.0209, Val Loss: 0.0178\n",
      "Epoch [39/1000] Fold 2, Train Loss: 0.0211, Val Loss: 0.0176\n",
      "Epoch [40/1000] Fold 2, Train Loss: 0.0205, Val Loss: 0.0176\n",
      "Epoch [41/1000] Fold 2, Train Loss: 0.0208, Val Loss: 0.0175\n",
      "Epoch [42/1000] Fold 2, Train Loss: 0.0205, Val Loss: 0.0171\n",
      "Epoch [43/1000] Fold 2, Train Loss: 0.0205, Val Loss: 0.0173\n",
      "Epoch [44/1000] Fold 2, Train Loss: 0.0205, Val Loss: 0.0170\n",
      "Epoch [45/1000] Fold 2, Train Loss: 0.0199, Val Loss: 0.0169\n",
      "Epoch [46/1000] Fold 2, Train Loss: 0.0200, Val Loss: 0.0169\n",
      "Epoch [47/1000] Fold 2, Train Loss: 0.0198, Val Loss: 0.0166\n",
      "Epoch [48/1000] Fold 2, Train Loss: 0.0197, Val Loss: 0.0165\n",
      "Epoch [49/1000] Fold 2, Train Loss: 0.0197, Val Loss: 0.0166\n",
      "Epoch [50/1000] Fold 2, Train Loss: 0.0195, Val Loss: 0.0163\n",
      "Epoch [51/1000] Fold 2, Train Loss: 0.0197, Val Loss: 0.0161\n",
      "Epoch [52/1000] Fold 2, Train Loss: 0.0194, Val Loss: 0.0162\n",
      "Epoch [53/1000] Fold 2, Train Loss: 0.0192, Val Loss: 0.0159\n",
      "Epoch [54/1000] Fold 2, Train Loss: 0.0192, Val Loss: 0.0158\n",
      "Epoch [55/1000] Fold 2, Train Loss: 0.0192, Val Loss: 0.0158\n",
      "Epoch [56/1000] Fold 2, Train Loss: 0.0191, Val Loss: 0.0157\n",
      "Epoch [57/1000] Fold 2, Train Loss: 0.0186, Val Loss: 0.0157\n",
      "Epoch [58/1000] Fold 2, Train Loss: 0.0186, Val Loss: 0.0154\n",
      "Epoch [59/1000] Fold 2, Train Loss: 0.0186, Val Loss: 0.0157\n",
      "Epoch [60/1000] Fold 2, Train Loss: 0.0185, Val Loss: 0.0155\n",
      "Epoch [61/1000] Fold 2, Train Loss: 0.0186, Val Loss: 0.0154\n",
      "Epoch [62/1000] Fold 2, Train Loss: 0.0184, Val Loss: 0.0151\n",
      "Epoch [63/1000] Fold 2, Train Loss: 0.0183, Val Loss: 0.0152\n",
      "Epoch [64/1000] Fold 2, Train Loss: 0.0183, Val Loss: 0.0151\n",
      "Epoch [65/1000] Fold 2, Train Loss: 0.0183, Val Loss: 0.0151\n",
      "Epoch [66/1000] Fold 2, Train Loss: 0.0182, Val Loss: 0.0149\n",
      "Epoch [67/1000] Fold 2, Train Loss: 0.0180, Val Loss: 0.0149\n",
      "Epoch [68/1000] Fold 2, Train Loss: 0.0181, Val Loss: 0.0149\n",
      "Epoch [69/1000] Fold 2, Train Loss: 0.0179, Val Loss: 0.0147\n",
      "Epoch [70/1000] Fold 2, Train Loss: 0.0180, Val Loss: 0.0146\n",
      "Epoch [71/1000] Fold 2, Train Loss: 0.0178, Val Loss: 0.0145\n",
      "Epoch [72/1000] Fold 2, Train Loss: 0.0175, Val Loss: 0.0145\n",
      "Epoch [73/1000] Fold 2, Train Loss: 0.0175, Val Loss: 0.0146\n",
      "Epoch [74/1000] Fold 2, Train Loss: 0.0174, Val Loss: 0.0144\n",
      "Epoch [75/1000] Fold 2, Train Loss: 0.0177, Val Loss: 0.0145\n",
      "Epoch [76/1000] Fold 2, Train Loss: 0.0176, Val Loss: 0.0145\n",
      "Epoch [77/1000] Fold 2, Train Loss: 0.0174, Val Loss: 0.0141\n",
      "Epoch [78/1000] Fold 2, Train Loss: 0.0175, Val Loss: 0.0141\n",
      "Epoch [79/1000] Fold 2, Train Loss: 0.0172, Val Loss: 0.0139\n",
      "Epoch [80/1000] Fold 2, Train Loss: 0.0171, Val Loss: 0.0140\n",
      "Epoch [81/1000] Fold 2, Train Loss: 0.0172, Val Loss: 0.0140\n",
      "Epoch [82/1000] Fold 2, Train Loss: 0.0171, Val Loss: 0.0141\n",
      "Epoch [83/1000] Fold 2, Train Loss: 0.0171, Val Loss: 0.0137\n",
      "Epoch [84/1000] Fold 2, Train Loss: 0.0173, Val Loss: 0.0137\n",
      "Epoch [85/1000] Fold 2, Train Loss: 0.0171, Val Loss: 0.0137\n",
      "Epoch [86/1000] Fold 2, Train Loss: 0.0170, Val Loss: 0.0138\n",
      "Epoch [87/1000] Fold 2, Train Loss: 0.0169, Val Loss: 0.0138\n",
      "Epoch [88/1000] Fold 2, Train Loss: 0.0172, Val Loss: 0.0138\n",
      "Epoch [89/1000] Fold 2, Train Loss: 0.0168, Val Loss: 0.0137\n",
      "Epoch [90/1000] Fold 2, Train Loss: 0.0168, Val Loss: 0.0137\n",
      "Epoch [91/1000] Fold 2, Train Loss: 0.0166, Val Loss: 0.0135\n",
      "Epoch [92/1000] Fold 2, Train Loss: 0.0168, Val Loss: 0.0137\n",
      "Epoch [93/1000] Fold 2, Train Loss: 0.0167, Val Loss: 0.0136\n",
      "Epoch [94/1000] Fold 2, Train Loss: 0.0166, Val Loss: 0.0135\n",
      "Epoch [95/1000] Fold 2, Train Loss: 0.0165, Val Loss: 0.0135\n",
      "Epoch [96/1000] Fold 2, Train Loss: 0.0167, Val Loss: 0.0132\n",
      "Epoch [97/1000] Fold 2, Train Loss: 0.0165, Val Loss: 0.0134\n",
      "Epoch [98/1000] Fold 2, Train Loss: 0.0163, Val Loss: 0.0131\n",
      "Epoch [99/1000] Fold 2, Train Loss: 0.0164, Val Loss: 0.0132\n",
      "Epoch [100/1000] Fold 2, Train Loss: 0.0163, Val Loss: 0.0131\n",
      "Epoch [101/1000] Fold 2, Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Epoch [102/1000] Fold 2, Train Loss: 0.0162, Val Loss: 0.0131\n",
      "Epoch [103/1000] Fold 2, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [104/1000] Fold 2, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [105/1000] Fold 2, Train Loss: 0.0158, Val Loss: 0.0130\n",
      "Epoch [106/1000] Fold 2, Train Loss: 0.0160, Val Loss: 0.0130\n",
      "Epoch [107/1000] Fold 2, Train Loss: 0.0159, Val Loss: 0.0130\n",
      "Epoch [108/1000] Fold 2, Train Loss: 0.0162, Val Loss: 0.0132\n",
      "Epoch [109/1000] Fold 2, Train Loss: 0.0159, Val Loss: 0.0129\n",
      "Epoch [110/1000] Fold 2, Train Loss: 0.0162, Val Loss: 0.0129\n",
      "Epoch [111/1000] Fold 2, Train Loss: 0.0156, Val Loss: 0.0129\n",
      "Epoch [112/1000] Fold 2, Train Loss: 0.0158, Val Loss: 0.0128\n",
      "Epoch [113/1000] Fold 2, Train Loss: 0.0156, Val Loss: 0.0129\n",
      "Epoch [114/1000] Fold 2, Train Loss: 0.0155, Val Loss: 0.0125\n",
      "Epoch [115/1000] Fold 2, Train Loss: 0.0155, Val Loss: 0.0127\n",
      "Epoch [116/1000] Fold 2, Train Loss: 0.0157, Val Loss: 0.0126\n",
      "Epoch [117/1000] Fold 2, Train Loss: 0.0157, Val Loss: 0.0125\n",
      "Epoch [118/1000] Fold 2, Train Loss: 0.0159, Val Loss: 0.0127\n",
      "Epoch [119/1000] Fold 2, Train Loss: 0.0156, Val Loss: 0.0124\n",
      "Epoch [120/1000] Fold 2, Train Loss: 0.0154, Val Loss: 0.0125\n",
      "Epoch [121/1000] Fold 2, Train Loss: 0.0152, Val Loss: 0.0124\n",
      "Epoch [122/1000] Fold 2, Train Loss: 0.0155, Val Loss: 0.0125\n",
      "Epoch [123/1000] Fold 2, Train Loss: 0.0152, Val Loss: 0.0125\n",
      "Epoch [124/1000] Fold 2, Train Loss: 0.0156, Val Loss: 0.0125\n",
      "Epoch [125/1000] Fold 2, Train Loss: 0.0154, Val Loss: 0.0125\n",
      "Epoch [126/1000] Fold 2, Train Loss: 0.0152, Val Loss: 0.0124\n",
      "Epoch [127/1000] Fold 2, Train Loss: 0.0152, Val Loss: 0.0125\n",
      "Epoch [128/1000] Fold 2, Train Loss: 0.0151, Val Loss: 0.0123\n",
      "Epoch [129/1000] Fold 2, Train Loss: 0.0149, Val Loss: 0.0124\n",
      "Epoch [130/1000] Fold 2, Train Loss: 0.0151, Val Loss: 0.0122\n",
      "Epoch [131/1000] Fold 2, Train Loss: 0.0151, Val Loss: 0.0123\n",
      "Epoch [132/1000] Fold 2, Train Loss: 0.0153, Val Loss: 0.0122\n",
      "Epoch [133/1000] Fold 2, Train Loss: 0.0151, Val Loss: 0.0123\n",
      "Epoch [134/1000] Fold 2, Train Loss: 0.0151, Val Loss: 0.0122\n",
      "Epoch [135/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0120\n",
      "Epoch [136/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0120\n",
      "Epoch [137/1000] Fold 2, Train Loss: 0.0152, Val Loss: 0.0120\n",
      "Epoch [138/1000] Fold 2, Train Loss: 0.0149, Val Loss: 0.0120\n",
      "Epoch [139/1000] Fold 2, Train Loss: 0.0149, Val Loss: 0.0120\n",
      "Epoch [140/1000] Fold 2, Train Loss: 0.0150, Val Loss: 0.0120\n",
      "Epoch [141/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0120\n",
      "Epoch [142/1000] Fold 2, Train Loss: 0.0146, Val Loss: 0.0119\n",
      "Epoch [143/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [144/1000] Fold 2, Train Loss: 0.0147, Val Loss: 0.0117\n",
      "Epoch [145/1000] Fold 2, Train Loss: 0.0146, Val Loss: 0.0117\n",
      "Epoch [146/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [147/1000] Fold 2, Train Loss: 0.0144, Val Loss: 0.0117\n",
      "Epoch [148/1000] Fold 2, Train Loss: 0.0146, Val Loss: 0.0118\n",
      "Epoch [149/1000] Fold 2, Train Loss: 0.0148, Val Loss: 0.0116\n",
      "Epoch [150/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0119\n",
      "Epoch [151/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0118\n",
      "Epoch [152/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0116\n",
      "Epoch [153/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0118\n",
      "Epoch [154/1000] Fold 2, Train Loss: 0.0144, Val Loss: 0.0116\n",
      "Epoch [155/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0119\n",
      "Epoch [156/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0119\n",
      "Epoch [157/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [158/1000] Fold 2, Train Loss: 0.0142, Val Loss: 0.0116\n",
      "Epoch [159/1000] Fold 2, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [160/1000] Fold 2, Train Loss: 0.0143, Val Loss: 0.0115\n",
      "Epoch [161/1000] Fold 2, Train Loss: 0.0145, Val Loss: 0.0114\n",
      "Epoch [162/1000] Fold 2, Train Loss: 0.0143, Val Loss: 0.0114\n",
      "Epoch [163/1000] Fold 2, Train Loss: 0.0144, Val Loss: 0.0116\n",
      "Epoch [164/1000] Fold 2, Train Loss: 0.0141, Val Loss: 0.0114\n",
      "Epoch [165/1000] Fold 2, Train Loss: 0.0142, Val Loss: 0.0113\n",
      "Epoch [166/1000] Fold 2, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [167/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0114\n",
      "Epoch [168/1000] Fold 2, Train Loss: 0.0141, Val Loss: 0.0113\n",
      "Epoch [169/1000] Fold 2, Train Loss: 0.0142, Val Loss: 0.0114\n",
      "Epoch [170/1000] Fold 2, Train Loss: 0.0141, Val Loss: 0.0114\n",
      "Epoch [171/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0114\n",
      "Epoch [172/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0113\n",
      "Epoch [173/1000] Fold 2, Train Loss: 0.0142, Val Loss: 0.0114\n",
      "Epoch [174/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [175/1000] Fold 2, Train Loss: 0.0139, Val Loss: 0.0113\n",
      "Epoch [176/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [177/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0111\n",
      "Epoch [178/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0113\n",
      "Epoch [179/1000] Fold 2, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [180/1000] Fold 2, Train Loss: 0.0139, Val Loss: 0.0112\n",
      "Epoch [181/1000] Fold 2, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [182/1000] Fold 2, Train Loss: 0.0141, Val Loss: 0.0112\n",
      "Epoch [183/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [184/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0112\n",
      "Epoch [185/1000] Fold 2, Train Loss: 0.0138, Val Loss: 0.0109\n",
      "Epoch [186/1000] Fold 2, Train Loss: 0.0140, Val Loss: 0.0111\n",
      "Epoch [187/1000] Fold 2, Train Loss: 0.0139, Val Loss: 0.0112\n",
      "Epoch [188/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0111\n",
      "Epoch [189/1000] Fold 2, Train Loss: 0.0138, Val Loss: 0.0109\n",
      "Epoch [190/1000] Fold 2, Train Loss: 0.0138, Val Loss: 0.0110\n",
      "Epoch [191/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0111\n",
      "Epoch [192/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [193/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0110\n",
      "Epoch [194/1000] Fold 2, Train Loss: 0.0138, Val Loss: 0.0110\n",
      "Epoch [195/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [196/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0109\n",
      "Epoch [197/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0110\n",
      "Epoch [198/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [199/1000] Fold 2, Train Loss: 0.0138, Val Loss: 0.0110\n",
      "Epoch [200/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [201/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [202/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [203/1000] Fold 2, Train Loss: 0.0135, Val Loss: 0.0109\n",
      "Epoch [204/1000] Fold 2, Train Loss: 0.0135, Val Loss: 0.0110\n",
      "Epoch [205/1000] Fold 2, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [206/1000] Fold 2, Train Loss: 0.0134, Val Loss: 0.0108\n",
      "Epoch [207/1000] Fold 2, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [208/1000] Fold 2, Train Loss: 0.0134, Val Loss: 0.0109\n",
      "Epoch [209/1000] Fold 2, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [210/1000] Fold 2, Train Loss: 0.0134, Val Loss: 0.0108\n",
      "Epoch [211/1000] Fold 2, Train Loss: 0.0135, Val Loss: 0.0109\n",
      "Epoch [212/1000] Fold 2, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [213/1000] Fold 2, Train Loss: 0.0133, Val Loss: 0.0107\n",
      "Epoch [214/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0108\n",
      "Epoch [215/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0106\n",
      "Epoch [216/1000] Fold 2, Train Loss: 0.0133, Val Loss: 0.0107\n",
      "Epoch [217/1000] Fold 2, Train Loss: 0.0133, Val Loss: 0.0106\n",
      "Epoch [218/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [219/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [220/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [221/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [222/1000] Fold 2, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [223/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [224/1000] Fold 2, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [225/1000] Fold 2, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [226/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0106\n",
      "Epoch [227/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [228/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0109\n",
      "Epoch [229/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [230/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [231/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [232/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [233/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [234/1000] Fold 2, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [235/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [236/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [237/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [238/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [239/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0105\n",
      "Epoch [240/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [241/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0105\n",
      "Epoch [242/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [243/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [244/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [245/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0104\n",
      "Epoch [246/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0104\n",
      "Epoch [247/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [248/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [249/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [250/1000] Fold 2, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [251/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0104\n",
      "Epoch [252/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [253/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [254/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [255/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [256/1000] Fold 2, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [257/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [258/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [259/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [260/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [261/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [262/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [263/1000] Fold 2, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [264/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [265/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [266/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0102\n",
      "Epoch [267/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [268/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0102\n",
      "Epoch [269/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [270/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [271/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [272/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [273/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [274/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [275/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [276/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [277/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [278/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [279/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [280/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [281/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [282/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0102\n",
      "Epoch [283/1000] Fold 2, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [284/1000] Fold 2, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [285/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [286/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [287/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [288/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [289/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [290/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [291/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [292/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [293/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [294/1000] Fold 2, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [295/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [296/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [297/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [298/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [299/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [300/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [301/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [302/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [303/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [304/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [305/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [306/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [307/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [308/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [309/1000] Fold 2, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [310/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [311/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [312/1000] Fold 2, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [313/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [314/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [315/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [316/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [317/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [318/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [319/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [320/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [321/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [322/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [323/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [324/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [325/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [326/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [327/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [328/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [329/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [330/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [331/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [332/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [333/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [334/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [335/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [336/1000] Fold 2, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [337/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [338/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [339/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [340/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [341/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [342/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [343/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [344/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [345/1000] Fold 2, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [346/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [347/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [348/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [349/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [350/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [351/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [352/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [353/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [354/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [355/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [356/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [357/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [358/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [359/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [360/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [361/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [362/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [363/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [364/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [365/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [366/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [367/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [368/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [369/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [370/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [371/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [372/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [373/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [374/1000] Fold 2, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [375/1000] Fold 2, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [376/1000] Fold 2, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [377/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [378/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [379/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [380/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [381/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [382/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [383/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [384/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [385/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [386/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [387/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [388/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [389/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [390/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [391/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [392/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [393/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [394/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [395/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [396/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [397/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [398/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [399/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [400/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [401/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [402/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [403/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [404/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [405/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [406/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [407/1000] Fold 2, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [408/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [409/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [410/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [411/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [412/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [413/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [414/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [415/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [416/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [417/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [418/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [419/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [420/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [421/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [422/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [423/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [424/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [425/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [426/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [427/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [428/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [429/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [430/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [431/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [432/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [433/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [434/1000] Fold 2, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [435/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [436/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [437/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [438/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [439/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [440/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [441/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [442/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [443/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [444/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [445/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [446/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [447/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [448/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [449/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [450/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [451/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [452/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [453/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [454/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [455/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [456/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [457/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [458/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [459/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [460/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [461/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [462/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [463/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [464/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [465/1000] Fold 2, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [466/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [467/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [468/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [469/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [470/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [471/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [472/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [473/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [474/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [475/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [476/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [477/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [478/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [479/1000] Fold 2, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [480/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [481/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [482/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [483/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [484/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [485/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [486/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [487/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [488/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [489/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [490/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [491/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [492/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [493/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [494/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [495/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [496/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [497/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [498/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [499/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [500/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [501/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [502/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [503/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [504/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [505/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [506/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [507/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [508/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [509/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [510/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [511/1000] Fold 2, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [512/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [513/1000] Fold 2, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [514/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [515/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [516/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [517/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [518/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [519/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [520/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [521/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [522/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [523/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [524/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [525/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [526/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [527/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [528/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [529/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [530/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [531/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [532/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [533/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [534/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [535/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [536/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [537/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [538/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [539/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [540/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [541/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [542/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [543/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [544/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [545/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [546/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [547/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [548/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [549/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [550/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [551/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [552/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [553/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [554/1000] Fold 2, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [555/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [556/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [557/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [558/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [559/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [560/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [561/1000] Fold 2, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [562/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [563/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [564/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [565/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [566/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [567/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [568/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [569/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [570/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [571/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [572/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [573/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [574/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [575/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [576/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [577/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [578/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [579/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [580/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [581/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [582/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [583/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [584/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [585/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [586/1000] Fold 2, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [587/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [588/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [589/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [590/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [591/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [592/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [593/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [594/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [595/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [596/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [597/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [598/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [599/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [600/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [601/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [602/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [603/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [604/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [605/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [606/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [607/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [608/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [609/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [610/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [611/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [612/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [613/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [614/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [615/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [616/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [617/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [618/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [619/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [620/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [621/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [622/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [623/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [624/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [625/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [626/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [627/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [628/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [629/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [630/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [631/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [632/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [633/1000] Fold 2, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [634/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [635/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [636/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [637/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [638/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [639/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [640/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [641/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [642/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [643/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [644/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [645/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [646/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [647/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [648/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [649/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [650/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [651/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [652/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [653/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [654/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [655/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [656/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [657/1000] Fold 2, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [658/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [659/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [660/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [661/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [662/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [663/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [664/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [665/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [666/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [667/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [668/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [669/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [670/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [671/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [672/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [673/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [674/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [675/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [676/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [677/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [678/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [679/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [680/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [681/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [682/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [683/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [684/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [685/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [686/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [687/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [688/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [689/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [690/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [691/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [692/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [693/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [694/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [695/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [696/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [697/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [698/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [699/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [700/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [701/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [702/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [703/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [704/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [705/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [706/1000] Fold 2, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [707/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [708/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [709/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [710/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [711/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [712/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [713/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [714/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [715/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [716/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [717/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [718/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [719/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [720/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [721/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [722/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [723/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [724/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [725/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [726/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [727/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [728/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [729/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [730/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [731/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [732/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [733/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [734/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [735/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [736/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [737/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [738/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [739/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [740/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [741/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [742/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [743/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [744/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [745/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [746/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [747/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [748/1000] Fold 2, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [749/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [750/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [751/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [752/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [753/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [754/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [755/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [756/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [757/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [758/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [759/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [760/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [761/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [762/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [763/1000] Fold 2, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [764/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [765/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [766/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [767/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [768/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [769/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [770/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [771/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [772/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [773/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [774/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [775/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [776/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [777/1000] Fold 2, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [778/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [779/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [780/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [781/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [782/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [783/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [784/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [785/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [786/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [787/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [788/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [789/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [790/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [791/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [792/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [793/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [794/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [795/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [796/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [797/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [798/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [799/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [800/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [801/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [802/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [803/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [804/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [805/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [806/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [807/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [808/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [809/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [810/1000] Fold 2, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [811/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [812/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [813/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [814/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [815/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [816/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [817/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [818/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [819/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [820/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [821/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [822/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [823/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [824/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [825/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [826/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [827/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [828/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [829/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [830/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [831/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [832/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [833/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [834/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [835/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [836/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [837/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [838/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [839/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [840/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [841/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [842/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [843/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [844/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [845/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [846/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [847/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [848/1000] Fold 2, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [849/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [850/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [851/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [852/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [853/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [854/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [855/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [856/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [857/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [858/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [859/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [860/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [861/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [862/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [863/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [864/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [865/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [866/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [867/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [868/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [869/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [870/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [871/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [872/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [873/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [874/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [875/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [876/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [877/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [878/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [879/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [880/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [881/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [882/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [883/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [884/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [885/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [886/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [887/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [888/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [889/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [890/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [891/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [892/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [893/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [894/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [895/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [896/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [897/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [898/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [899/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [900/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [901/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [902/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [903/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [904/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [905/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [906/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [907/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [908/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [909/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [910/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [911/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [912/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [913/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [914/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [915/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [916/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [917/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [918/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [919/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [920/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [921/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [922/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [923/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [924/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [925/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [926/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [927/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [928/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [929/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [930/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [931/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [932/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [933/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [934/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [935/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [936/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [937/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [938/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [939/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [940/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [941/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [942/1000] Fold 2, Train Loss: 0.0096, Val Loss: 0.0084\n",
      "Epoch [943/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [944/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [945/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [946/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [947/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [948/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [949/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [950/1000] Fold 2, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [951/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [952/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [953/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [954/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [955/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [956/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [957/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [958/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [959/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [960/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [961/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [962/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [963/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [964/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [965/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [966/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [967/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [968/1000] Fold 2, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [969/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [970/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [971/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [972/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [973/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [974/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [975/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [976/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [977/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [978/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [979/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [980/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [981/1000] Fold 2, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [982/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [983/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [984/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [985/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [986/1000] Fold 2, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [987/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [988/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [989/1000] Fold 2, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [990/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [991/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [992/1000] Fold 2, Train Loss: 0.0096, Val Loss: 0.0084\n",
      "Epoch [993/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [994/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [995/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [996/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [997/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [998/1000] Fold 2, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [999/1000] Fold 2, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [1000/1000] Fold 2, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Fold 3/10\n",
      "Epoch [1/1000] Fold 3, Train Loss: 0.1435, Val Loss: 0.0716\n",
      "Epoch [2/1000] Fold 3, Train Loss: 0.0757, Val Loss: 0.0455\n",
      "Epoch [3/1000] Fold 3, Train Loss: 0.0515, Val Loss: 0.0348\n",
      "Epoch [4/1000] Fold 3, Train Loss: 0.0413, Val Loss: 0.0309\n",
      "Epoch [5/1000] Fold 3, Train Loss: 0.0367, Val Loss: 0.0290\n",
      "Epoch [6/1000] Fold 3, Train Loss: 0.0339, Val Loss: 0.0282\n",
      "Epoch [7/1000] Fold 3, Train Loss: 0.0326, Val Loss: 0.0271\n",
      "Epoch [8/1000] Fold 3, Train Loss: 0.0312, Val Loss: 0.0266\n",
      "Epoch [9/1000] Fold 3, Train Loss: 0.0307, Val Loss: 0.0264\n",
      "Epoch [10/1000] Fold 3, Train Loss: 0.0299, Val Loss: 0.0257\n",
      "Epoch [11/1000] Fold 3, Train Loss: 0.0291, Val Loss: 0.0251\n",
      "Epoch [12/1000] Fold 3, Train Loss: 0.0284, Val Loss: 0.0252\n",
      "Epoch [13/1000] Fold 3, Train Loss: 0.0282, Val Loss: 0.0246\n",
      "Epoch [14/1000] Fold 3, Train Loss: 0.0277, Val Loss: 0.0248\n",
      "Epoch [15/1000] Fold 3, Train Loss: 0.0273, Val Loss: 0.0242\n",
      "Epoch [16/1000] Fold 3, Train Loss: 0.0269, Val Loss: 0.0238\n",
      "Epoch [17/1000] Fold 3, Train Loss: 0.0263, Val Loss: 0.0236\n",
      "Epoch [18/1000] Fold 3, Train Loss: 0.0263, Val Loss: 0.0233\n",
      "Epoch [19/1000] Fold 3, Train Loss: 0.0260, Val Loss: 0.0229\n",
      "Epoch [20/1000] Fold 3, Train Loss: 0.0256, Val Loss: 0.0225\n",
      "Epoch [21/1000] Fold 3, Train Loss: 0.0252, Val Loss: 0.0222\n",
      "Epoch [22/1000] Fold 3, Train Loss: 0.0247, Val Loss: 0.0218\n",
      "Epoch [23/1000] Fold 3, Train Loss: 0.0245, Val Loss: 0.0215\n",
      "Epoch [24/1000] Fold 3, Train Loss: 0.0245, Val Loss: 0.0211\n",
      "Epoch [25/1000] Fold 3, Train Loss: 0.0239, Val Loss: 0.0206\n",
      "Epoch [26/1000] Fold 3, Train Loss: 0.0235, Val Loss: 0.0205\n",
      "Epoch [27/1000] Fold 3, Train Loss: 0.0231, Val Loss: 0.0199\n",
      "Epoch [28/1000] Fold 3, Train Loss: 0.0228, Val Loss: 0.0199\n",
      "Epoch [29/1000] Fold 3, Train Loss: 0.0227, Val Loss: 0.0197\n",
      "Epoch [30/1000] Fold 3, Train Loss: 0.0226, Val Loss: 0.0192\n",
      "Epoch [31/1000] Fold 3, Train Loss: 0.0223, Val Loss: 0.0194\n",
      "Epoch [32/1000] Fold 3, Train Loss: 0.0223, Val Loss: 0.0190\n",
      "Epoch [33/1000] Fold 3, Train Loss: 0.0220, Val Loss: 0.0183\n",
      "Epoch [34/1000] Fold 3, Train Loss: 0.0217, Val Loss: 0.0185\n",
      "Epoch [35/1000] Fold 3, Train Loss: 0.0218, Val Loss: 0.0183\n",
      "Epoch [36/1000] Fold 3, Train Loss: 0.0212, Val Loss: 0.0180\n",
      "Epoch [37/1000] Fold 3, Train Loss: 0.0212, Val Loss: 0.0178\n",
      "Epoch [38/1000] Fold 3, Train Loss: 0.0211, Val Loss: 0.0174\n",
      "Epoch [39/1000] Fold 3, Train Loss: 0.0206, Val Loss: 0.0171\n",
      "Epoch [40/1000] Fold 3, Train Loss: 0.0207, Val Loss: 0.0172\n",
      "Epoch [41/1000] Fold 3, Train Loss: 0.0209, Val Loss: 0.0174\n",
      "Epoch [42/1000] Fold 3, Train Loss: 0.0204, Val Loss: 0.0169\n",
      "Epoch [43/1000] Fold 3, Train Loss: 0.0204, Val Loss: 0.0168\n",
      "Epoch [44/1000] Fold 3, Train Loss: 0.0201, Val Loss: 0.0165\n",
      "Epoch [45/1000] Fold 3, Train Loss: 0.0200, Val Loss: 0.0165\n",
      "Epoch [46/1000] Fold 3, Train Loss: 0.0201, Val Loss: 0.0165\n",
      "Epoch [47/1000] Fold 3, Train Loss: 0.0197, Val Loss: 0.0159\n",
      "Epoch [48/1000] Fold 3, Train Loss: 0.0200, Val Loss: 0.0160\n",
      "Epoch [49/1000] Fold 3, Train Loss: 0.0195, Val Loss: 0.0159\n",
      "Epoch [50/1000] Fold 3, Train Loss: 0.0196, Val Loss: 0.0158\n",
      "Epoch [51/1000] Fold 3, Train Loss: 0.0196, Val Loss: 0.0157\n",
      "Epoch [52/1000] Fold 3, Train Loss: 0.0192, Val Loss: 0.0154\n",
      "Epoch [53/1000] Fold 3, Train Loss: 0.0192, Val Loss: 0.0154\n",
      "Epoch [54/1000] Fold 3, Train Loss: 0.0187, Val Loss: 0.0156\n",
      "Epoch [55/1000] Fold 3, Train Loss: 0.0191, Val Loss: 0.0153\n",
      "Epoch [56/1000] Fold 3, Train Loss: 0.0191, Val Loss: 0.0152\n",
      "Epoch [57/1000] Fold 3, Train Loss: 0.0186, Val Loss: 0.0151\n",
      "Epoch [58/1000] Fold 3, Train Loss: 0.0187, Val Loss: 0.0149\n",
      "Epoch [59/1000] Fold 3, Train Loss: 0.0186, Val Loss: 0.0150\n",
      "Epoch [60/1000] Fold 3, Train Loss: 0.0185, Val Loss: 0.0147\n",
      "Epoch [61/1000] Fold 3, Train Loss: 0.0186, Val Loss: 0.0146\n",
      "Epoch [62/1000] Fold 3, Train Loss: 0.0183, Val Loss: 0.0150\n",
      "Epoch [63/1000] Fold 3, Train Loss: 0.0181, Val Loss: 0.0146\n",
      "Epoch [64/1000] Fold 3, Train Loss: 0.0180, Val Loss: 0.0147\n",
      "Epoch [65/1000] Fold 3, Train Loss: 0.0181, Val Loss: 0.0148\n",
      "Epoch [66/1000] Fold 3, Train Loss: 0.0184, Val Loss: 0.0144\n",
      "Epoch [67/1000] Fold 3, Train Loss: 0.0180, Val Loss: 0.0144\n",
      "Epoch [68/1000] Fold 3, Train Loss: 0.0175, Val Loss: 0.0141\n",
      "Epoch [69/1000] Fold 3, Train Loss: 0.0180, Val Loss: 0.0142\n",
      "Epoch [70/1000] Fold 3, Train Loss: 0.0178, Val Loss: 0.0141\n",
      "Epoch [71/1000] Fold 3, Train Loss: 0.0177, Val Loss: 0.0140\n",
      "Epoch [72/1000] Fold 3, Train Loss: 0.0179, Val Loss: 0.0140\n",
      "Epoch [73/1000] Fold 3, Train Loss: 0.0175, Val Loss: 0.0138\n",
      "Epoch [74/1000] Fold 3, Train Loss: 0.0175, Val Loss: 0.0141\n",
      "Epoch [75/1000] Fold 3, Train Loss: 0.0175, Val Loss: 0.0136\n",
      "Epoch [76/1000] Fold 3, Train Loss: 0.0175, Val Loss: 0.0139\n",
      "Epoch [77/1000] Fold 3, Train Loss: 0.0174, Val Loss: 0.0136\n",
      "Epoch [78/1000] Fold 3, Train Loss: 0.0173, Val Loss: 0.0138\n",
      "Epoch [79/1000] Fold 3, Train Loss: 0.0174, Val Loss: 0.0136\n",
      "Epoch [80/1000] Fold 3, Train Loss: 0.0172, Val Loss: 0.0134\n",
      "Epoch [81/1000] Fold 3, Train Loss: 0.0173, Val Loss: 0.0134\n",
      "Epoch [82/1000] Fold 3, Train Loss: 0.0173, Val Loss: 0.0136\n",
      "Epoch [83/1000] Fold 3, Train Loss: 0.0171, Val Loss: 0.0136\n",
      "Epoch [84/1000] Fold 3, Train Loss: 0.0171, Val Loss: 0.0138\n",
      "Epoch [85/1000] Fold 3, Train Loss: 0.0168, Val Loss: 0.0134\n",
      "Epoch [86/1000] Fold 3, Train Loss: 0.0171, Val Loss: 0.0133\n",
      "Epoch [87/1000] Fold 3, Train Loss: 0.0171, Val Loss: 0.0133\n",
      "Epoch [88/1000] Fold 3, Train Loss: 0.0167, Val Loss: 0.0132\n",
      "Epoch [89/1000] Fold 3, Train Loss: 0.0170, Val Loss: 0.0133\n",
      "Epoch [90/1000] Fold 3, Train Loss: 0.0167, Val Loss: 0.0131\n",
      "Epoch [91/1000] Fold 3, Train Loss: 0.0166, Val Loss: 0.0130\n",
      "Epoch [92/1000] Fold 3, Train Loss: 0.0165, Val Loss: 0.0129\n",
      "Epoch [93/1000] Fold 3, Train Loss: 0.0163, Val Loss: 0.0128\n",
      "Epoch [94/1000] Fold 3, Train Loss: 0.0166, Val Loss: 0.0129\n",
      "Epoch [95/1000] Fold 3, Train Loss: 0.0166, Val Loss: 0.0128\n",
      "Epoch [96/1000] Fold 3, Train Loss: 0.0166, Val Loss: 0.0129\n",
      "Epoch [97/1000] Fold 3, Train Loss: 0.0165, Val Loss: 0.0127\n",
      "Epoch [98/1000] Fold 3, Train Loss: 0.0165, Val Loss: 0.0126\n",
      "Epoch [99/1000] Fold 3, Train Loss: 0.0162, Val Loss: 0.0128\n",
      "Epoch [100/1000] Fold 3, Train Loss: 0.0161, Val Loss: 0.0126\n",
      "Epoch [101/1000] Fold 3, Train Loss: 0.0163, Val Loss: 0.0126\n",
      "Epoch [102/1000] Fold 3, Train Loss: 0.0163, Val Loss: 0.0126\n",
      "Epoch [103/1000] Fold 3, Train Loss: 0.0161, Val Loss: 0.0127\n",
      "Epoch [104/1000] Fold 3, Train Loss: 0.0163, Val Loss: 0.0126\n",
      "Epoch [105/1000] Fold 3, Train Loss: 0.0163, Val Loss: 0.0127\n",
      "Epoch [106/1000] Fold 3, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Epoch [107/1000] Fold 3, Train Loss: 0.0162, Val Loss: 0.0123\n",
      "Epoch [108/1000] Fold 3, Train Loss: 0.0157, Val Loss: 0.0124\n",
      "Epoch [109/1000] Fold 3, Train Loss: 0.0160, Val Loss: 0.0123\n",
      "Epoch [110/1000] Fold 3, Train Loss: 0.0162, Val Loss: 0.0124\n",
      "Epoch [111/1000] Fold 3, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Epoch [112/1000] Fold 3, Train Loss: 0.0158, Val Loss: 0.0123\n",
      "Epoch [113/1000] Fold 3, Train Loss: 0.0156, Val Loss: 0.0120\n",
      "Epoch [114/1000] Fold 3, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [115/1000] Fold 3, Train Loss: 0.0157, Val Loss: 0.0123\n",
      "Epoch [116/1000] Fold 3, Train Loss: 0.0159, Val Loss: 0.0121\n",
      "Epoch [117/1000] Fold 3, Train Loss: 0.0156, Val Loss: 0.0118\n",
      "Epoch [118/1000] Fold 3, Train Loss: 0.0157, Val Loss: 0.0121\n",
      "Epoch [119/1000] Fold 3, Train Loss: 0.0159, Val Loss: 0.0122\n",
      "Epoch [120/1000] Fold 3, Train Loss: 0.0157, Val Loss: 0.0121\n",
      "Epoch [121/1000] Fold 3, Train Loss: 0.0156, Val Loss: 0.0121\n",
      "Epoch [122/1000] Fold 3, Train Loss: 0.0157, Val Loss: 0.0122\n",
      "Epoch [123/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [124/1000] Fold 3, Train Loss: 0.0155, Val Loss: 0.0118\n",
      "Epoch [125/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [126/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [127/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [128/1000] Fold 3, Train Loss: 0.0153, Val Loss: 0.0120\n",
      "Epoch [129/1000] Fold 3, Train Loss: 0.0152, Val Loss: 0.0117\n",
      "Epoch [130/1000] Fold 3, Train Loss: 0.0152, Val Loss: 0.0117\n",
      "Epoch [131/1000] Fold 3, Train Loss: 0.0155, Val Loss: 0.0117\n",
      "Epoch [132/1000] Fold 3, Train Loss: 0.0152, Val Loss: 0.0118\n",
      "Epoch [133/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0114\n",
      "Epoch [134/1000] Fold 3, Train Loss: 0.0152, Val Loss: 0.0118\n",
      "Epoch [135/1000] Fold 3, Train Loss: 0.0149, Val Loss: 0.0117\n",
      "Epoch [136/1000] Fold 3, Train Loss: 0.0155, Val Loss: 0.0118\n",
      "Epoch [137/1000] Fold 3, Train Loss: 0.0154, Val Loss: 0.0121\n",
      "Epoch [138/1000] Fold 3, Train Loss: 0.0149, Val Loss: 0.0117\n",
      "Epoch [139/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0116\n",
      "Epoch [140/1000] Fold 3, Train Loss: 0.0150, Val Loss: 0.0114\n",
      "Epoch [141/1000] Fold 3, Train Loss: 0.0150, Val Loss: 0.0115\n",
      "Epoch [142/1000] Fold 3, Train Loss: 0.0148, Val Loss: 0.0113\n",
      "Epoch [143/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0112\n",
      "Epoch [144/1000] Fold 3, Train Loss: 0.0150, Val Loss: 0.0113\n",
      "Epoch [145/1000] Fold 3, Train Loss: 0.0148, Val Loss: 0.0111\n",
      "Epoch [146/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0113\n",
      "Epoch [147/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0111\n",
      "Epoch [148/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0110\n",
      "Epoch [149/1000] Fold 3, Train Loss: 0.0150, Val Loss: 0.0112\n",
      "Epoch [150/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0114\n",
      "Epoch [151/1000] Fold 3, Train Loss: 0.0149, Val Loss: 0.0114\n",
      "Epoch [152/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0112\n",
      "Epoch [153/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0112\n",
      "Epoch [154/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0111\n",
      "Epoch [155/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [156/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0111\n",
      "Epoch [157/1000] Fold 3, Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [158/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0110\n",
      "Epoch [159/1000] Fold 3, Train Loss: 0.0145, Val Loss: 0.0109\n",
      "Epoch [160/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [161/1000] Fold 3, Train Loss: 0.0147, Val Loss: 0.0109\n",
      "Epoch [162/1000] Fold 3, Train Loss: 0.0145, Val Loss: 0.0112\n",
      "Epoch [163/1000] Fold 3, Train Loss: 0.0142, Val Loss: 0.0110\n",
      "Epoch [164/1000] Fold 3, Train Loss: 0.0146, Val Loss: 0.0110\n",
      "Epoch [165/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [166/1000] Fold 3, Train Loss: 0.0141, Val Loss: 0.0108\n",
      "Epoch [167/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0109\n",
      "Epoch [168/1000] Fold 3, Train Loss: 0.0145, Val Loss: 0.0108\n",
      "Epoch [169/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0109\n",
      "Epoch [170/1000] Fold 3, Train Loss: 0.0144, Val Loss: 0.0109\n",
      "Epoch [171/1000] Fold 3, Train Loss: 0.0144, Val Loss: 0.0107\n",
      "Epoch [172/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0107\n",
      "Epoch [173/1000] Fold 3, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [174/1000] Fold 3, Train Loss: 0.0141, Val Loss: 0.0106\n",
      "Epoch [175/1000] Fold 3, Train Loss: 0.0142, Val Loss: 0.0107\n",
      "Epoch [176/1000] Fold 3, Train Loss: 0.0142, Val Loss: 0.0107\n",
      "Epoch [177/1000] Fold 3, Train Loss: 0.0143, Val Loss: 0.0108\n",
      "Epoch [178/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0107\n",
      "Epoch [179/1000] Fold 3, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [180/1000] Fold 3, Train Loss: 0.0141, Val Loss: 0.0108\n",
      "Epoch [181/1000] Fold 3, Train Loss: 0.0140, Val Loss: 0.0106\n",
      "Epoch [182/1000] Fold 3, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [183/1000] Fold 3, Train Loss: 0.0142, Val Loss: 0.0106\n",
      "Epoch [184/1000] Fold 3, Train Loss: 0.0141, Val Loss: 0.0105\n",
      "Epoch [185/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0106\n",
      "Epoch [186/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0105\n",
      "Epoch [187/1000] Fold 3, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [188/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0105\n",
      "Epoch [189/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [190/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0104\n",
      "Epoch [191/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0105\n",
      "Epoch [192/1000] Fold 3, Train Loss: 0.0140, Val Loss: 0.0105\n",
      "Epoch [193/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0104\n",
      "Epoch [194/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [195/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0104\n",
      "Epoch [196/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [197/1000] Fold 3, Train Loss: 0.0139, Val Loss: 0.0103\n",
      "Epoch [198/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [199/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0102\n",
      "Epoch [200/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [201/1000] Fold 3, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [202/1000] Fold 3, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [203/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [204/1000] Fold 3, Train Loss: 0.0135, Val Loss: 0.0104\n",
      "Epoch [205/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0102\n",
      "Epoch [206/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [207/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [208/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [209/1000] Fold 3, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [210/1000] Fold 3, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [211/1000] Fold 3, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [212/1000] Fold 3, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [213/1000] Fold 3, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [214/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0101\n",
      "Epoch [215/1000] Fold 3, Train Loss: 0.0135, Val Loss: 0.0103\n",
      "Epoch [216/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [217/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0100\n",
      "Epoch [218/1000] Fold 3, Train Loss: 0.0135, Val Loss: 0.0102\n",
      "Epoch [219/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [220/1000] Fold 3, Train Loss: 0.0135, Val Loss: 0.0101\n",
      "Epoch [221/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0102\n",
      "Epoch [222/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [223/1000] Fold 3, Train Loss: 0.0135, Val Loss: 0.0101\n",
      "Epoch [224/1000] Fold 3, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [225/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0101\n",
      "Epoch [226/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [227/1000] Fold 3, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [228/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0102\n",
      "Epoch [229/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0100\n",
      "Epoch [230/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0102\n",
      "Epoch [231/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0099\n",
      "Epoch [232/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0102\n",
      "Epoch [233/1000] Fold 3, Train Loss: 0.0133, Val Loss: 0.0101\n",
      "Epoch [234/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [235/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0100\n",
      "Epoch [236/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0099\n",
      "Epoch [237/1000] Fold 3, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [238/1000] Fold 3, Train Loss: 0.0132, Val Loss: 0.0100\n",
      "Epoch [239/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [240/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [241/1000] Fold 3, Train Loss: 0.0134, Val Loss: 0.0101\n",
      "Epoch [242/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0100\n",
      "Epoch [243/1000] Fold 3, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [244/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [245/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [246/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [247/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [248/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [249/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0099\n",
      "Epoch [250/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [251/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [252/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [253/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [254/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [255/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0098\n",
      "Epoch [256/1000] Fold 3, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [257/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [258/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [259/1000] Fold 3, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [260/1000] Fold 3, Train Loss: 0.0129, Val Loss: 0.0099\n",
      "Epoch [261/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0098\n",
      "Epoch [262/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [263/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [264/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [265/1000] Fold 3, Train Loss: 0.0129, Val Loss: 0.0097\n",
      "Epoch [266/1000] Fold 3, Train Loss: 0.0129, Val Loss: 0.0097\n",
      "Epoch [267/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0097\n",
      "Epoch [268/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [269/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [270/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0096\n",
      "Epoch [271/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [272/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [273/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [274/1000] Fold 3, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [275/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [276/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0098\n",
      "Epoch [277/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [278/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [279/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [280/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0095\n",
      "Epoch [281/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [282/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [283/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [284/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0096\n",
      "Epoch [285/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [286/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0095\n",
      "Epoch [287/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [288/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [289/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [290/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [291/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0095\n",
      "Epoch [292/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [293/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [294/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [295/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0096\n",
      "Epoch [296/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [297/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [298/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [299/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [300/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0094\n",
      "Epoch [301/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0094\n",
      "Epoch [302/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [303/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [304/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0094\n",
      "Epoch [305/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [306/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0094\n",
      "Epoch [307/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [308/1000] Fold 3, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [309/1000] Fold 3, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [310/1000] Fold 3, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [311/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [312/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [313/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [314/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [315/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0094\n",
      "Epoch [316/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [317/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [318/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0092\n",
      "Epoch [319/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [320/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [321/1000] Fold 3, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [322/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [323/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0094\n",
      "Epoch [324/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [325/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [326/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [327/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0095\n",
      "Epoch [328/1000] Fold 3, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [329/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [330/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0094\n",
      "Epoch [331/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [332/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0094\n",
      "Epoch [333/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [334/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [335/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [336/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [337/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [338/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0092\n",
      "Epoch [339/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [340/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [341/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [342/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [343/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [344/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [345/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [346/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [347/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [348/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0093\n",
      "Epoch [349/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [350/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [351/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0091\n",
      "Epoch [352/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [353/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [354/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [355/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [356/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [357/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [358/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [359/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [360/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [361/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [362/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [363/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [364/1000] Fold 3, Train Loss: 0.0123, Val Loss: 0.0091\n",
      "Epoch [365/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [366/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [367/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [368/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [369/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [370/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [371/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [372/1000] Fold 3, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [373/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [374/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [375/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [376/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [377/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [378/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [379/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [380/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [381/1000] Fold 3, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [382/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [383/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [384/1000] Fold 3, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [385/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [386/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [387/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [388/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [389/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [390/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [391/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [392/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [393/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [394/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [395/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [396/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [397/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [398/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [399/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [400/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [401/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [402/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [403/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [404/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [405/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [406/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [407/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [408/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [409/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0091\n",
      "Epoch [410/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [411/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [412/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [413/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [414/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [415/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [416/1000] Fold 3, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [417/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [418/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [419/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [420/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [421/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [422/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [423/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [424/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [425/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [426/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [427/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [428/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [429/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [430/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [431/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [432/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [433/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [434/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [435/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [436/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [437/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [438/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [439/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [440/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [441/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [442/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [443/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [444/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [445/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [446/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [447/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [448/1000] Fold 3, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [449/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [450/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [451/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [452/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [453/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [454/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [455/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [456/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [457/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [458/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [459/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [460/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [461/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [462/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [463/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [464/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [465/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [466/1000] Fold 3, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [467/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [468/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [469/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [470/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [471/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [472/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [473/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [474/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [475/1000] Fold 3, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [476/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [477/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [478/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [479/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [480/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [481/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [482/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [483/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [484/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [485/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [486/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [487/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [488/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [489/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [490/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [491/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [492/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [493/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [494/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [495/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [496/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [497/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [498/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [499/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [500/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [501/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [502/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [503/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [504/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [505/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [506/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [507/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [508/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [509/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [510/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [511/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [512/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [513/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [514/1000] Fold 3, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [515/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [516/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [517/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [518/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [519/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [520/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [521/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [522/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [523/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [524/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [525/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [526/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [527/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [528/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [529/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [530/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [531/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [532/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [533/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [534/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [535/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [536/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [537/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [538/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [539/1000] Fold 3, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [540/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [541/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [542/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [543/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [544/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [545/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [546/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [547/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [548/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [549/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [550/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [551/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [552/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [553/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [554/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [555/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [556/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [557/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [558/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [559/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [560/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [561/1000] Fold 3, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [562/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [563/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [564/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [565/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [566/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [567/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [568/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [569/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [570/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [571/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [572/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [573/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [574/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [575/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [576/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [577/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [578/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [579/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [580/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [581/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [582/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [583/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [584/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [585/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [586/1000] Fold 3, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [587/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [588/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [589/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [590/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [591/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [592/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [593/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [594/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [595/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [596/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [597/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [598/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [599/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [600/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [601/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [602/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [603/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [604/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [605/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [606/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [607/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [608/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [609/1000] Fold 3, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [610/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [611/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [612/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [613/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [614/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [615/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [616/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [617/1000] Fold 3, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [618/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [619/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [620/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [621/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [622/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [623/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [624/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [625/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [626/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [627/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [628/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [629/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [630/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [631/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [632/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [633/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [634/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [635/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [636/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [637/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [638/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [639/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [640/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [641/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [642/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [643/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [644/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [645/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [646/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [647/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [648/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [649/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [650/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [651/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [652/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [653/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [654/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [655/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [656/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [657/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [658/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [659/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [660/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [661/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [662/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [663/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [664/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [665/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [666/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [667/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [668/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [669/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [670/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [671/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [672/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [673/1000] Fold 3, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [674/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [675/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [676/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [677/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [678/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [679/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [680/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [681/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [682/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [683/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [684/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [685/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [686/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [687/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [688/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [689/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [690/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [691/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [692/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [693/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [694/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [695/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [696/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [697/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [698/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [699/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [700/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [701/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [702/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [703/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [704/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [705/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [706/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [707/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [708/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [709/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [710/1000] Fold 3, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [711/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [712/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [713/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [714/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [715/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [716/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [717/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [718/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [719/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [720/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [721/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [722/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [723/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [724/1000] Fold 3, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [725/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [726/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [727/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [728/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [729/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [730/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [731/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [732/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [733/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [734/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [735/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [736/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [737/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [738/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [739/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [740/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [741/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [742/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [743/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [744/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [745/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [746/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [747/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [748/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [749/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [750/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [751/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [752/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [753/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [754/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [755/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [756/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [757/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [758/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [759/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [760/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [761/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [762/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [763/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [764/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [765/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [766/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [767/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [768/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [769/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [770/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [771/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [772/1000] Fold 3, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [773/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [774/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [775/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [776/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [777/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [778/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [779/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [780/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [781/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [782/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [783/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [784/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [785/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [786/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [787/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [788/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [789/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [790/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [791/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [792/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [793/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [794/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [795/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [796/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [797/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [798/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [799/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [800/1000] Fold 3, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [801/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [802/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [803/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [804/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [805/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [806/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [807/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [808/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [809/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [810/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [811/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [812/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [813/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [814/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [815/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [816/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [817/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [818/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [819/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [820/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [821/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [822/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [823/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [824/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [825/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [826/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [827/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [828/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [829/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [830/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [831/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [832/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [833/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [834/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [835/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [836/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [837/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [838/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [839/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [840/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [841/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [842/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [843/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [844/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [845/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [846/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [847/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [848/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [849/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [850/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [851/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [852/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [853/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [854/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [855/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [856/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [857/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [858/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [859/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [860/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [861/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [862/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [863/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [864/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [865/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [866/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [867/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [868/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [869/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [870/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [871/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [872/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [873/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [874/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [875/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [876/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [877/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [878/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [879/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [880/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [881/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [882/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [883/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [884/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [885/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [886/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [887/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [888/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [889/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [890/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [891/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [892/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [893/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [894/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [895/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [896/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [897/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [898/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [899/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [900/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [901/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [902/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [903/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [904/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [905/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [906/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [907/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [908/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [909/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [910/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [911/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [912/1000] Fold 3, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [913/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [914/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [915/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [916/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [917/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [918/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [919/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [920/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [921/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [922/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [923/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [924/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [925/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [926/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [927/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [928/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [929/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [930/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [931/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [932/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [933/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [934/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [935/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [936/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [937/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [938/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [939/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [940/1000] Fold 3, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [941/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [942/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [943/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [944/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [945/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [946/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [947/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [948/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [949/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [950/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [951/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [952/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [953/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [954/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [955/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [956/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [957/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [958/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [959/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [960/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [961/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [962/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [963/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [964/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [965/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [966/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [967/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [968/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [969/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [970/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [971/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [972/1000] Fold 3, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [973/1000] Fold 3, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [974/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [975/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [976/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [977/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [978/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [979/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [980/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [981/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [982/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [983/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [984/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [985/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [986/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [987/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [988/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [989/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [990/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [991/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [992/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [993/1000] Fold 3, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [994/1000] Fold 3, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [995/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [996/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [997/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [998/1000] Fold 3, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [999/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [1000/1000] Fold 3, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Fold 4/10\n",
      "Epoch [1/1000] Fold 4, Train Loss: 0.1402, Val Loss: 0.0669\n",
      "Epoch [2/1000] Fold 4, Train Loss: 0.0741, Val Loss: 0.0436\n",
      "Epoch [3/1000] Fold 4, Train Loss: 0.0518, Val Loss: 0.0353\n",
      "Epoch [4/1000] Fold 4, Train Loss: 0.0422, Val Loss: 0.0310\n",
      "Epoch [5/1000] Fold 4, Train Loss: 0.0371, Val Loss: 0.0286\n",
      "Epoch [6/1000] Fold 4, Train Loss: 0.0340, Val Loss: 0.0272\n",
      "Epoch [7/1000] Fold 4, Train Loss: 0.0321, Val Loss: 0.0261\n",
      "Epoch [8/1000] Fold 4, Train Loss: 0.0313, Val Loss: 0.0253\n",
      "Epoch [9/1000] Fold 4, Train Loss: 0.0301, Val Loss: 0.0247\n",
      "Epoch [10/1000] Fold 4, Train Loss: 0.0294, Val Loss: 0.0243\n",
      "Epoch [11/1000] Fold 4, Train Loss: 0.0287, Val Loss: 0.0239\n",
      "Epoch [12/1000] Fold 4, Train Loss: 0.0282, Val Loss: 0.0237\n",
      "Epoch [13/1000] Fold 4, Train Loss: 0.0277, Val Loss: 0.0232\n",
      "Epoch [14/1000] Fold 4, Train Loss: 0.0269, Val Loss: 0.0226\n",
      "Epoch [15/1000] Fold 4, Train Loss: 0.0269, Val Loss: 0.0225\n",
      "Epoch [16/1000] Fold 4, Train Loss: 0.0264, Val Loss: 0.0221\n",
      "Epoch [17/1000] Fold 4, Train Loss: 0.0264, Val Loss: 0.0220\n",
      "Epoch [18/1000] Fold 4, Train Loss: 0.0258, Val Loss: 0.0214\n",
      "Epoch [19/1000] Fold 4, Train Loss: 0.0250, Val Loss: 0.0212\n",
      "Epoch [20/1000] Fold 4, Train Loss: 0.0250, Val Loss: 0.0211\n",
      "Epoch [21/1000] Fold 4, Train Loss: 0.0247, Val Loss: 0.0206\n",
      "Epoch [22/1000] Fold 4, Train Loss: 0.0243, Val Loss: 0.0203\n",
      "Epoch [23/1000] Fold 4, Train Loss: 0.0242, Val Loss: 0.0202\n",
      "Epoch [24/1000] Fold 4, Train Loss: 0.0241, Val Loss: 0.0199\n",
      "Epoch [25/1000] Fold 4, Train Loss: 0.0236, Val Loss: 0.0198\n",
      "Epoch [26/1000] Fold 4, Train Loss: 0.0237, Val Loss: 0.0196\n",
      "Epoch [27/1000] Fold 4, Train Loss: 0.0232, Val Loss: 0.0192\n",
      "Epoch [28/1000] Fold 4, Train Loss: 0.0229, Val Loss: 0.0193\n",
      "Epoch [29/1000] Fold 4, Train Loss: 0.0228, Val Loss: 0.0191\n",
      "Epoch [30/1000] Fold 4, Train Loss: 0.0226, Val Loss: 0.0187\n",
      "Epoch [31/1000] Fold 4, Train Loss: 0.0223, Val Loss: 0.0185\n",
      "Epoch [32/1000] Fold 4, Train Loss: 0.0222, Val Loss: 0.0185\n",
      "Epoch [33/1000] Fold 4, Train Loss: 0.0219, Val Loss: 0.0182\n",
      "Epoch [34/1000] Fold 4, Train Loss: 0.0220, Val Loss: 0.0182\n",
      "Epoch [35/1000] Fold 4, Train Loss: 0.0219, Val Loss: 0.0180\n",
      "Epoch [36/1000] Fold 4, Train Loss: 0.0215, Val Loss: 0.0178\n",
      "Epoch [37/1000] Fold 4, Train Loss: 0.0213, Val Loss: 0.0174\n",
      "Epoch [38/1000] Fold 4, Train Loss: 0.0212, Val Loss: 0.0174\n",
      "Epoch [39/1000] Fold 4, Train Loss: 0.0212, Val Loss: 0.0172\n",
      "Epoch [40/1000] Fold 4, Train Loss: 0.0208, Val Loss: 0.0171\n",
      "Epoch [41/1000] Fold 4, Train Loss: 0.0210, Val Loss: 0.0168\n",
      "Epoch [42/1000] Fold 4, Train Loss: 0.0208, Val Loss: 0.0169\n",
      "Epoch [43/1000] Fold 4, Train Loss: 0.0207, Val Loss: 0.0167\n",
      "Epoch [44/1000] Fold 4, Train Loss: 0.0206, Val Loss: 0.0164\n",
      "Epoch [45/1000] Fold 4, Train Loss: 0.0201, Val Loss: 0.0164\n",
      "Epoch [46/1000] Fold 4, Train Loss: 0.0204, Val Loss: 0.0163\n",
      "Epoch [47/1000] Fold 4, Train Loss: 0.0202, Val Loss: 0.0163\n",
      "Epoch [48/1000] Fold 4, Train Loss: 0.0199, Val Loss: 0.0161\n",
      "Epoch [49/1000] Fold 4, Train Loss: 0.0198, Val Loss: 0.0161\n",
      "Epoch [50/1000] Fold 4, Train Loss: 0.0198, Val Loss: 0.0157\n",
      "Epoch [51/1000] Fold 4, Train Loss: 0.0195, Val Loss: 0.0156\n",
      "Epoch [52/1000] Fold 4, Train Loss: 0.0193, Val Loss: 0.0155\n",
      "Epoch [53/1000] Fold 4, Train Loss: 0.0194, Val Loss: 0.0154\n",
      "Epoch [54/1000] Fold 4, Train Loss: 0.0190, Val Loss: 0.0153\n",
      "Epoch [55/1000] Fold 4, Train Loss: 0.0190, Val Loss: 0.0151\n",
      "Epoch [56/1000] Fold 4, Train Loss: 0.0190, Val Loss: 0.0152\n",
      "Epoch [57/1000] Fold 4, Train Loss: 0.0192, Val Loss: 0.0151\n",
      "Epoch [58/1000] Fold 4, Train Loss: 0.0189, Val Loss: 0.0150\n",
      "Epoch [59/1000] Fold 4, Train Loss: 0.0187, Val Loss: 0.0150\n",
      "Epoch [60/1000] Fold 4, Train Loss: 0.0187, Val Loss: 0.0148\n",
      "Epoch [61/1000] Fold 4, Train Loss: 0.0184, Val Loss: 0.0147\n",
      "Epoch [62/1000] Fold 4, Train Loss: 0.0182, Val Loss: 0.0146\n",
      "Epoch [63/1000] Fold 4, Train Loss: 0.0182, Val Loss: 0.0144\n",
      "Epoch [64/1000] Fold 4, Train Loss: 0.0182, Val Loss: 0.0142\n",
      "Epoch [65/1000] Fold 4, Train Loss: 0.0184, Val Loss: 0.0142\n",
      "Epoch [66/1000] Fold 4, Train Loss: 0.0180, Val Loss: 0.0144\n",
      "Epoch [67/1000] Fold 4, Train Loss: 0.0182, Val Loss: 0.0142\n",
      "Epoch [68/1000] Fold 4, Train Loss: 0.0180, Val Loss: 0.0140\n",
      "Epoch [69/1000] Fold 4, Train Loss: 0.0180, Val Loss: 0.0139\n",
      "Epoch [70/1000] Fold 4, Train Loss: 0.0178, Val Loss: 0.0139\n",
      "Epoch [71/1000] Fold 4, Train Loss: 0.0178, Val Loss: 0.0138\n",
      "Epoch [72/1000] Fold 4, Train Loss: 0.0176, Val Loss: 0.0137\n",
      "Epoch [73/1000] Fold 4, Train Loss: 0.0176, Val Loss: 0.0138\n",
      "Epoch [74/1000] Fold 4, Train Loss: 0.0176, Val Loss: 0.0136\n",
      "Epoch [75/1000] Fold 4, Train Loss: 0.0175, Val Loss: 0.0135\n",
      "Epoch [76/1000] Fold 4, Train Loss: 0.0179, Val Loss: 0.0136\n",
      "Epoch [77/1000] Fold 4, Train Loss: 0.0174, Val Loss: 0.0133\n",
      "Epoch [78/1000] Fold 4, Train Loss: 0.0175, Val Loss: 0.0133\n",
      "Epoch [79/1000] Fold 4, Train Loss: 0.0171, Val Loss: 0.0132\n",
      "Epoch [80/1000] Fold 4, Train Loss: 0.0169, Val Loss: 0.0130\n",
      "Epoch [81/1000] Fold 4, Train Loss: 0.0171, Val Loss: 0.0131\n",
      "Epoch [82/1000] Fold 4, Train Loss: 0.0171, Val Loss: 0.0132\n",
      "Epoch [83/1000] Fold 4, Train Loss: 0.0168, Val Loss: 0.0130\n",
      "Epoch [84/1000] Fold 4, Train Loss: 0.0167, Val Loss: 0.0129\n",
      "Epoch [85/1000] Fold 4, Train Loss: 0.0170, Val Loss: 0.0126\n",
      "Epoch [86/1000] Fold 4, Train Loss: 0.0167, Val Loss: 0.0129\n",
      "Epoch [87/1000] Fold 4, Train Loss: 0.0166, Val Loss: 0.0127\n",
      "Epoch [88/1000] Fold 4, Train Loss: 0.0167, Val Loss: 0.0128\n",
      "Epoch [89/1000] Fold 4, Train Loss: 0.0170, Val Loss: 0.0127\n",
      "Epoch [90/1000] Fold 4, Train Loss: 0.0166, Val Loss: 0.0127\n",
      "Epoch [91/1000] Fold 4, Train Loss: 0.0164, Val Loss: 0.0126\n",
      "Epoch [92/1000] Fold 4, Train Loss: 0.0167, Val Loss: 0.0127\n",
      "Epoch [93/1000] Fold 4, Train Loss: 0.0162, Val Loss: 0.0125\n",
      "Epoch [94/1000] Fold 4, Train Loss: 0.0164, Val Loss: 0.0125\n",
      "Epoch [95/1000] Fold 4, Train Loss: 0.0164, Val Loss: 0.0123\n",
      "Epoch [96/1000] Fold 4, Train Loss: 0.0163, Val Loss: 0.0124\n",
      "Epoch [97/1000] Fold 4, Train Loss: 0.0163, Val Loss: 0.0124\n",
      "Epoch [98/1000] Fold 4, Train Loss: 0.0159, Val Loss: 0.0123\n",
      "Epoch [99/1000] Fold 4, Train Loss: 0.0164, Val Loss: 0.0121\n",
      "Epoch [100/1000] Fold 4, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Epoch [101/1000] Fold 4, Train Loss: 0.0160, Val Loss: 0.0121\n",
      "Epoch [102/1000] Fold 4, Train Loss: 0.0159, Val Loss: 0.0119\n",
      "Epoch [103/1000] Fold 4, Train Loss: 0.0160, Val Loss: 0.0121\n",
      "Epoch [104/1000] Fold 4, Train Loss: 0.0162, Val Loss: 0.0121\n",
      "Epoch [105/1000] Fold 4, Train Loss: 0.0158, Val Loss: 0.0121\n",
      "Epoch [106/1000] Fold 4, Train Loss: 0.0158, Val Loss: 0.0120\n",
      "Epoch [107/1000] Fold 4, Train Loss: 0.0158, Val Loss: 0.0119\n",
      "Epoch [108/1000] Fold 4, Train Loss: 0.0157, Val Loss: 0.0119\n",
      "Epoch [109/1000] Fold 4, Train Loss: 0.0156, Val Loss: 0.0119\n",
      "Epoch [110/1000] Fold 4, Train Loss: 0.0157, Val Loss: 0.0119\n",
      "Epoch [111/1000] Fold 4, Train Loss: 0.0158, Val Loss: 0.0117\n",
      "Epoch [112/1000] Fold 4, Train Loss: 0.0155, Val Loss: 0.0118\n",
      "Epoch [113/1000] Fold 4, Train Loss: 0.0158, Val Loss: 0.0118\n",
      "Epoch [114/1000] Fold 4, Train Loss: 0.0154, Val Loss: 0.0117\n",
      "Epoch [115/1000] Fold 4, Train Loss: 0.0157, Val Loss: 0.0116\n",
      "Epoch [116/1000] Fold 4, Train Loss: 0.0156, Val Loss: 0.0118\n",
      "Epoch [117/1000] Fold 4, Train Loss: 0.0156, Val Loss: 0.0116\n",
      "Epoch [118/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0114\n",
      "Epoch [119/1000] Fold 4, Train Loss: 0.0155, Val Loss: 0.0117\n",
      "Epoch [120/1000] Fold 4, Train Loss: 0.0154, Val Loss: 0.0116\n",
      "Epoch [121/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0114\n",
      "Epoch [122/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0115\n",
      "Epoch [123/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0113\n",
      "Epoch [124/1000] Fold 4, Train Loss: 0.0155, Val Loss: 0.0116\n",
      "Epoch [125/1000] Fold 4, Train Loss: 0.0150, Val Loss: 0.0113\n",
      "Epoch [126/1000] Fold 4, Train Loss: 0.0150, Val Loss: 0.0113\n",
      "Epoch [127/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0114\n",
      "Epoch [128/1000] Fold 4, Train Loss: 0.0150, Val Loss: 0.0114\n",
      "Epoch [129/1000] Fold 4, Train Loss: 0.0153, Val Loss: 0.0113\n",
      "Epoch [130/1000] Fold 4, Train Loss: 0.0148, Val Loss: 0.0111\n",
      "Epoch [131/1000] Fold 4, Train Loss: 0.0151, Val Loss: 0.0112\n",
      "Epoch [132/1000] Fold 4, Train Loss: 0.0148, Val Loss: 0.0111\n",
      "Epoch [133/1000] Fold 4, Train Loss: 0.0151, Val Loss: 0.0112\n",
      "Epoch [134/1000] Fold 4, Train Loss: 0.0146, Val Loss: 0.0110\n",
      "Epoch [135/1000] Fold 4, Train Loss: 0.0149, Val Loss: 0.0111\n",
      "Epoch [136/1000] Fold 4, Train Loss: 0.0148, Val Loss: 0.0111\n",
      "Epoch [137/1000] Fold 4, Train Loss: 0.0149, Val Loss: 0.0110\n",
      "Epoch [138/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0109\n",
      "Epoch [139/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [140/1000] Fold 4, Train Loss: 0.0146, Val Loss: 0.0108\n",
      "Epoch [141/1000] Fold 4, Train Loss: 0.0149, Val Loss: 0.0109\n",
      "Epoch [142/1000] Fold 4, Train Loss: 0.0147, Val Loss: 0.0108\n",
      "Epoch [143/1000] Fold 4, Train Loss: 0.0147, Val Loss: 0.0110\n",
      "Epoch [144/1000] Fold 4, Train Loss: 0.0147, Val Loss: 0.0109\n",
      "Epoch [145/1000] Fold 4, Train Loss: 0.0147, Val Loss: 0.0109\n",
      "Epoch [146/1000] Fold 4, Train Loss: 0.0144, Val Loss: 0.0109\n",
      "Epoch [147/1000] Fold 4, Train Loss: 0.0146, Val Loss: 0.0108\n",
      "Epoch [148/1000] Fold 4, Train Loss: 0.0144, Val Loss: 0.0109\n",
      "Epoch [149/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0107\n",
      "Epoch [150/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0107\n",
      "Epoch [151/1000] Fold 4, Train Loss: 0.0146, Val Loss: 0.0108\n",
      "Epoch [152/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0108\n",
      "Epoch [153/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0107\n",
      "Epoch [154/1000] Fold 4, Train Loss: 0.0144, Val Loss: 0.0106\n",
      "Epoch [155/1000] Fold 4, Train Loss: 0.0145, Val Loss: 0.0107\n",
      "Epoch [156/1000] Fold 4, Train Loss: 0.0144, Val Loss: 0.0108\n",
      "Epoch [157/1000] Fold 4, Train Loss: 0.0142, Val Loss: 0.0105\n",
      "Epoch [158/1000] Fold 4, Train Loss: 0.0143, Val Loss: 0.0106\n",
      "Epoch [159/1000] Fold 4, Train Loss: 0.0146, Val Loss: 0.0108\n",
      "Epoch [160/1000] Fold 4, Train Loss: 0.0142, Val Loss: 0.0106\n",
      "Epoch [161/1000] Fold 4, Train Loss: 0.0142, Val Loss: 0.0106\n",
      "Epoch [162/1000] Fold 4, Train Loss: 0.0141, Val Loss: 0.0106\n",
      "Epoch [163/1000] Fold 4, Train Loss: 0.0142, Val Loss: 0.0105\n",
      "Epoch [164/1000] Fold 4, Train Loss: 0.0143, Val Loss: 0.0105\n",
      "Epoch [165/1000] Fold 4, Train Loss: 0.0142, Val Loss: 0.0108\n",
      "Epoch [166/1000] Fold 4, Train Loss: 0.0141, Val Loss: 0.0104\n",
      "Epoch [167/1000] Fold 4, Train Loss: 0.0141, Val Loss: 0.0106\n",
      "Epoch [168/1000] Fold 4, Train Loss: 0.0141, Val Loss: 0.0106\n",
      "Epoch [169/1000] Fold 4, Train Loss: 0.0139, Val Loss: 0.0105\n",
      "Epoch [170/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0104\n",
      "Epoch [171/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0105\n",
      "Epoch [172/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0106\n",
      "Epoch [173/1000] Fold 4, Train Loss: 0.0139, Val Loss: 0.0104\n",
      "Epoch [174/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0105\n",
      "Epoch [175/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0104\n",
      "Epoch [176/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0103\n",
      "Epoch [177/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [178/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0103\n",
      "Epoch [179/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0103\n",
      "Epoch [180/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0102\n",
      "Epoch [181/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0102\n",
      "Epoch [182/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0102\n",
      "Epoch [183/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0103\n",
      "Epoch [184/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0103\n",
      "Epoch [185/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0102\n",
      "Epoch [186/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0101\n",
      "Epoch [187/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0103\n",
      "Epoch [188/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0102\n",
      "Epoch [189/1000] Fold 4, Train Loss: 0.0140, Val Loss: 0.0100\n",
      "Epoch [190/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0101\n",
      "Epoch [191/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0101\n",
      "Epoch [192/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0101\n",
      "Epoch [193/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0101\n",
      "Epoch [194/1000] Fold 4, Train Loss: 0.0138, Val Loss: 0.0101\n",
      "Epoch [195/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0102\n",
      "Epoch [196/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0102\n",
      "Epoch [197/1000] Fold 4, Train Loss: 0.0137, Val Loss: 0.0100\n",
      "Epoch [198/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0101\n",
      "Epoch [199/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0101\n",
      "Epoch [200/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0100\n",
      "Epoch [201/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0100\n",
      "Epoch [202/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0100\n",
      "Epoch [203/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0099\n",
      "Epoch [204/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0101\n",
      "Epoch [205/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0100\n",
      "Epoch [206/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0099\n",
      "Epoch [207/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0101\n",
      "Epoch [208/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0100\n",
      "Epoch [209/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0100\n",
      "Epoch [210/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0098\n",
      "Epoch [211/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0100\n",
      "Epoch [212/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0099\n",
      "Epoch [213/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0099\n",
      "Epoch [214/1000] Fold 4, Train Loss: 0.0135, Val Loss: 0.0100\n",
      "Epoch [215/1000] Fold 4, Train Loss: 0.0133, Val Loss: 0.0098\n",
      "Epoch [216/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [217/1000] Fold 4, Train Loss: 0.0133, Val Loss: 0.0098\n",
      "Epoch [218/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0098\n",
      "Epoch [219/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0099\n",
      "Epoch [220/1000] Fold 4, Train Loss: 0.0136, Val Loss: 0.0098\n",
      "Epoch [221/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0100\n",
      "Epoch [222/1000] Fold 4, Train Loss: 0.0133, Val Loss: 0.0098\n",
      "Epoch [223/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0098\n",
      "Epoch [224/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0097\n",
      "Epoch [225/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0098\n",
      "Epoch [226/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0099\n",
      "Epoch [227/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [228/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0098\n",
      "Epoch [229/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [230/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0097\n",
      "Epoch [231/1000] Fold 4, Train Loss: 0.0134, Val Loss: 0.0099\n",
      "Epoch [232/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0097\n",
      "Epoch [233/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0097\n",
      "Epoch [234/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0098\n",
      "Epoch [235/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [236/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0095\n",
      "Epoch [237/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0097\n",
      "Epoch [238/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0097\n",
      "Epoch [239/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0098\n",
      "Epoch [240/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0095\n",
      "Epoch [241/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [242/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0095\n",
      "Epoch [243/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0097\n",
      "Epoch [244/1000] Fold 4, Train Loss: 0.0131, Val Loss: 0.0097\n",
      "Epoch [245/1000] Fold 4, Train Loss: 0.0130, Val Loss: 0.0095\n",
      "Epoch [246/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0097\n",
      "Epoch [247/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [248/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0095\n",
      "Epoch [249/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0096\n",
      "Epoch [250/1000] Fold 4, Train Loss: 0.0132, Val Loss: 0.0096\n",
      "Epoch [251/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [252/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [253/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [254/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [255/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0096\n",
      "Epoch [256/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [257/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0095\n",
      "Epoch [258/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [259/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0094\n",
      "Epoch [260/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [261/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0095\n",
      "Epoch [262/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0096\n",
      "Epoch [263/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0095\n",
      "Epoch [264/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0096\n",
      "Epoch [265/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [266/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0094\n",
      "Epoch [267/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0094\n",
      "Epoch [268/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0094\n",
      "Epoch [269/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0096\n",
      "Epoch [270/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0094\n",
      "Epoch [271/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [272/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [273/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [274/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [275/1000] Fold 4, Train Loss: 0.0128, Val Loss: 0.0095\n",
      "Epoch [276/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0094\n",
      "Epoch [277/1000] Fold 4, Train Loss: 0.0129, Val Loss: 0.0095\n",
      "Epoch [278/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0094\n",
      "Epoch [279/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0094\n",
      "Epoch [280/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0093\n",
      "Epoch [281/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0093\n",
      "Epoch [282/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [283/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [284/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [285/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [286/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [287/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0093\n",
      "Epoch [288/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [289/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0094\n",
      "Epoch [290/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [291/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0094\n",
      "Epoch [292/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [293/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [294/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [295/1000] Fold 4, Train Loss: 0.0126, Val Loss: 0.0092\n",
      "Epoch [296/1000] Fold 4, Train Loss: 0.0127, Val Loss: 0.0092\n",
      "Epoch [297/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [298/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [299/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0092\n",
      "Epoch [300/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0092\n",
      "Epoch [301/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0092\n",
      "Epoch [302/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0093\n",
      "Epoch [303/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [304/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [305/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [306/1000] Fold 4, Train Loss: 0.0125, Val Loss: 0.0093\n",
      "Epoch [307/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [308/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0093\n",
      "Epoch [309/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0092\n",
      "Epoch [310/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0092\n",
      "Epoch [311/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [312/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [313/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0091\n",
      "Epoch [314/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0092\n",
      "Epoch [315/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0090\n",
      "Epoch [316/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0090\n",
      "Epoch [317/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0091\n",
      "Epoch [318/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [319/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [320/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [321/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0091\n",
      "Epoch [322/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0093\n",
      "Epoch [323/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0092\n",
      "Epoch [324/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [325/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [326/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [327/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0092\n",
      "Epoch [328/1000] Fold 4, Train Loss: 0.0123, Val Loss: 0.0091\n",
      "Epoch [329/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [330/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [331/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0090\n",
      "Epoch [332/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [333/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [334/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [335/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [336/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0091\n",
      "Epoch [337/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [338/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [339/1000] Fold 4, Train Loss: 0.0124, Val Loss: 0.0089\n",
      "Epoch [340/1000] Fold 4, Train Loss: 0.0122, Val Loss: 0.0091\n",
      "Epoch [341/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [342/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [343/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0091\n",
      "Epoch [344/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [345/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [346/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [347/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [348/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [349/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [350/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [351/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [352/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [353/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0091\n",
      "Epoch [354/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [355/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [356/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [357/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [358/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0089\n",
      "Epoch [359/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0090\n",
      "Epoch [360/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [361/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [362/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [363/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [364/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [365/1000] Fold 4, Train Loss: 0.0121, Val Loss: 0.0089\n",
      "Epoch [366/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0090\n",
      "Epoch [367/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [368/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [369/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [370/1000] Fold 4, Train Loss: 0.0120, Val Loss: 0.0090\n",
      "Epoch [371/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [372/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [373/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0089\n",
      "Epoch [374/1000] Fold 4, Train Loss: 0.0119, Val Loss: 0.0089\n",
      "Epoch [375/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [376/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [377/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0088\n",
      "Epoch [378/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [379/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [380/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [381/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [382/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [383/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [384/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [385/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [386/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [387/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [388/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [389/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [390/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [391/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [392/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0088\n",
      "Epoch [393/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [394/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [395/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0088\n",
      "Epoch [396/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [397/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [398/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0090\n",
      "Epoch [399/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [400/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [401/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [402/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [403/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [404/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0089\n",
      "Epoch [405/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [406/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [407/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [408/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [409/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0089\n",
      "Epoch [410/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [411/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [412/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0088\n",
      "Epoch [413/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [414/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [415/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0088\n",
      "Epoch [416/1000] Fold 4, Train Loss: 0.0118, Val Loss: 0.0088\n",
      "Epoch [417/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [418/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [419/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [420/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [421/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [422/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0087\n",
      "Epoch [423/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [424/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [425/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [426/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0087\n",
      "Epoch [427/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [428/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [429/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [430/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [431/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [432/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [433/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [434/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0086\n",
      "Epoch [435/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [436/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0088\n",
      "Epoch [437/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [438/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [439/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0086\n",
      "Epoch [440/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0086\n",
      "Epoch [441/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [442/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [443/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [444/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [445/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [446/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [447/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0086\n",
      "Epoch [448/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [449/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [450/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [451/1000] Fold 4, Train Loss: 0.0117, Val Loss: 0.0087\n",
      "Epoch [452/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0087\n",
      "Epoch [453/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [454/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [455/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [456/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [457/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [458/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [459/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [460/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [461/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0087\n",
      "Epoch [462/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [463/1000] Fold 4, Train Loss: 0.0116, Val Loss: 0.0087\n",
      "Epoch [464/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [465/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [466/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [467/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [468/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [469/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [470/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [471/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [472/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [473/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [474/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0086\n",
      "Epoch [475/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [476/1000] Fold 4, Train Loss: 0.0115, Val Loss: 0.0087\n",
      "Epoch [477/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0085\n",
      "Epoch [478/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [479/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [480/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [481/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [482/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [483/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [484/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0086\n",
      "Epoch [485/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [486/1000] Fold 4, Train Loss: 0.0114, Val Loss: 0.0086\n",
      "Epoch [487/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [488/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [489/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [490/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [491/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0085\n",
      "Epoch [492/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0085\n",
      "Epoch [493/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0086\n",
      "Epoch [494/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [495/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [496/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [497/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [498/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [499/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [500/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [501/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [502/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [503/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [504/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [505/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [506/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0087\n",
      "Epoch [507/1000] Fold 4, Train Loss: 0.0113, Val Loss: 0.0086\n",
      "Epoch [508/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [509/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [510/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [511/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [512/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [513/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [514/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [515/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [516/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [517/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [518/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [519/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [520/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [521/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [522/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0084\n",
      "Epoch [523/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [524/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [525/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [526/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [527/1000] Fold 4, Train Loss: 0.0112, Val Loss: 0.0084\n",
      "Epoch [528/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [529/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [530/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [531/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [532/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [533/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0084\n",
      "Epoch [534/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [535/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [536/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [537/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [538/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [539/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [540/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0084\n",
      "Epoch [541/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [542/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [543/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [544/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [545/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [546/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [547/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [548/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [549/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [550/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [551/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [552/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [553/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0083\n",
      "Epoch [554/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [555/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [556/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [557/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [558/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [559/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0085\n",
      "Epoch [560/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [561/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [562/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [563/1000] Fold 4, Train Loss: 0.0111, Val Loss: 0.0084\n",
      "Epoch [564/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [565/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [566/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [567/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [568/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [569/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [570/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [571/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [572/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [573/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [574/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [575/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [576/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [577/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [578/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [579/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [580/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0084\n",
      "Epoch [581/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [582/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [583/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [584/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [585/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [586/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [587/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [588/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [589/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [590/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [591/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [592/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [593/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [594/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [595/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [596/1000] Fold 4, Train Loss: 0.0110, Val Loss: 0.0084\n",
      "Epoch [597/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [598/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [599/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [600/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [601/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [602/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [603/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [604/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [605/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [606/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [607/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [608/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [609/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [610/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [611/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [612/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [613/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [614/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [615/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [616/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [617/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [618/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [619/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [620/1000] Fold 4, Train Loss: 0.0109, Val Loss: 0.0082\n",
      "Epoch [621/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [622/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [623/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [624/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [625/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [626/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [627/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [628/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [629/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [630/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [631/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0084\n",
      "Epoch [632/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [633/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [634/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0083\n",
      "Epoch [635/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [636/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [637/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [638/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [639/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [640/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [641/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [642/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [643/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [644/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [645/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [646/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [647/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [648/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0083\n",
      "Epoch [649/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [650/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [651/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [652/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [653/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [654/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [655/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [656/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0084\n",
      "Epoch [657/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0083\n",
      "Epoch [658/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [659/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [660/1000] Fold 4, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [661/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [662/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [663/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [664/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [665/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [666/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [667/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [668/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [669/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [670/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [671/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [672/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [673/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [674/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [675/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [676/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [677/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [678/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0083\n",
      "Epoch [679/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [680/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [681/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [682/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [683/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [684/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [685/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [686/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [687/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [688/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [689/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [690/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [691/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [692/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [693/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [694/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [695/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [696/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [697/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [698/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [699/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [700/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [701/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [702/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [703/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [704/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [705/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [706/1000] Fold 4, Train Loss: 0.0107, Val Loss: 0.0082\n",
      "Epoch [707/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [708/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [709/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [710/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [711/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [712/1000] Fold 4, Train Loss: 0.0106, Val Loss: 0.0082\n",
      "Epoch [713/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [714/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [715/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [716/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [717/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0082\n",
      "Epoch [718/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0082\n",
      "Epoch [719/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [720/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [721/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [722/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [723/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [724/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [725/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [726/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [727/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [728/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [729/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [730/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [731/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [732/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [733/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [734/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [735/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [736/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [737/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [738/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [739/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [740/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [741/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [742/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [743/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [744/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [745/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [746/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [747/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [748/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [749/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [750/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [751/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [752/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [753/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0083\n",
      "Epoch [754/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0082\n",
      "Epoch [755/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0081\n",
      "Epoch [756/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [757/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [758/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [759/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [760/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [761/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [762/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [763/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [764/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [765/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [766/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [767/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [768/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [769/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [770/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [771/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0082\n",
      "Epoch [772/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [773/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [774/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [775/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [776/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [777/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [778/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [779/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [780/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [781/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [782/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [783/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [784/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [785/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [786/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [787/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [788/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [789/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [790/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [791/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [792/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [793/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [794/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [795/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [796/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [797/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [798/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [799/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [800/1000] Fold 4, Train Loss: 0.0105, Val Loss: 0.0081\n",
      "Epoch [801/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [802/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0081\n",
      "Epoch [803/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [804/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [805/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [806/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [807/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [808/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [809/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [810/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [811/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [812/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [813/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [814/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [815/1000] Fold 4, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [816/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [817/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [818/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [819/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [820/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [821/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [822/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [823/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [824/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [825/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [826/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [827/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [828/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [829/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [830/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [831/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [832/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0081\n",
      "Epoch [833/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [834/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [835/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [836/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [837/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [838/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [839/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [840/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [841/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [842/1000] Fold 4, Train Loss: 0.0103, Val Loss: 0.0082\n",
      "Epoch [843/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [844/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [845/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [846/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [847/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [848/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [849/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [850/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [851/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [852/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [853/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0080\n",
      "Epoch [854/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [855/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [856/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [857/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [858/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [859/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [860/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [861/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [862/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [863/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [864/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [865/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [866/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [867/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [868/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [869/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [870/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [871/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [872/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [873/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [874/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [875/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [876/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [877/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [878/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [879/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [880/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [881/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [882/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [883/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [884/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [885/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [886/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [887/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [888/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [889/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [890/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [891/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [892/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [893/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [894/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [895/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [896/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [897/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [898/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [899/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [900/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [901/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [902/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [903/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [904/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [905/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [906/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [907/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [908/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [909/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [910/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [911/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [912/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [913/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [914/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [915/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [916/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [917/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0081\n",
      "Epoch [918/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0081\n",
      "Epoch [919/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0080\n",
      "Epoch [920/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [921/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0080\n",
      "Epoch [922/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [923/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [924/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0080\n",
      "Epoch [925/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0080\n",
      "Epoch [926/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [927/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0080\n",
      "Epoch [928/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0080\n",
      "Epoch [929/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [930/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [931/1000] Fold 4, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [932/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [933/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [934/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [935/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [936/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0080\n",
      "Epoch [937/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [938/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [939/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [940/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [941/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [942/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [943/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0080\n",
      "Epoch [944/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [945/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [946/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [947/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [948/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0081\n",
      "Epoch [949/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0080\n",
      "Epoch [950/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [951/1000] Fold 4, Train Loss: 0.0101, Val Loss: 0.0080\n",
      "Epoch [952/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0080\n",
      "Epoch [953/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [954/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [955/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0080\n",
      "Epoch [956/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [957/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [958/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [959/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [960/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [961/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [962/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [963/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [964/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [965/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [966/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [967/1000] Fold 4, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [968/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [969/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [970/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [971/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [972/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [973/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [974/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [975/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [976/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [977/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [978/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [979/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [980/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [981/1000] Fold 4, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [982/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [983/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [984/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [985/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [986/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [987/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [988/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [989/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [990/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [991/1000] Fold 4, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [992/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [993/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [994/1000] Fold 4, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [995/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [996/1000] Fold 4, Train Loss: 0.0098, Val Loss: 0.0080\n",
      "Epoch [997/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Epoch [998/1000] Fold 4, Train Loss: 0.0097, Val Loss: 0.0080\n",
      "Epoch [999/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [1000/1000] Fold 4, Train Loss: 0.0099, Val Loss: 0.0081\n",
      "Fold 5/10\n",
      "Epoch [1/1000] Fold 5, Train Loss: 0.1386, Val Loss: 0.0639\n",
      "Epoch [2/1000] Fold 5, Train Loss: 0.0723, Val Loss: 0.0425\n",
      "Epoch [3/1000] Fold 5, Train Loss: 0.0511, Val Loss: 0.0343\n",
      "Epoch [4/1000] Fold 5, Train Loss: 0.0411, Val Loss: 0.0299\n",
      "Epoch [5/1000] Fold 5, Train Loss: 0.0361, Val Loss: 0.0279\n",
      "Epoch [6/1000] Fold 5, Train Loss: 0.0333, Val Loss: 0.0268\n",
      "Epoch [7/1000] Fold 5, Train Loss: 0.0316, Val Loss: 0.0260\n",
      "Epoch [8/1000] Fold 5, Train Loss: 0.0305, Val Loss: 0.0256\n",
      "Epoch [9/1000] Fold 5, Train Loss: 0.0292, Val Loss: 0.0250\n",
      "Epoch [10/1000] Fold 5, Train Loss: 0.0286, Val Loss: 0.0246\n",
      "Epoch [11/1000] Fold 5, Train Loss: 0.0279, Val Loss: 0.0241\n",
      "Epoch [12/1000] Fold 5, Train Loss: 0.0276, Val Loss: 0.0239\n",
      "Epoch [13/1000] Fold 5, Train Loss: 0.0270, Val Loss: 0.0231\n",
      "Epoch [14/1000] Fold 5, Train Loss: 0.0263, Val Loss: 0.0228\n",
      "Epoch [15/1000] Fold 5, Train Loss: 0.0262, Val Loss: 0.0222\n",
      "Epoch [16/1000] Fold 5, Train Loss: 0.0258, Val Loss: 0.0219\n",
      "Epoch [17/1000] Fold 5, Train Loss: 0.0251, Val Loss: 0.0217\n",
      "Epoch [18/1000] Fold 5, Train Loss: 0.0249, Val Loss: 0.0212\n",
      "Epoch [19/1000] Fold 5, Train Loss: 0.0244, Val Loss: 0.0209\n",
      "Epoch [20/1000] Fold 5, Train Loss: 0.0242, Val Loss: 0.0207\n",
      "Epoch [21/1000] Fold 5, Train Loss: 0.0238, Val Loss: 0.0204\n",
      "Epoch [22/1000] Fold 5, Train Loss: 0.0235, Val Loss: 0.0200\n",
      "Epoch [23/1000] Fold 5, Train Loss: 0.0232, Val Loss: 0.0200\n",
      "Epoch [24/1000] Fold 5, Train Loss: 0.0232, Val Loss: 0.0195\n",
      "Epoch [25/1000] Fold 5, Train Loss: 0.0226, Val Loss: 0.0192\n",
      "Epoch [26/1000] Fold 5, Train Loss: 0.0228, Val Loss: 0.0189\n",
      "Epoch [27/1000] Fold 5, Train Loss: 0.0223, Val Loss: 0.0189\n",
      "Epoch [28/1000] Fold 5, Train Loss: 0.0223, Val Loss: 0.0186\n",
      "Epoch [29/1000] Fold 5, Train Loss: 0.0218, Val Loss: 0.0185\n",
      "Epoch [30/1000] Fold 5, Train Loss: 0.0217, Val Loss: 0.0181\n",
      "Epoch [31/1000] Fold 5, Train Loss: 0.0216, Val Loss: 0.0182\n",
      "Epoch [32/1000] Fold 5, Train Loss: 0.0212, Val Loss: 0.0178\n",
      "Epoch [33/1000] Fold 5, Train Loss: 0.0213, Val Loss: 0.0177\n",
      "Epoch [34/1000] Fold 5, Train Loss: 0.0210, Val Loss: 0.0176\n",
      "Epoch [35/1000] Fold 5, Train Loss: 0.0209, Val Loss: 0.0173\n",
      "Epoch [36/1000] Fold 5, Train Loss: 0.0209, Val Loss: 0.0173\n",
      "Epoch [37/1000] Fold 5, Train Loss: 0.0203, Val Loss: 0.0170\n",
      "Epoch [38/1000] Fold 5, Train Loss: 0.0203, Val Loss: 0.0167\n",
      "Epoch [39/1000] Fold 5, Train Loss: 0.0199, Val Loss: 0.0169\n",
      "Epoch [40/1000] Fold 5, Train Loss: 0.0198, Val Loss: 0.0165\n",
      "Epoch [41/1000] Fold 5, Train Loss: 0.0199, Val Loss: 0.0166\n",
      "Epoch [42/1000] Fold 5, Train Loss: 0.0199, Val Loss: 0.0165\n",
      "Epoch [43/1000] Fold 5, Train Loss: 0.0198, Val Loss: 0.0163\n",
      "Epoch [44/1000] Fold 5, Train Loss: 0.0195, Val Loss: 0.0163\n",
      "Epoch [45/1000] Fold 5, Train Loss: 0.0193, Val Loss: 0.0160\n",
      "Epoch [46/1000] Fold 5, Train Loss: 0.0193, Val Loss: 0.0161\n",
      "Epoch [47/1000] Fold 5, Train Loss: 0.0190, Val Loss: 0.0157\n",
      "Epoch [48/1000] Fold 5, Train Loss: 0.0191, Val Loss: 0.0156\n",
      "Epoch [49/1000] Fold 5, Train Loss: 0.0189, Val Loss: 0.0156\n",
      "Epoch [50/1000] Fold 5, Train Loss: 0.0190, Val Loss: 0.0154\n",
      "Epoch [51/1000] Fold 5, Train Loss: 0.0187, Val Loss: 0.0153\n",
      "Epoch [52/1000] Fold 5, Train Loss: 0.0187, Val Loss: 0.0154\n",
      "Epoch [53/1000] Fold 5, Train Loss: 0.0187, Val Loss: 0.0154\n",
      "Epoch [54/1000] Fold 5, Train Loss: 0.0186, Val Loss: 0.0152\n",
      "Epoch [55/1000] Fold 5, Train Loss: 0.0183, Val Loss: 0.0152\n",
      "Epoch [56/1000] Fold 5, Train Loss: 0.0182, Val Loss: 0.0149\n",
      "Epoch [57/1000] Fold 5, Train Loss: 0.0182, Val Loss: 0.0151\n",
      "Epoch [58/1000] Fold 5, Train Loss: 0.0179, Val Loss: 0.0149\n",
      "Epoch [59/1000] Fold 5, Train Loss: 0.0180, Val Loss: 0.0147\n",
      "Epoch [60/1000] Fold 5, Train Loss: 0.0180, Val Loss: 0.0149\n",
      "Epoch [61/1000] Fold 5, Train Loss: 0.0181, Val Loss: 0.0148\n",
      "Epoch [62/1000] Fold 5, Train Loss: 0.0179, Val Loss: 0.0146\n",
      "Epoch [63/1000] Fold 5, Train Loss: 0.0176, Val Loss: 0.0146\n",
      "Epoch [64/1000] Fold 5, Train Loss: 0.0177, Val Loss: 0.0145\n",
      "Epoch [65/1000] Fold 5, Train Loss: 0.0178, Val Loss: 0.0144\n",
      "Epoch [66/1000] Fold 5, Train Loss: 0.0176, Val Loss: 0.0143\n",
      "Epoch [67/1000] Fold 5, Train Loss: 0.0173, Val Loss: 0.0140\n",
      "Epoch [68/1000] Fold 5, Train Loss: 0.0174, Val Loss: 0.0143\n",
      "Epoch [69/1000] Fold 5, Train Loss: 0.0174, Val Loss: 0.0141\n",
      "Epoch [70/1000] Fold 5, Train Loss: 0.0174, Val Loss: 0.0142\n",
      "Epoch [71/1000] Fold 5, Train Loss: 0.0173, Val Loss: 0.0139\n",
      "Epoch [72/1000] Fold 5, Train Loss: 0.0172, Val Loss: 0.0139\n",
      "Epoch [73/1000] Fold 5, Train Loss: 0.0169, Val Loss: 0.0138\n",
      "Epoch [74/1000] Fold 5, Train Loss: 0.0169, Val Loss: 0.0136\n",
      "Epoch [75/1000] Fold 5, Train Loss: 0.0172, Val Loss: 0.0136\n",
      "Epoch [76/1000] Fold 5, Train Loss: 0.0169, Val Loss: 0.0137\n",
      "Epoch [77/1000] Fold 5, Train Loss: 0.0167, Val Loss: 0.0134\n",
      "Epoch [78/1000] Fold 5, Train Loss: 0.0167, Val Loss: 0.0136\n",
      "Epoch [79/1000] Fold 5, Train Loss: 0.0170, Val Loss: 0.0136\n",
      "Epoch [80/1000] Fold 5, Train Loss: 0.0167, Val Loss: 0.0134\n",
      "Epoch [81/1000] Fold 5, Train Loss: 0.0166, Val Loss: 0.0134\n",
      "Epoch [82/1000] Fold 5, Train Loss: 0.0167, Val Loss: 0.0136\n",
      "Epoch [83/1000] Fold 5, Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Epoch [84/1000] Fold 5, Train Loss: 0.0166, Val Loss: 0.0132\n",
      "Epoch [85/1000] Fold 5, Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Epoch [86/1000] Fold 5, Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Epoch [87/1000] Fold 5, Train Loss: 0.0165, Val Loss: 0.0133\n",
      "Epoch [88/1000] Fold 5, Train Loss: 0.0164, Val Loss: 0.0129\n",
      "Epoch [89/1000] Fold 5, Train Loss: 0.0163, Val Loss: 0.0131\n",
      "Epoch [90/1000] Fold 5, Train Loss: 0.0165, Val Loss: 0.0131\n",
      "Epoch [91/1000] Fold 5, Train Loss: 0.0163, Val Loss: 0.0131\n",
      "Epoch [92/1000] Fold 5, Train Loss: 0.0162, Val Loss: 0.0131\n",
      "Epoch [93/1000] Fold 5, Train Loss: 0.0161, Val Loss: 0.0129\n",
      "Epoch [94/1000] Fold 5, Train Loss: 0.0159, Val Loss: 0.0128\n",
      "Epoch [95/1000] Fold 5, Train Loss: 0.0159, Val Loss: 0.0128\n",
      "Epoch [96/1000] Fold 5, Train Loss: 0.0160, Val Loss: 0.0129\n",
      "Epoch [97/1000] Fold 5, Train Loss: 0.0158, Val Loss: 0.0128\n",
      "Epoch [98/1000] Fold 5, Train Loss: 0.0158, Val Loss: 0.0129\n",
      "Epoch [99/1000] Fold 5, Train Loss: 0.0159, Val Loss: 0.0128\n",
      "Epoch [100/1000] Fold 5, Train Loss: 0.0163, Val Loss: 0.0125\n",
      "Epoch [101/1000] Fold 5, Train Loss: 0.0158, Val Loss: 0.0128\n",
      "Epoch [102/1000] Fold 5, Train Loss: 0.0157, Val Loss: 0.0126\n",
      "Epoch [103/1000] Fold 5, Train Loss: 0.0156, Val Loss: 0.0128\n",
      "Epoch [104/1000] Fold 5, Train Loss: 0.0158, Val Loss: 0.0125\n",
      "Epoch [105/1000] Fold 5, Train Loss: 0.0159, Val Loss: 0.0125\n",
      "Epoch [106/1000] Fold 5, Train Loss: 0.0155, Val Loss: 0.0124\n",
      "Epoch [107/1000] Fold 5, Train Loss: 0.0155, Val Loss: 0.0123\n",
      "Epoch [108/1000] Fold 5, Train Loss: 0.0155, Val Loss: 0.0126\n",
      "Epoch [109/1000] Fold 5, Train Loss: 0.0158, Val Loss: 0.0122\n",
      "Epoch [110/1000] Fold 5, Train Loss: 0.0157, Val Loss: 0.0123\n",
      "Epoch [111/1000] Fold 5, Train Loss: 0.0155, Val Loss: 0.0122\n",
      "Epoch [112/1000] Fold 5, Train Loss: 0.0156, Val Loss: 0.0121\n",
      "Epoch [113/1000] Fold 5, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [114/1000] Fold 5, Train Loss: 0.0156, Val Loss: 0.0120\n",
      "Epoch [115/1000] Fold 5, Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [116/1000] Fold 5, Train Loss: 0.0153, Val Loss: 0.0122\n",
      "Epoch [117/1000] Fold 5, Train Loss: 0.0155, Val Loss: 0.0123\n",
      "Epoch [118/1000] Fold 5, Train Loss: 0.0153, Val Loss: 0.0120\n",
      "Epoch [119/1000] Fold 5, Train Loss: 0.0149, Val Loss: 0.0122\n",
      "Epoch [120/1000] Fold 5, Train Loss: 0.0151, Val Loss: 0.0119\n",
      "Epoch [121/1000] Fold 5, Train Loss: 0.0151, Val Loss: 0.0119\n",
      "Epoch [122/1000] Fold 5, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [123/1000] Fold 5, Train Loss: 0.0151, Val Loss: 0.0119\n",
      "Epoch [124/1000] Fold 5, Train Loss: 0.0149, Val Loss: 0.0118\n",
      "Epoch [125/1000] Fold 5, Train Loss: 0.0150, Val Loss: 0.0120\n",
      "Epoch [126/1000] Fold 5, Train Loss: 0.0149, Val Loss: 0.0120\n",
      "Epoch [127/1000] Fold 5, Train Loss: 0.0148, Val Loss: 0.0120\n",
      "Epoch [128/1000] Fold 5, Train Loss: 0.0150, Val Loss: 0.0118\n",
      "Epoch [129/1000] Fold 5, Train Loss: 0.0152, Val Loss: 0.0119\n",
      "Epoch [130/1000] Fold 5, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [131/1000] Fold 5, Train Loss: 0.0149, Val Loss: 0.0117\n",
      "Epoch [132/1000] Fold 5, Train Loss: 0.0150, Val Loss: 0.0117\n",
      "Epoch [133/1000] Fold 5, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [134/1000] Fold 5, Train Loss: 0.0146, Val Loss: 0.0117\n",
      "Epoch [135/1000] Fold 5, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [136/1000] Fold 5, Train Loss: 0.0148, Val Loss: 0.0118\n",
      "Epoch [137/1000] Fold 5, Train Loss: 0.0144, Val Loss: 0.0115\n",
      "Epoch [138/1000] Fold 5, Train Loss: 0.0149, Val Loss: 0.0116\n",
      "Epoch [139/1000] Fold 5, Train Loss: 0.0146, Val Loss: 0.0117\n",
      "Epoch [140/1000] Fold 5, Train Loss: 0.0146, Val Loss: 0.0116\n",
      "Epoch [141/1000] Fold 5, Train Loss: 0.0145, Val Loss: 0.0116\n",
      "Epoch [142/1000] Fold 5, Train Loss: 0.0145, Val Loss: 0.0118\n",
      "Epoch [143/1000] Fold 5, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [144/1000] Fold 5, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [145/1000] Fold 5, Train Loss: 0.0144, Val Loss: 0.0115\n",
      "Epoch [146/1000] Fold 5, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [147/1000] Fold 5, Train Loss: 0.0143, Val Loss: 0.0111\n",
      "Epoch [148/1000] Fold 5, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [149/1000] Fold 5, Train Loss: 0.0142, Val Loss: 0.0113\n",
      "Epoch [150/1000] Fold 5, Train Loss: 0.0142, Val Loss: 0.0115\n",
      "Epoch [151/1000] Fold 5, Train Loss: 0.0141, Val Loss: 0.0113\n",
      "Epoch [152/1000] Fold 5, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [153/1000] Fold 5, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [154/1000] Fold 5, Train Loss: 0.0143, Val Loss: 0.0112\n",
      "Epoch [155/1000] Fold 5, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [156/1000] Fold 5, Train Loss: 0.0142, Val Loss: 0.0113\n",
      "Epoch [157/1000] Fold 5, Train Loss: 0.0144, Val Loss: 0.0111\n",
      "Epoch [158/1000] Fold 5, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [159/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0113\n",
      "Epoch [160/1000] Fold 5, Train Loss: 0.0139, Val Loss: 0.0112\n",
      "Epoch [161/1000] Fold 5, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [162/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [163/1000] Fold 5, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [164/1000] Fold 5, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [165/1000] Fold 5, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [166/1000] Fold 5, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [167/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [168/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0111\n",
      "Epoch [169/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [170/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [171/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [172/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0110\n",
      "Epoch [173/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [174/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [175/1000] Fold 5, Train Loss: 0.0139, Val Loss: 0.0110\n",
      "Epoch [176/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0110\n",
      "Epoch [177/1000] Fold 5, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [178/1000] Fold 5, Train Loss: 0.0135, Val Loss: 0.0108\n",
      "Epoch [179/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0109\n",
      "Epoch [180/1000] Fold 5, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [181/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0111\n",
      "Epoch [182/1000] Fold 5, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [183/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [184/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [185/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [186/1000] Fold 5, Train Loss: 0.0137, Val Loss: 0.0106\n",
      "Epoch [187/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [188/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [189/1000] Fold 5, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [190/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [191/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [192/1000] Fold 5, Train Loss: 0.0136, Val Loss: 0.0110\n",
      "Epoch [193/1000] Fold 5, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [194/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0107\n",
      "Epoch [195/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0106\n",
      "Epoch [196/1000] Fold 5, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [197/1000] Fold 5, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [198/1000] Fold 5, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [199/1000] Fold 5, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [200/1000] Fold 5, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [201/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [202/1000] Fold 5, Train Loss: 0.0134, Val Loss: 0.0106\n",
      "Epoch [203/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0107\n",
      "Epoch [204/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0106\n",
      "Epoch [205/1000] Fold 5, Train Loss: 0.0135, Val Loss: 0.0105\n",
      "Epoch [206/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [207/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [208/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0107\n",
      "Epoch [209/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0105\n",
      "Epoch [210/1000] Fold 5, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [211/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [212/1000] Fold 5, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [213/1000] Fold 5, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [214/1000] Fold 5, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [215/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [216/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [217/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [218/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [219/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [220/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0106\n",
      "Epoch [221/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [222/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [223/1000] Fold 5, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [224/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [225/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [226/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [227/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [228/1000] Fold 5, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [229/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [230/1000] Fold 5, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [231/1000] Fold 5, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [232/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [233/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [234/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [235/1000] Fold 5, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [236/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [237/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [238/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [239/1000] Fold 5, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [240/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [241/1000] Fold 5, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [242/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [243/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [244/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [245/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [246/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [247/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [248/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [249/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [250/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [251/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [252/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [253/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [254/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [255/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [256/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0102\n",
      "Epoch [257/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [258/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [259/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [260/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [261/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [262/1000] Fold 5, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [263/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [264/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [265/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [266/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [267/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [268/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [269/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [270/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [271/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [272/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [273/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [274/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [275/1000] Fold 5, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [276/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [277/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [278/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0101\n",
      "Epoch [279/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [280/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [281/1000] Fold 5, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [282/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [283/1000] Fold 5, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [284/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [285/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [286/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [287/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [288/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [289/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [290/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [291/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [292/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [293/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [294/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [295/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [296/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [297/1000] Fold 5, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [298/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [299/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [300/1000] Fold 5, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [301/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [302/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [303/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [304/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [305/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [306/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [307/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [308/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [309/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [310/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [311/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [312/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0098\n",
      "Epoch [313/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [314/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [315/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [316/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [317/1000] Fold 5, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [318/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0098\n",
      "Epoch [319/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [320/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [321/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [322/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [323/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [324/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [325/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [326/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [327/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [328/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [329/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [330/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [331/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [332/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [333/1000] Fold 5, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [334/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [335/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [336/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0098\n",
      "Epoch [337/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [338/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [339/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [340/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [341/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [342/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [343/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [344/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [345/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [346/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [347/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [348/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [349/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [350/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [351/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [352/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [353/1000] Fold 5, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [354/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [355/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [356/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [357/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [358/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [359/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [360/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [361/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [362/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [363/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [364/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [365/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [366/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [367/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [368/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [369/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [370/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [371/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [372/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [373/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [374/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [375/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [376/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [377/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [378/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [379/1000] Fold 5, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [380/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [381/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [382/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [383/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [384/1000] Fold 5, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [385/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [386/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [387/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [388/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [389/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [390/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [391/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [392/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [393/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [394/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [395/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [396/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [397/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [398/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [399/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [400/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [401/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [402/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [403/1000] Fold 5, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [404/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [405/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [406/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [407/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [408/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [409/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [410/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [411/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [412/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [413/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [414/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [415/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [416/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [417/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [418/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [419/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [420/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [421/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [422/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [423/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [424/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [425/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [426/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [427/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [428/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [429/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [430/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [431/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [432/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [433/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [434/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [435/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [436/1000] Fold 5, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [437/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [438/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [439/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [440/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [441/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [442/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [443/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [444/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [445/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [446/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [447/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [448/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [449/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [450/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [451/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [452/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [453/1000] Fold 5, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [454/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [455/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [456/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [457/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [458/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [459/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [460/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [461/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [462/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [463/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [464/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [465/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [466/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [467/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [468/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [469/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [470/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [471/1000] Fold 5, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [472/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [473/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [474/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [475/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [476/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [477/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [478/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [479/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [480/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [481/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [482/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [483/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [484/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [485/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [486/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [487/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [488/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [489/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [490/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [491/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [492/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [493/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [494/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [495/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [496/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [497/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [498/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [499/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [500/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [501/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [502/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [503/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [504/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [505/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [506/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [507/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [508/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [509/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [510/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [511/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [512/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [513/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [514/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [515/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [516/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [517/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [518/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [519/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [520/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [521/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [522/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [523/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [524/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [525/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [526/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [527/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [528/1000] Fold 5, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [529/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [530/1000] Fold 5, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [531/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [532/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [533/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [534/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [535/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [536/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [537/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [538/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [539/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [540/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [541/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [542/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [543/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [544/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [545/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [546/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [547/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [548/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [549/1000] Fold 5, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [550/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [551/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [552/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [553/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [554/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [555/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [556/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [557/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [558/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [559/1000] Fold 5, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [560/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [561/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [562/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [563/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [564/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [565/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [566/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [567/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [568/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [569/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [570/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [571/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [572/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [573/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [574/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [575/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [576/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [577/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [578/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [579/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [580/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [581/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [582/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [583/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [584/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [585/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [586/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [587/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [588/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [589/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [590/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [591/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [592/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [593/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [594/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [595/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [596/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [597/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [598/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [599/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [600/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [601/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [602/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [603/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [604/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [605/1000] Fold 5, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [606/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [607/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [608/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [609/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [610/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [611/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [612/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [613/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [614/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [615/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [616/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [617/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [618/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [619/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [620/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [621/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [622/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [623/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [624/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [625/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [626/1000] Fold 5, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [627/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [628/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [629/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [630/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [631/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [632/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [633/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [634/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [635/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [636/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [637/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [638/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [639/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [640/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [641/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [642/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [643/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [644/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [645/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [646/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [647/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [648/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [649/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [650/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [651/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [652/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [653/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [654/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [655/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [656/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [657/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [658/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [659/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [660/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [661/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [662/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [663/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [664/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [665/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [666/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [667/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [668/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [669/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [670/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [671/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [672/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [673/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [674/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [675/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [676/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [677/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [678/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [679/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [680/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [681/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [682/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [683/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [684/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [685/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [686/1000] Fold 5, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [687/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [688/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [689/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [690/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [691/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [692/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [693/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [694/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [695/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [696/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [697/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [698/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [699/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [700/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [701/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [702/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [703/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [704/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [705/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [706/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [707/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [708/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [709/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [710/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [711/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [712/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [713/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [714/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [715/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [716/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [717/1000] Fold 5, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [718/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [719/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [720/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [721/1000] Fold 5, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [722/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [723/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [724/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [725/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [726/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [727/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [728/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [729/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [730/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [731/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [732/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [733/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [734/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [735/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [736/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [737/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [738/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [739/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [740/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [741/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [742/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [743/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [744/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [745/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [746/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [747/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [748/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [749/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [750/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [751/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [752/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [753/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [754/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [755/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [756/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [757/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [758/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [759/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [760/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [761/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [762/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [763/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [764/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [765/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [766/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [767/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [768/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [769/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [770/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [771/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [772/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [773/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [774/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [775/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [776/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [777/1000] Fold 5, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [778/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [779/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [780/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [781/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [782/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [783/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [784/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [785/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [786/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [787/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [788/1000] Fold 5, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [789/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [790/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [791/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [792/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [793/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [794/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [795/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [796/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [797/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [798/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [799/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [800/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [801/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [802/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [803/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [804/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [805/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [806/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [807/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [808/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [809/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [810/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [811/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [812/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [813/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [814/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [815/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [816/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [817/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [818/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [819/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [820/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [821/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [822/1000] Fold 5, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [823/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [824/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [825/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [826/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [827/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [828/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [829/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [830/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [831/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [832/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [833/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [834/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [835/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [836/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [837/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [838/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [839/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [840/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [841/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [842/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [843/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [844/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [845/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [846/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [847/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [848/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [849/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [850/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [851/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [852/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [853/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [854/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [855/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [856/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [857/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [858/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [859/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [860/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [861/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [862/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [863/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [864/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [865/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [866/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [867/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [868/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [869/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [870/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [871/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [872/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [873/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [874/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [875/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [876/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [877/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [878/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [879/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [880/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [881/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [882/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [883/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [884/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [885/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [886/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [887/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [888/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [889/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [890/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [891/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [892/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [893/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [894/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [895/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [896/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [897/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [898/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [899/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [900/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [901/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [902/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [903/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [904/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [905/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [906/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [907/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [908/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [909/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [910/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [911/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [912/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [913/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [914/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [915/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [916/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [917/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [918/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [919/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [920/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [921/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [922/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [923/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [924/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [925/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [926/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [927/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [928/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [929/1000] Fold 5, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [930/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [931/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [932/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [933/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [934/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [935/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [936/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [937/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [938/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [939/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [940/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [941/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [942/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [943/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [944/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [945/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [946/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [947/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [948/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [949/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [950/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [951/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [952/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [953/1000] Fold 5, Train Loss: 0.0095, Val Loss: 0.0088\n",
      "Epoch [954/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [955/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [956/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [957/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [958/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [959/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [960/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [961/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [962/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [963/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [964/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [965/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [966/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [967/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [968/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [969/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [970/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [971/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [972/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [973/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [974/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [975/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [976/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [977/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [978/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [979/1000] Fold 5, Train Loss: 0.0095, Val Loss: 0.0089\n",
      "Epoch [980/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0089\n",
      "Epoch [981/1000] Fold 5, Train Loss: 0.0095, Val Loss: 0.0088\n",
      "Epoch [982/1000] Fold 5, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [983/1000] Fold 5, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [984/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [985/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [986/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [987/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [988/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [989/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [990/1000] Fold 5, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [991/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [992/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [993/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [994/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [995/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [996/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [997/1000] Fold 5, Train Loss: 0.0095, Val Loss: 0.0088\n",
      "Epoch [998/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [999/1000] Fold 5, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [1000/1000] Fold 5, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Fold 6/10\n",
      "Epoch [1/1000] Fold 6, Train Loss: 0.1433, Val Loss: 0.0694\n",
      "Epoch [2/1000] Fold 6, Train Loss: 0.0728, Val Loss: 0.0447\n",
      "Epoch [3/1000] Fold 6, Train Loss: 0.0488, Val Loss: 0.0349\n",
      "Epoch [4/1000] Fold 6, Train Loss: 0.0401, Val Loss: 0.0310\n",
      "Epoch [5/1000] Fold 6, Train Loss: 0.0352, Val Loss: 0.0291\n",
      "Epoch [6/1000] Fold 6, Train Loss: 0.0330, Val Loss: 0.0280\n",
      "Epoch [7/1000] Fold 6, Train Loss: 0.0312, Val Loss: 0.0272\n",
      "Epoch [8/1000] Fold 6, Train Loss: 0.0304, Val Loss: 0.0266\n",
      "Epoch [9/1000] Fold 6, Train Loss: 0.0297, Val Loss: 0.0261\n",
      "Epoch [10/1000] Fold 6, Train Loss: 0.0289, Val Loss: 0.0256\n",
      "Epoch [11/1000] Fold 6, Train Loss: 0.0284, Val Loss: 0.0254\n",
      "Epoch [12/1000] Fold 6, Train Loss: 0.0282, Val Loss: 0.0251\n",
      "Epoch [13/1000] Fold 6, Train Loss: 0.0276, Val Loss: 0.0247\n",
      "Epoch [14/1000] Fold 6, Train Loss: 0.0271, Val Loss: 0.0244\n",
      "Epoch [15/1000] Fold 6, Train Loss: 0.0266, Val Loss: 0.0238\n",
      "Epoch [16/1000] Fold 6, Train Loss: 0.0262, Val Loss: 0.0234\n",
      "Epoch [17/1000] Fold 6, Train Loss: 0.0262, Val Loss: 0.0233\n",
      "Epoch [18/1000] Fold 6, Train Loss: 0.0260, Val Loss: 0.0228\n",
      "Epoch [19/1000] Fold 6, Train Loss: 0.0253, Val Loss: 0.0223\n",
      "Epoch [20/1000] Fold 6, Train Loss: 0.0253, Val Loss: 0.0221\n",
      "Epoch [21/1000] Fold 6, Train Loss: 0.0247, Val Loss: 0.0217\n",
      "Epoch [22/1000] Fold 6, Train Loss: 0.0245, Val Loss: 0.0215\n",
      "Epoch [23/1000] Fold 6, Train Loss: 0.0245, Val Loss: 0.0214\n",
      "Epoch [24/1000] Fold 6, Train Loss: 0.0241, Val Loss: 0.0210\n",
      "Epoch [25/1000] Fold 6, Train Loss: 0.0238, Val Loss: 0.0206\n",
      "Epoch [26/1000] Fold 6, Train Loss: 0.0239, Val Loss: 0.0206\n",
      "Epoch [27/1000] Fold 6, Train Loss: 0.0234, Val Loss: 0.0202\n",
      "Epoch [28/1000] Fold 6, Train Loss: 0.0233, Val Loss: 0.0202\n",
      "Epoch [29/1000] Fold 6, Train Loss: 0.0228, Val Loss: 0.0197\n",
      "Epoch [30/1000] Fold 6, Train Loss: 0.0228, Val Loss: 0.0195\n",
      "Epoch [31/1000] Fold 6, Train Loss: 0.0228, Val Loss: 0.0194\n",
      "Epoch [32/1000] Fold 6, Train Loss: 0.0222, Val Loss: 0.0190\n",
      "Epoch [33/1000] Fold 6, Train Loss: 0.0220, Val Loss: 0.0186\n",
      "Epoch [34/1000] Fold 6, Train Loss: 0.0221, Val Loss: 0.0188\n",
      "Epoch [35/1000] Fold 6, Train Loss: 0.0217, Val Loss: 0.0185\n",
      "Epoch [36/1000] Fold 6, Train Loss: 0.0214, Val Loss: 0.0181\n",
      "Epoch [37/1000] Fold 6, Train Loss: 0.0214, Val Loss: 0.0181\n",
      "Epoch [38/1000] Fold 6, Train Loss: 0.0213, Val Loss: 0.0178\n",
      "Epoch [39/1000] Fold 6, Train Loss: 0.0209, Val Loss: 0.0176\n",
      "Epoch [40/1000] Fold 6, Train Loss: 0.0207, Val Loss: 0.0175\n",
      "Epoch [41/1000] Fold 6, Train Loss: 0.0207, Val Loss: 0.0174\n",
      "Epoch [42/1000] Fold 6, Train Loss: 0.0205, Val Loss: 0.0172\n",
      "Epoch [43/1000] Fold 6, Train Loss: 0.0203, Val Loss: 0.0172\n",
      "Epoch [44/1000] Fold 6, Train Loss: 0.0202, Val Loss: 0.0170\n",
      "Epoch [45/1000] Fold 6, Train Loss: 0.0202, Val Loss: 0.0168\n",
      "Epoch [46/1000] Fold 6, Train Loss: 0.0202, Val Loss: 0.0168\n",
      "Epoch [47/1000] Fold 6, Train Loss: 0.0198, Val Loss: 0.0166\n",
      "Epoch [48/1000] Fold 6, Train Loss: 0.0199, Val Loss: 0.0165\n",
      "Epoch [49/1000] Fold 6, Train Loss: 0.0195, Val Loss: 0.0165\n",
      "Epoch [50/1000] Fold 6, Train Loss: 0.0195, Val Loss: 0.0162\n",
      "Epoch [51/1000] Fold 6, Train Loss: 0.0196, Val Loss: 0.0161\n",
      "Epoch [52/1000] Fold 6, Train Loss: 0.0193, Val Loss: 0.0161\n",
      "Epoch [53/1000] Fold 6, Train Loss: 0.0191, Val Loss: 0.0158\n",
      "Epoch [54/1000] Fold 6, Train Loss: 0.0189, Val Loss: 0.0157\n",
      "Epoch [55/1000] Fold 6, Train Loss: 0.0191, Val Loss: 0.0158\n",
      "Epoch [56/1000] Fold 6, Train Loss: 0.0188, Val Loss: 0.0155\n",
      "Epoch [57/1000] Fold 6, Train Loss: 0.0186, Val Loss: 0.0155\n",
      "Epoch [58/1000] Fold 6, Train Loss: 0.0186, Val Loss: 0.0151\n",
      "Epoch [59/1000] Fold 6, Train Loss: 0.0184, Val Loss: 0.0151\n",
      "Epoch [60/1000] Fold 6, Train Loss: 0.0184, Val Loss: 0.0152\n",
      "Epoch [61/1000] Fold 6, Train Loss: 0.0185, Val Loss: 0.0151\n",
      "Epoch [62/1000] Fold 6, Train Loss: 0.0182, Val Loss: 0.0148\n",
      "Epoch [63/1000] Fold 6, Train Loss: 0.0183, Val Loss: 0.0149\n",
      "Epoch [64/1000] Fold 6, Train Loss: 0.0183, Val Loss: 0.0148\n",
      "Epoch [65/1000] Fold 6, Train Loss: 0.0182, Val Loss: 0.0148\n",
      "Epoch [66/1000] Fold 6, Train Loss: 0.0178, Val Loss: 0.0148\n",
      "Epoch [67/1000] Fold 6, Train Loss: 0.0180, Val Loss: 0.0147\n",
      "Epoch [68/1000] Fold 6, Train Loss: 0.0177, Val Loss: 0.0146\n",
      "Epoch [69/1000] Fold 6, Train Loss: 0.0178, Val Loss: 0.0145\n",
      "Epoch [70/1000] Fold 6, Train Loss: 0.0178, Val Loss: 0.0144\n",
      "Epoch [71/1000] Fold 6, Train Loss: 0.0176, Val Loss: 0.0144\n",
      "Epoch [72/1000] Fold 6, Train Loss: 0.0175, Val Loss: 0.0142\n",
      "Epoch [73/1000] Fold 6, Train Loss: 0.0175, Val Loss: 0.0143\n",
      "Epoch [74/1000] Fold 6, Train Loss: 0.0175, Val Loss: 0.0139\n",
      "Epoch [75/1000] Fold 6, Train Loss: 0.0171, Val Loss: 0.0140\n",
      "Epoch [76/1000] Fold 6, Train Loss: 0.0174, Val Loss: 0.0140\n",
      "Epoch [77/1000] Fold 6, Train Loss: 0.0176, Val Loss: 0.0139\n",
      "Epoch [78/1000] Fold 6, Train Loss: 0.0170, Val Loss: 0.0137\n",
      "Epoch [79/1000] Fold 6, Train Loss: 0.0171, Val Loss: 0.0137\n",
      "Epoch [80/1000] Fold 6, Train Loss: 0.0169, Val Loss: 0.0137\n",
      "Epoch [81/1000] Fold 6, Train Loss: 0.0170, Val Loss: 0.0135\n",
      "Epoch [82/1000] Fold 6, Train Loss: 0.0168, Val Loss: 0.0135\n",
      "Epoch [83/1000] Fold 6, Train Loss: 0.0172, Val Loss: 0.0134\n",
      "Epoch [84/1000] Fold 6, Train Loss: 0.0168, Val Loss: 0.0134\n",
      "Epoch [85/1000] Fold 6, Train Loss: 0.0168, Val Loss: 0.0135\n",
      "Epoch [86/1000] Fold 6, Train Loss: 0.0168, Val Loss: 0.0136\n",
      "Epoch [87/1000] Fold 6, Train Loss: 0.0168, Val Loss: 0.0132\n",
      "Epoch [88/1000] Fold 6, Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Epoch [89/1000] Fold 6, Train Loss: 0.0167, Val Loss: 0.0134\n",
      "Epoch [90/1000] Fold 6, Train Loss: 0.0165, Val Loss: 0.0130\n",
      "Epoch [91/1000] Fold 6, Train Loss: 0.0162, Val Loss: 0.0130\n",
      "Epoch [92/1000] Fold 6, Train Loss: 0.0161, Val Loss: 0.0131\n",
      "Epoch [93/1000] Fold 6, Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Epoch [94/1000] Fold 6, Train Loss: 0.0161, Val Loss: 0.0129\n",
      "Epoch [95/1000] Fold 6, Train Loss: 0.0162, Val Loss: 0.0130\n",
      "Epoch [96/1000] Fold 6, Train Loss: 0.0160, Val Loss: 0.0128\n",
      "Epoch [97/1000] Fold 6, Train Loss: 0.0163, Val Loss: 0.0128\n",
      "Epoch [98/1000] Fold 6, Train Loss: 0.0161, Val Loss: 0.0128\n",
      "Epoch [99/1000] Fold 6, Train Loss: 0.0161, Val Loss: 0.0127\n",
      "Epoch [100/1000] Fold 6, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [101/1000] Fold 6, Train Loss: 0.0161, Val Loss: 0.0127\n",
      "Epoch [102/1000] Fold 6, Train Loss: 0.0158, Val Loss: 0.0126\n",
      "Epoch [103/1000] Fold 6, Train Loss: 0.0159, Val Loss: 0.0126\n",
      "Epoch [104/1000] Fold 6, Train Loss: 0.0159, Val Loss: 0.0126\n",
      "Epoch [105/1000] Fold 6, Train Loss: 0.0157, Val Loss: 0.0123\n",
      "Epoch [106/1000] Fold 6, Train Loss: 0.0156, Val Loss: 0.0123\n",
      "Epoch [107/1000] Fold 6, Train Loss: 0.0157, Val Loss: 0.0123\n",
      "Epoch [108/1000] Fold 6, Train Loss: 0.0159, Val Loss: 0.0123\n",
      "Epoch [109/1000] Fold 6, Train Loss: 0.0155, Val Loss: 0.0123\n",
      "Epoch [110/1000] Fold 6, Train Loss: 0.0156, Val Loss: 0.0123\n",
      "Epoch [111/1000] Fold 6, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Epoch [112/1000] Fold 6, Train Loss: 0.0155, Val Loss: 0.0123\n",
      "Epoch [113/1000] Fold 6, Train Loss: 0.0154, Val Loss: 0.0122\n",
      "Epoch [114/1000] Fold 6, Train Loss: 0.0157, Val Loss: 0.0122\n",
      "Epoch [115/1000] Fold 6, Train Loss: 0.0154, Val Loss: 0.0121\n",
      "Epoch [116/1000] Fold 6, Train Loss: 0.0153, Val Loss: 0.0121\n",
      "Epoch [117/1000] Fold 6, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [118/1000] Fold 6, Train Loss: 0.0153, Val Loss: 0.0120\n",
      "Epoch [119/1000] Fold 6, Train Loss: 0.0150, Val Loss: 0.0119\n",
      "Epoch [120/1000] Fold 6, Train Loss: 0.0155, Val Loss: 0.0121\n",
      "Epoch [121/1000] Fold 6, Train Loss: 0.0152, Val Loss: 0.0119\n",
      "Epoch [122/1000] Fold 6, Train Loss: 0.0150, Val Loss: 0.0120\n",
      "Epoch [123/1000] Fold 6, Train Loss: 0.0150, Val Loss: 0.0119\n",
      "Epoch [124/1000] Fold 6, Train Loss: 0.0150, Val Loss: 0.0118\n",
      "Epoch [125/1000] Fold 6, Train Loss: 0.0149, Val Loss: 0.0118\n",
      "Epoch [126/1000] Fold 6, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [127/1000] Fold 6, Train Loss: 0.0149, Val Loss: 0.0119\n",
      "Epoch [128/1000] Fold 6, Train Loss: 0.0146, Val Loss: 0.0116\n",
      "Epoch [129/1000] Fold 6, Train Loss: 0.0151, Val Loss: 0.0116\n",
      "Epoch [130/1000] Fold 6, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [131/1000] Fold 6, Train Loss: 0.0148, Val Loss: 0.0117\n",
      "Epoch [132/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0116\n",
      "Epoch [133/1000] Fold 6, Train Loss: 0.0147, Val Loss: 0.0116\n",
      "Epoch [134/1000] Fold 6, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [135/1000] Fold 6, Train Loss: 0.0150, Val Loss: 0.0115\n",
      "Epoch [136/1000] Fold 6, Train Loss: 0.0149, Val Loss: 0.0116\n",
      "Epoch [137/1000] Fold 6, Train Loss: 0.0146, Val Loss: 0.0116\n",
      "Epoch [138/1000] Fold 6, Train Loss: 0.0147, Val Loss: 0.0115\n",
      "Epoch [139/1000] Fold 6, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [140/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [141/1000] Fold 6, Train Loss: 0.0147, Val Loss: 0.0115\n",
      "Epoch [142/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0114\n",
      "Epoch [143/1000] Fold 6, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [144/1000] Fold 6, Train Loss: 0.0147, Val Loss: 0.0114\n",
      "Epoch [145/1000] Fold 6, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [146/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [147/1000] Fold 6, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [148/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [149/1000] Fold 6, Train Loss: 0.0145, Val Loss: 0.0112\n",
      "Epoch [150/1000] Fold 6, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [151/1000] Fold 6, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [152/1000] Fold 6, Train Loss: 0.0144, Val Loss: 0.0111\n",
      "Epoch [153/1000] Fold 6, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [154/1000] Fold 6, Train Loss: 0.0143, Val Loss: 0.0111\n",
      "Epoch [155/1000] Fold 6, Train Loss: 0.0143, Val Loss: 0.0111\n",
      "Epoch [156/1000] Fold 6, Train Loss: 0.0143, Val Loss: 0.0109\n",
      "Epoch [157/1000] Fold 6, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [158/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0110\n",
      "Epoch [159/1000] Fold 6, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [160/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [161/1000] Fold 6, Train Loss: 0.0141, Val Loss: 0.0109\n",
      "Epoch [162/1000] Fold 6, Train Loss: 0.0144, Val Loss: 0.0109\n",
      "Epoch [163/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [164/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [165/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [166/1000] Fold 6, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [167/1000] Fold 6, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [168/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [169/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [170/1000] Fold 6, Train Loss: 0.0141, Val Loss: 0.0108\n",
      "Epoch [171/1000] Fold 6, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [172/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [173/1000] Fold 6, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [174/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [175/1000] Fold 6, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [176/1000] Fold 6, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [177/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [178/1000] Fold 6, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [179/1000] Fold 6, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [180/1000] Fold 6, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [181/1000] Fold 6, Train Loss: 0.0138, Val Loss: 0.0109\n",
      "Epoch [182/1000] Fold 6, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [183/1000] Fold 6, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [184/1000] Fold 6, Train Loss: 0.0138, Val Loss: 0.0106\n",
      "Epoch [185/1000] Fold 6, Train Loss: 0.0136, Val Loss: 0.0109\n",
      "Epoch [186/1000] Fold 6, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [187/1000] Fold 6, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [188/1000] Fold 6, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [189/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [190/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [191/1000] Fold 6, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [192/1000] Fold 6, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [193/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [194/1000] Fold 6, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [195/1000] Fold 6, Train Loss: 0.0133, Val Loss: 0.0106\n",
      "Epoch [196/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [197/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0106\n",
      "Epoch [198/1000] Fold 6, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [199/1000] Fold 6, Train Loss: 0.0136, Val Loss: 0.0105\n",
      "Epoch [200/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [201/1000] Fold 6, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [202/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [203/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [204/1000] Fold 6, Train Loss: 0.0133, Val Loss: 0.0107\n",
      "Epoch [205/1000] Fold 6, Train Loss: 0.0135, Val Loss: 0.0105\n",
      "Epoch [206/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [207/1000] Fold 6, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [208/1000] Fold 6, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [209/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [210/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [211/1000] Fold 6, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [212/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [213/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [214/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [215/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [216/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [217/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [218/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [219/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [220/1000] Fold 6, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [221/1000] Fold 6, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [222/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [223/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [224/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [225/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [226/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [227/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [228/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [229/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [230/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [231/1000] Fold 6, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [232/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [233/1000] Fold 6, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [234/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [235/1000] Fold 6, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [236/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0100\n",
      "Epoch [237/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [238/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [239/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [240/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [241/1000] Fold 6, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [242/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [243/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [244/1000] Fold 6, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [245/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [246/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0102\n",
      "Epoch [247/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [248/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [249/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [250/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [251/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [252/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [253/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [254/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [255/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [256/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [257/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [258/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [259/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [260/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [261/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [262/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [263/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [264/1000] Fold 6, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [265/1000] Fold 6, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [266/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0102\n",
      "Epoch [267/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [268/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [269/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [270/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [271/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [272/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [273/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [274/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [275/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [276/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [277/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [278/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [279/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [280/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [281/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [282/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [283/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [284/1000] Fold 6, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [285/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [286/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [287/1000] Fold 6, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [288/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [289/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [290/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [291/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [292/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [293/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [294/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [295/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [296/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [297/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [298/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [299/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [300/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [301/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [302/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [303/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [304/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [305/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [306/1000] Fold 6, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [307/1000] Fold 6, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [308/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [309/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [310/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [311/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [312/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [313/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [314/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [315/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [316/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [317/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [318/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [319/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [320/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [321/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [322/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [323/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [324/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [325/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [326/1000] Fold 6, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [327/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [328/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [329/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [330/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [331/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [332/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [333/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [334/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [335/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [336/1000] Fold 6, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [337/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [338/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [339/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [340/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [341/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [342/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [343/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [344/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [345/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [346/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [347/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [348/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [349/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [350/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [351/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [352/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [353/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [354/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [355/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [356/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [357/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [358/1000] Fold 6, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [359/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [360/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [361/1000] Fold 6, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [362/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [363/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [364/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [365/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [366/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [367/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [368/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [369/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [370/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [371/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [372/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [373/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [374/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [375/1000] Fold 6, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [376/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [377/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [378/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [379/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [380/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [381/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [382/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [383/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [384/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [385/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [386/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [387/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [388/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [389/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [390/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [391/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [392/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [393/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [394/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [395/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [396/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [397/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [398/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [399/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [400/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [401/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [402/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [403/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [404/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [405/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [406/1000] Fold 6, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [407/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [408/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [409/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [410/1000] Fold 6, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [411/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [412/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [413/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [414/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [415/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [416/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [417/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [418/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [419/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [420/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [421/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [422/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [423/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [424/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [425/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [426/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [427/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [428/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [429/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [430/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [431/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [432/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [433/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [434/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [435/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [436/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [437/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [438/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [439/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [440/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [441/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [442/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [443/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [444/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [445/1000] Fold 6, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [446/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [447/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [448/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [449/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [450/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [451/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [452/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [453/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [454/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [455/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [456/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [457/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [458/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [459/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [460/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [461/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [462/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [463/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [464/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [465/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [466/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [467/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [468/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [469/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [470/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [471/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [472/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [473/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [474/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [475/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [476/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [477/1000] Fold 6, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [478/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [479/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [480/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [481/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [482/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [483/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [484/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [485/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [486/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [487/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [488/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [489/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [490/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [491/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [492/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [493/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [494/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [495/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [496/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [497/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [498/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [499/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [500/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [501/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [502/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [503/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [504/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [505/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0092\n",
      "Epoch [506/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [507/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [508/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [509/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [510/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [511/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [512/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [513/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [514/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [515/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [516/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [517/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [518/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [519/1000] Fold 6, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [520/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [521/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [522/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [523/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [524/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [525/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [526/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [527/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [528/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [529/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [530/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [531/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [532/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [533/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [534/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [535/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [536/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [537/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [538/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [539/1000] Fold 6, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [540/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [541/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [542/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [543/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0091\n",
      "Epoch [544/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [545/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [546/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [547/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [548/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [549/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [550/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [551/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [552/1000] Fold 6, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [553/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [554/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [555/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [556/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [557/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [558/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [559/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [560/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [561/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [562/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [563/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [564/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [565/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [566/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [567/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [568/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [569/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [570/1000] Fold 6, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [571/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [572/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [573/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [574/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [575/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [576/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [577/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [578/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [579/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [580/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [581/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [582/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [583/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [584/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [585/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [586/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [587/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [588/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [589/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [590/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [591/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [592/1000] Fold 6, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [593/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [594/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [595/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [596/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [597/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [598/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [599/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [600/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [601/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [602/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [603/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [604/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [605/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [606/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [607/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [608/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [609/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [610/1000] Fold 6, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [611/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [612/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [613/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [614/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [615/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [616/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [617/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [618/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [619/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [620/1000] Fold 6, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [621/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [622/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [623/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [624/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [625/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [626/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [627/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [628/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [629/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [630/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [631/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [632/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [633/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [634/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [635/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [636/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [637/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [638/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [639/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [640/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [641/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [642/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [643/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [644/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [645/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [646/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [647/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [648/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [649/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [650/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [651/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [652/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [653/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [654/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [655/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [656/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [657/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [658/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0090\n",
      "Epoch [659/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [660/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [661/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [662/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [663/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [664/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [665/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [666/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [667/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [668/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [669/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [670/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [671/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [672/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [673/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [674/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [675/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [676/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [677/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [678/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [679/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [680/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [681/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [682/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [683/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [684/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [685/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [686/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [687/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [688/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [689/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [690/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [691/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [692/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [693/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [694/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [695/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [696/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [697/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [698/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [699/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [700/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [701/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [702/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [703/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [704/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [705/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [706/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [707/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [708/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [709/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [710/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [711/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [712/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [713/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [714/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [715/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [716/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [717/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0089\n",
      "Epoch [718/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [719/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [720/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [721/1000] Fold 6, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [722/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [723/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [724/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [725/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [726/1000] Fold 6, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [727/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [728/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [729/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [730/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [731/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [732/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [733/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [734/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [735/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [736/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [737/1000] Fold 6, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [738/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [739/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [740/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [741/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [742/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [743/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [744/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [745/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [746/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [747/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [748/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [749/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [750/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [751/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [752/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [753/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [754/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [755/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [756/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [757/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [758/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [759/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [760/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [761/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [762/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [763/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [764/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [765/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [766/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [767/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [768/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [769/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [770/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [771/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [772/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [773/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [774/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [775/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [776/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [777/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [778/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [779/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [780/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [781/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [782/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [783/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [784/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [785/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [786/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [787/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [788/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [789/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [790/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [791/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [792/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [793/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [794/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [795/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [796/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [797/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [798/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [799/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [800/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [801/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [802/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [803/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [804/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [805/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0088\n",
      "Epoch [806/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [807/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [808/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [809/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [810/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [811/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [812/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [813/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [814/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [815/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [816/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [817/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [818/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [819/1000] Fold 6, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [820/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [821/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [822/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [823/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [824/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [825/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [826/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [827/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [828/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [829/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [830/1000] Fold 6, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [831/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [832/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [833/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [834/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [835/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [836/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0088\n",
      "Epoch [837/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [838/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [839/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [840/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [841/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [842/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [843/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [844/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [845/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [846/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [847/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [848/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [849/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [850/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [851/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [852/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [853/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [854/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [855/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [856/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [857/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [858/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [859/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [860/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [861/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [862/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [863/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [864/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [865/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [866/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [867/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [868/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [869/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [870/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [871/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [872/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [873/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [874/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [875/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [876/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [877/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [878/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [879/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [880/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [881/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [882/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [883/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [884/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [885/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [886/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [887/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [888/1000] Fold 6, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [889/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [890/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [891/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [892/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0088\n",
      "Epoch [893/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [894/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [895/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [896/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [897/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [898/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [899/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [900/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [901/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [902/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [903/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [904/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [905/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [906/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [907/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [908/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [909/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [910/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [911/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [912/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [913/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [914/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [915/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [916/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [917/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [918/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [919/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [920/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [921/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [922/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [923/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [924/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [925/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [926/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [927/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [928/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [929/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [930/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [931/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [932/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [933/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0086\n",
      "Epoch [934/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [935/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [936/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [937/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [938/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [939/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [940/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [941/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [942/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [943/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [944/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [945/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [946/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [947/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [948/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0088\n",
      "Epoch [949/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [950/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [951/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [952/1000] Fold 6, Train Loss: 0.0099, Val Loss: 0.0087\n",
      "Epoch [953/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [954/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [955/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [956/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [957/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [958/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [959/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [960/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [961/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [962/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [963/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0087\n",
      "Epoch [964/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [965/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [966/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0085\n",
      "Epoch [967/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0084\n",
      "Epoch [968/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [969/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [970/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [971/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [972/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [973/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [974/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [975/1000] Fold 6, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [976/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [977/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [978/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [979/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [980/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [981/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0085\n",
      "Epoch [982/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [983/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0086\n",
      "Epoch [984/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [985/1000] Fold 6, Train Loss: 0.0098, Val Loss: 0.0086\n",
      "Epoch [986/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0086\n",
      "Epoch [987/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [988/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [989/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [990/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [991/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Epoch [992/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [993/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0086\n",
      "Epoch [994/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0086\n",
      "Epoch [995/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [996/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0087\n",
      "Epoch [997/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0087\n",
      "Epoch [998/1000] Fold 6, Train Loss: 0.0095, Val Loss: 0.0086\n",
      "Epoch [999/1000] Fold 6, Train Loss: 0.0096, Val Loss: 0.0085\n",
      "Epoch [1000/1000] Fold 6, Train Loss: 0.0097, Val Loss: 0.0085\n",
      "Fold 7/10\n",
      "Epoch [1/1000] Fold 7, Train Loss: 0.1405, Val Loss: 0.0689\n",
      "Epoch [2/1000] Fold 7, Train Loss: 0.0685, Val Loss: 0.0444\n",
      "Epoch [3/1000] Fold 7, Train Loss: 0.0482, Val Loss: 0.0367\n",
      "Epoch [4/1000] Fold 7, Train Loss: 0.0400, Val Loss: 0.0330\n",
      "Epoch [5/1000] Fold 7, Train Loss: 0.0361, Val Loss: 0.0312\n",
      "Epoch [6/1000] Fold 7, Train Loss: 0.0336, Val Loss: 0.0298\n",
      "Epoch [7/1000] Fold 7, Train Loss: 0.0319, Val Loss: 0.0288\n",
      "Epoch [8/1000] Fold 7, Train Loss: 0.0309, Val Loss: 0.0283\n",
      "Epoch [9/1000] Fold 7, Train Loss: 0.0301, Val Loss: 0.0274\n",
      "Epoch [10/1000] Fold 7, Train Loss: 0.0295, Val Loss: 0.0271\n",
      "Epoch [11/1000] Fold 7, Train Loss: 0.0289, Val Loss: 0.0267\n",
      "Epoch [12/1000] Fold 7, Train Loss: 0.0283, Val Loss: 0.0265\n",
      "Epoch [13/1000] Fold 7, Train Loss: 0.0278, Val Loss: 0.0260\n",
      "Epoch [14/1000] Fold 7, Train Loss: 0.0276, Val Loss: 0.0255\n",
      "Epoch [15/1000] Fold 7, Train Loss: 0.0271, Val Loss: 0.0253\n",
      "Epoch [16/1000] Fold 7, Train Loss: 0.0268, Val Loss: 0.0249\n",
      "Epoch [17/1000] Fold 7, Train Loss: 0.0265, Val Loss: 0.0245\n",
      "Epoch [18/1000] Fold 7, Train Loss: 0.0261, Val Loss: 0.0242\n",
      "Epoch [19/1000] Fold 7, Train Loss: 0.0254, Val Loss: 0.0236\n",
      "Epoch [20/1000] Fold 7, Train Loss: 0.0254, Val Loss: 0.0233\n",
      "Epoch [21/1000] Fold 7, Train Loss: 0.0253, Val Loss: 0.0228\n",
      "Epoch [22/1000] Fold 7, Train Loss: 0.0248, Val Loss: 0.0225\n",
      "Epoch [23/1000] Fold 7, Train Loss: 0.0244, Val Loss: 0.0222\n",
      "Epoch [24/1000] Fold 7, Train Loss: 0.0240, Val Loss: 0.0217\n",
      "Epoch [25/1000] Fold 7, Train Loss: 0.0239, Val Loss: 0.0212\n",
      "Epoch [26/1000] Fold 7, Train Loss: 0.0237, Val Loss: 0.0210\n",
      "Epoch [27/1000] Fold 7, Train Loss: 0.0233, Val Loss: 0.0209\n",
      "Epoch [28/1000] Fold 7, Train Loss: 0.0232, Val Loss: 0.0204\n",
      "Epoch [29/1000] Fold 7, Train Loss: 0.0228, Val Loss: 0.0205\n",
      "Epoch [30/1000] Fold 7, Train Loss: 0.0225, Val Loss: 0.0201\n",
      "Epoch [31/1000] Fold 7, Train Loss: 0.0226, Val Loss: 0.0199\n",
      "Epoch [32/1000] Fold 7, Train Loss: 0.0220, Val Loss: 0.0193\n",
      "Epoch [33/1000] Fold 7, Train Loss: 0.0220, Val Loss: 0.0196\n",
      "Epoch [34/1000] Fold 7, Train Loss: 0.0217, Val Loss: 0.0191\n",
      "Epoch [35/1000] Fold 7, Train Loss: 0.0214, Val Loss: 0.0189\n",
      "Epoch [36/1000] Fold 7, Train Loss: 0.0212, Val Loss: 0.0188\n",
      "Epoch [37/1000] Fold 7, Train Loss: 0.0211, Val Loss: 0.0186\n",
      "Epoch [38/1000] Fold 7, Train Loss: 0.0212, Val Loss: 0.0185\n",
      "Epoch [39/1000] Fold 7, Train Loss: 0.0207, Val Loss: 0.0183\n",
      "Epoch [40/1000] Fold 7, Train Loss: 0.0209, Val Loss: 0.0182\n",
      "Epoch [41/1000] Fold 7, Train Loss: 0.0207, Val Loss: 0.0181\n",
      "Epoch [42/1000] Fold 7, Train Loss: 0.0205, Val Loss: 0.0180\n",
      "Epoch [43/1000] Fold 7, Train Loss: 0.0202, Val Loss: 0.0177\n",
      "Epoch [44/1000] Fold 7, Train Loss: 0.0202, Val Loss: 0.0174\n",
      "Epoch [45/1000] Fold 7, Train Loss: 0.0202, Val Loss: 0.0174\n",
      "Epoch [46/1000] Fold 7, Train Loss: 0.0196, Val Loss: 0.0173\n",
      "Epoch [47/1000] Fold 7, Train Loss: 0.0196, Val Loss: 0.0171\n",
      "Epoch [48/1000] Fold 7, Train Loss: 0.0196, Val Loss: 0.0172\n",
      "Epoch [49/1000] Fold 7, Train Loss: 0.0196, Val Loss: 0.0169\n",
      "Epoch [50/1000] Fold 7, Train Loss: 0.0197, Val Loss: 0.0170\n",
      "Epoch [51/1000] Fold 7, Train Loss: 0.0193, Val Loss: 0.0167\n",
      "Epoch [52/1000] Fold 7, Train Loss: 0.0193, Val Loss: 0.0166\n",
      "Epoch [53/1000] Fold 7, Train Loss: 0.0194, Val Loss: 0.0167\n",
      "Epoch [54/1000] Fold 7, Train Loss: 0.0192, Val Loss: 0.0163\n",
      "Epoch [55/1000] Fold 7, Train Loss: 0.0190, Val Loss: 0.0162\n",
      "Epoch [56/1000] Fold 7, Train Loss: 0.0191, Val Loss: 0.0160\n",
      "Epoch [57/1000] Fold 7, Train Loss: 0.0191, Val Loss: 0.0160\n",
      "Epoch [58/1000] Fold 7, Train Loss: 0.0191, Val Loss: 0.0161\n",
      "Epoch [59/1000] Fold 7, Train Loss: 0.0186, Val Loss: 0.0159\n",
      "Epoch [60/1000] Fold 7, Train Loss: 0.0188, Val Loss: 0.0159\n",
      "Epoch [61/1000] Fold 7, Train Loss: 0.0183, Val Loss: 0.0157\n",
      "Epoch [62/1000] Fold 7, Train Loss: 0.0185, Val Loss: 0.0156\n",
      "Epoch [63/1000] Fold 7, Train Loss: 0.0183, Val Loss: 0.0155\n",
      "Epoch [64/1000] Fold 7, Train Loss: 0.0183, Val Loss: 0.0155\n",
      "Epoch [65/1000] Fold 7, Train Loss: 0.0183, Val Loss: 0.0154\n",
      "Epoch [66/1000] Fold 7, Train Loss: 0.0181, Val Loss: 0.0156\n",
      "Epoch [67/1000] Fold 7, Train Loss: 0.0179, Val Loss: 0.0158\n",
      "Epoch [68/1000] Fold 7, Train Loss: 0.0182, Val Loss: 0.0152\n",
      "Epoch [69/1000] Fold 7, Train Loss: 0.0179, Val Loss: 0.0152\n",
      "Epoch [70/1000] Fold 7, Train Loss: 0.0177, Val Loss: 0.0149\n",
      "Epoch [71/1000] Fold 7, Train Loss: 0.0174, Val Loss: 0.0148\n",
      "Epoch [72/1000] Fold 7, Train Loss: 0.0175, Val Loss: 0.0148\n",
      "Epoch [73/1000] Fold 7, Train Loss: 0.0177, Val Loss: 0.0148\n",
      "Epoch [74/1000] Fold 7, Train Loss: 0.0178, Val Loss: 0.0149\n",
      "Epoch [75/1000] Fold 7, Train Loss: 0.0175, Val Loss: 0.0148\n",
      "Epoch [76/1000] Fold 7, Train Loss: 0.0174, Val Loss: 0.0147\n",
      "Epoch [77/1000] Fold 7, Train Loss: 0.0175, Val Loss: 0.0145\n",
      "Epoch [78/1000] Fold 7, Train Loss: 0.0172, Val Loss: 0.0143\n",
      "Epoch [79/1000] Fold 7, Train Loss: 0.0172, Val Loss: 0.0145\n",
      "Epoch [80/1000] Fold 7, Train Loss: 0.0173, Val Loss: 0.0143\n",
      "Epoch [81/1000] Fold 7, Train Loss: 0.0168, Val Loss: 0.0143\n",
      "Epoch [82/1000] Fold 7, Train Loss: 0.0172, Val Loss: 0.0142\n",
      "Epoch [83/1000] Fold 7, Train Loss: 0.0173, Val Loss: 0.0142\n",
      "Epoch [84/1000] Fold 7, Train Loss: 0.0168, Val Loss: 0.0143\n",
      "Epoch [85/1000] Fold 7, Train Loss: 0.0169, Val Loss: 0.0142\n",
      "Epoch [86/1000] Fold 7, Train Loss: 0.0169, Val Loss: 0.0142\n",
      "Epoch [87/1000] Fold 7, Train Loss: 0.0168, Val Loss: 0.0141\n",
      "Epoch [88/1000] Fold 7, Train Loss: 0.0166, Val Loss: 0.0140\n",
      "Epoch [89/1000] Fold 7, Train Loss: 0.0169, Val Loss: 0.0139\n",
      "Epoch [90/1000] Fold 7, Train Loss: 0.0168, Val Loss: 0.0139\n",
      "Epoch [91/1000] Fold 7, Train Loss: 0.0168, Val Loss: 0.0138\n",
      "Epoch [92/1000] Fold 7, Train Loss: 0.0166, Val Loss: 0.0139\n",
      "Epoch [93/1000] Fold 7, Train Loss: 0.0167, Val Loss: 0.0138\n",
      "Epoch [94/1000] Fold 7, Train Loss: 0.0167, Val Loss: 0.0137\n",
      "Epoch [95/1000] Fold 7, Train Loss: 0.0165, Val Loss: 0.0135\n",
      "Epoch [96/1000] Fold 7, Train Loss: 0.0164, Val Loss: 0.0136\n",
      "Epoch [97/1000] Fold 7, Train Loss: 0.0163, Val Loss: 0.0134\n",
      "Epoch [98/1000] Fold 7, Train Loss: 0.0163, Val Loss: 0.0136\n",
      "Epoch [99/1000] Fold 7, Train Loss: 0.0164, Val Loss: 0.0136\n",
      "Epoch [100/1000] Fold 7, Train Loss: 0.0165, Val Loss: 0.0137\n",
      "Epoch [101/1000] Fold 7, Train Loss: 0.0158, Val Loss: 0.0133\n",
      "Epoch [102/1000] Fold 7, Train Loss: 0.0161, Val Loss: 0.0136\n",
      "Epoch [103/1000] Fold 7, Train Loss: 0.0159, Val Loss: 0.0133\n",
      "Epoch [104/1000] Fold 7, Train Loss: 0.0162, Val Loss: 0.0133\n",
      "Epoch [105/1000] Fold 7, Train Loss: 0.0161, Val Loss: 0.0133\n",
      "Epoch [106/1000] Fold 7, Train Loss: 0.0162, Val Loss: 0.0134\n",
      "Epoch [107/1000] Fold 7, Train Loss: 0.0161, Val Loss: 0.0132\n",
      "Epoch [108/1000] Fold 7, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [109/1000] Fold 7, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [110/1000] Fold 7, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [111/1000] Fold 7, Train Loss: 0.0158, Val Loss: 0.0130\n",
      "Epoch [112/1000] Fold 7, Train Loss: 0.0156, Val Loss: 0.0132\n",
      "Epoch [113/1000] Fold 7, Train Loss: 0.0160, Val Loss: 0.0131\n",
      "Epoch [114/1000] Fold 7, Train Loss: 0.0159, Val Loss: 0.0132\n",
      "Epoch [115/1000] Fold 7, Train Loss: 0.0157, Val Loss: 0.0129\n",
      "Epoch [116/1000] Fold 7, Train Loss: 0.0156, Val Loss: 0.0129\n",
      "Epoch [117/1000] Fold 7, Train Loss: 0.0156, Val Loss: 0.0128\n",
      "Epoch [118/1000] Fold 7, Train Loss: 0.0156, Val Loss: 0.0128\n",
      "Epoch [119/1000] Fold 7, Train Loss: 0.0155, Val Loss: 0.0129\n",
      "Epoch [120/1000] Fold 7, Train Loss: 0.0155, Val Loss: 0.0127\n",
      "Epoch [121/1000] Fold 7, Train Loss: 0.0155, Val Loss: 0.0128\n",
      "Epoch [122/1000] Fold 7, Train Loss: 0.0154, Val Loss: 0.0125\n",
      "Epoch [123/1000] Fold 7, Train Loss: 0.0160, Val Loss: 0.0128\n",
      "Epoch [124/1000] Fold 7, Train Loss: 0.0153, Val Loss: 0.0127\n",
      "Epoch [125/1000] Fold 7, Train Loss: 0.0152, Val Loss: 0.0126\n",
      "Epoch [126/1000] Fold 7, Train Loss: 0.0152, Val Loss: 0.0127\n",
      "Epoch [127/1000] Fold 7, Train Loss: 0.0153, Val Loss: 0.0126\n",
      "Epoch [128/1000] Fold 7, Train Loss: 0.0151, Val Loss: 0.0126\n",
      "Epoch [129/1000] Fold 7, Train Loss: 0.0151, Val Loss: 0.0127\n",
      "Epoch [130/1000] Fold 7, Train Loss: 0.0152, Val Loss: 0.0127\n",
      "Epoch [131/1000] Fold 7, Train Loss: 0.0153, Val Loss: 0.0125\n",
      "Epoch [132/1000] Fold 7, Train Loss: 0.0151, Val Loss: 0.0126\n",
      "Epoch [133/1000] Fold 7, Train Loss: 0.0150, Val Loss: 0.0126\n",
      "Epoch [134/1000] Fold 7, Train Loss: 0.0150, Val Loss: 0.0125\n",
      "Epoch [135/1000] Fold 7, Train Loss: 0.0153, Val Loss: 0.0125\n",
      "Epoch [136/1000] Fold 7, Train Loss: 0.0149, Val Loss: 0.0124\n",
      "Epoch [137/1000] Fold 7, Train Loss: 0.0148, Val Loss: 0.0126\n",
      "Epoch [138/1000] Fold 7, Train Loss: 0.0148, Val Loss: 0.0123\n",
      "Epoch [139/1000] Fold 7, Train Loss: 0.0149, Val Loss: 0.0122\n",
      "Epoch [140/1000] Fold 7, Train Loss: 0.0153, Val Loss: 0.0124\n",
      "Epoch [141/1000] Fold 7, Train Loss: 0.0152, Val Loss: 0.0123\n",
      "Epoch [142/1000] Fold 7, Train Loss: 0.0145, Val Loss: 0.0123\n",
      "Epoch [143/1000] Fold 7, Train Loss: 0.0150, Val Loss: 0.0125\n",
      "Epoch [144/1000] Fold 7, Train Loss: 0.0151, Val Loss: 0.0122\n",
      "Epoch [145/1000] Fold 7, Train Loss: 0.0149, Val Loss: 0.0121\n",
      "Epoch [146/1000] Fold 7, Train Loss: 0.0149, Val Loss: 0.0122\n",
      "Epoch [147/1000] Fold 7, Train Loss: 0.0147, Val Loss: 0.0121\n",
      "Epoch [148/1000] Fold 7, Train Loss: 0.0147, Val Loss: 0.0121\n",
      "Epoch [149/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0121\n",
      "Epoch [150/1000] Fold 7, Train Loss: 0.0147, Val Loss: 0.0121\n",
      "Epoch [151/1000] Fold 7, Train Loss: 0.0147, Val Loss: 0.0120\n",
      "Epoch [152/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0119\n",
      "Epoch [153/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0120\n",
      "Epoch [154/1000] Fold 7, Train Loss: 0.0145, Val Loss: 0.0121\n",
      "Epoch [155/1000] Fold 7, Train Loss: 0.0147, Val Loss: 0.0122\n",
      "Epoch [156/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0118\n",
      "Epoch [157/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0119\n",
      "Epoch [158/1000] Fold 7, Train Loss: 0.0145, Val Loss: 0.0120\n",
      "Epoch [159/1000] Fold 7, Train Loss: 0.0142, Val Loss: 0.0119\n",
      "Epoch [160/1000] Fold 7, Train Loss: 0.0149, Val Loss: 0.0119\n",
      "Epoch [161/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0119\n",
      "Epoch [162/1000] Fold 7, Train Loss: 0.0142, Val Loss: 0.0119\n",
      "Epoch [163/1000] Fold 7, Train Loss: 0.0142, Val Loss: 0.0119\n",
      "Epoch [164/1000] Fold 7, Train Loss: 0.0142, Val Loss: 0.0119\n",
      "Epoch [165/1000] Fold 7, Train Loss: 0.0143, Val Loss: 0.0120\n",
      "Epoch [166/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0121\n",
      "Epoch [167/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0119\n",
      "Epoch [168/1000] Fold 7, Train Loss: 0.0144, Val Loss: 0.0117\n",
      "Epoch [169/1000] Fold 7, Train Loss: 0.0140, Val Loss: 0.0116\n",
      "Epoch [170/1000] Fold 7, Train Loss: 0.0142, Val Loss: 0.0119\n",
      "Epoch [171/1000] Fold 7, Train Loss: 0.0145, Val Loss: 0.0116\n",
      "Epoch [172/1000] Fold 7, Train Loss: 0.0140, Val Loss: 0.0116\n",
      "Epoch [173/1000] Fold 7, Train Loss: 0.0143, Val Loss: 0.0117\n",
      "Epoch [174/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0117\n",
      "Epoch [175/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0116\n",
      "Epoch [176/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0115\n",
      "Epoch [177/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0116\n",
      "Epoch [178/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0115\n",
      "Epoch [179/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0115\n",
      "Epoch [180/1000] Fold 7, Train Loss: 0.0140, Val Loss: 0.0116\n",
      "Epoch [181/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0114\n",
      "Epoch [182/1000] Fold 7, Train Loss: 0.0140, Val Loss: 0.0118\n",
      "Epoch [183/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0115\n",
      "Epoch [184/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0114\n",
      "Epoch [185/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0114\n",
      "Epoch [186/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0114\n",
      "Epoch [187/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0116\n",
      "Epoch [188/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0116\n",
      "Epoch [189/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0114\n",
      "Epoch [190/1000] Fold 7, Train Loss: 0.0136, Val Loss: 0.0115\n",
      "Epoch [191/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0115\n",
      "Epoch [192/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0115\n",
      "Epoch [193/1000] Fold 7, Train Loss: 0.0136, Val Loss: 0.0115\n",
      "Epoch [194/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0114\n",
      "Epoch [195/1000] Fold 7, Train Loss: 0.0141, Val Loss: 0.0116\n",
      "Epoch [196/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0115\n",
      "Epoch [197/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0113\n",
      "Epoch [198/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0112\n",
      "Epoch [199/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0114\n",
      "Epoch [200/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0112\n",
      "Epoch [201/1000] Fold 7, Train Loss: 0.0136, Val Loss: 0.0114\n",
      "Epoch [202/1000] Fold 7, Train Loss: 0.0139, Val Loss: 0.0114\n",
      "Epoch [203/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0112\n",
      "Epoch [204/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0112\n",
      "Epoch [205/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0112\n",
      "Epoch [206/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0112\n",
      "Epoch [207/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0110\n",
      "Epoch [208/1000] Fold 7, Train Loss: 0.0138, Val Loss: 0.0113\n",
      "Epoch [209/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0112\n",
      "Epoch [210/1000] Fold 7, Train Loss: 0.0134, Val Loss: 0.0111\n",
      "Epoch [211/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0111\n",
      "Epoch [212/1000] Fold 7, Train Loss: 0.0134, Val Loss: 0.0111\n",
      "Epoch [213/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0110\n",
      "Epoch [214/1000] Fold 7, Train Loss: 0.0136, Val Loss: 0.0111\n",
      "Epoch [215/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0112\n",
      "Epoch [216/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0110\n",
      "Epoch [217/1000] Fold 7, Train Loss: 0.0137, Val Loss: 0.0113\n",
      "Epoch [218/1000] Fold 7, Train Loss: 0.0136, Val Loss: 0.0112\n",
      "Epoch [219/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0112\n",
      "Epoch [220/1000] Fold 7, Train Loss: 0.0134, Val Loss: 0.0112\n",
      "Epoch [221/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0110\n",
      "Epoch [222/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0110\n",
      "Epoch [223/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0112\n",
      "Epoch [224/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0109\n",
      "Epoch [225/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0111\n",
      "Epoch [226/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0109\n",
      "Epoch [227/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0109\n",
      "Epoch [228/1000] Fold 7, Train Loss: 0.0135, Val Loss: 0.0110\n",
      "Epoch [229/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0108\n",
      "Epoch [230/1000] Fold 7, Train Loss: 0.0134, Val Loss: 0.0109\n",
      "Epoch [231/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0110\n",
      "Epoch [232/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0108\n",
      "Epoch [233/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0108\n",
      "Epoch [234/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0108\n",
      "Epoch [235/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0108\n",
      "Epoch [236/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0107\n",
      "Epoch [237/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0110\n",
      "Epoch [238/1000] Fold 7, Train Loss: 0.0133, Val Loss: 0.0109\n",
      "Epoch [239/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0109\n",
      "Epoch [240/1000] Fold 7, Train Loss: 0.0134, Val Loss: 0.0110\n",
      "Epoch [241/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0110\n",
      "Epoch [242/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0109\n",
      "Epoch [243/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0109\n",
      "Epoch [244/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0107\n",
      "Epoch [245/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0108\n",
      "Epoch [246/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0108\n",
      "Epoch [247/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0109\n",
      "Epoch [248/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0107\n",
      "Epoch [249/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [250/1000] Fold 7, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [251/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0109\n",
      "Epoch [252/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0107\n",
      "Epoch [253/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0107\n",
      "Epoch [254/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0108\n",
      "Epoch [255/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [256/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0107\n",
      "Epoch [257/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0107\n",
      "Epoch [258/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [259/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [260/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0107\n",
      "Epoch [261/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0107\n",
      "Epoch [262/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0107\n",
      "Epoch [263/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0106\n",
      "Epoch [264/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0105\n",
      "Epoch [265/1000] Fold 7, Train Loss: 0.0130, Val Loss: 0.0106\n",
      "Epoch [266/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0105\n",
      "Epoch [267/1000] Fold 7, Train Loss: 0.0131, Val Loss: 0.0106\n",
      "Epoch [268/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [269/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0107\n",
      "Epoch [270/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0105\n",
      "Epoch [271/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0106\n",
      "Epoch [272/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0105\n",
      "Epoch [273/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0105\n",
      "Epoch [274/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0104\n",
      "Epoch [275/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0107\n",
      "Epoch [276/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [277/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0104\n",
      "Epoch [278/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0105\n",
      "Epoch [279/1000] Fold 7, Train Loss: 0.0128, Val Loss: 0.0105\n",
      "Epoch [280/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0105\n",
      "Epoch [281/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0104\n",
      "Epoch [282/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0105\n",
      "Epoch [283/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0107\n",
      "Epoch [284/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0105\n",
      "Epoch [285/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0104\n",
      "Epoch [286/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [287/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0106\n",
      "Epoch [288/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0104\n",
      "Epoch [289/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0105\n",
      "Epoch [290/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0106\n",
      "Epoch [291/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0104\n",
      "Epoch [292/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [293/1000] Fold 7, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [294/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0104\n",
      "Epoch [295/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0104\n",
      "Epoch [296/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0104\n",
      "Epoch [297/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0104\n",
      "Epoch [298/1000] Fold 7, Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [299/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0105\n",
      "Epoch [300/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [301/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0105\n",
      "Epoch [302/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0102\n",
      "Epoch [303/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [304/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0103\n",
      "Epoch [305/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [306/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [307/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0103\n",
      "Epoch [308/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0103\n",
      "Epoch [309/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [310/1000] Fold 7, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [311/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0101\n",
      "Epoch [312/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [313/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0101\n",
      "Epoch [314/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0102\n",
      "Epoch [315/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0103\n",
      "Epoch [316/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0102\n",
      "Epoch [317/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0104\n",
      "Epoch [318/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0103\n",
      "Epoch [319/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0104\n",
      "Epoch [320/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [321/1000] Fold 7, Train Loss: 0.0125, Val Loss: 0.0102\n",
      "Epoch [322/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0103\n",
      "Epoch [323/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [324/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [325/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0103\n",
      "Epoch [326/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0104\n",
      "Epoch [327/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0102\n",
      "Epoch [328/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0103\n",
      "Epoch [329/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0102\n",
      "Epoch [330/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0104\n",
      "Epoch [331/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [332/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0102\n",
      "Epoch [333/1000] Fold 7, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [334/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [335/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [336/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0102\n",
      "Epoch [337/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [338/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [339/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [340/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [341/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [342/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [343/1000] Fold 7, Train Loss: 0.0122, Val Loss: 0.0101\n",
      "Epoch [344/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0101\n",
      "Epoch [345/1000] Fold 7, Train Loss: 0.0124, Val Loss: 0.0102\n",
      "Epoch [346/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [347/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0101\n",
      "Epoch [348/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [349/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [350/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0101\n",
      "Epoch [351/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0101\n",
      "Epoch [352/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [353/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [354/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [355/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [356/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0101\n",
      "Epoch [357/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [358/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [359/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0101\n",
      "Epoch [360/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [361/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [362/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0099\n",
      "Epoch [363/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0100\n",
      "Epoch [364/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0099\n",
      "Epoch [365/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [366/1000] Fold 7, Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [367/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [368/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [369/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [370/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [371/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [372/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0101\n",
      "Epoch [373/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [374/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0102\n",
      "Epoch [375/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0099\n",
      "Epoch [376/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [377/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [378/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [379/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [380/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0098\n",
      "Epoch [381/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0101\n",
      "Epoch [382/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [383/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [384/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [385/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [386/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [387/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0099\n",
      "Epoch [388/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0099\n",
      "Epoch [389/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0100\n",
      "Epoch [390/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0100\n",
      "Epoch [391/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [392/1000] Fold 7, Train Loss: 0.0120, Val Loss: 0.0100\n",
      "Epoch [393/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [394/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [395/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0099\n",
      "Epoch [396/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0100\n",
      "Epoch [397/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [398/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0099\n",
      "Epoch [399/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [400/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0099\n",
      "Epoch [401/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [402/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [403/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [404/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [405/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [406/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [407/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [408/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0099\n",
      "Epoch [409/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0099\n",
      "Epoch [410/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [411/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [412/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [413/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [414/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [415/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [416/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [417/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [418/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [419/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [420/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [421/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [422/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0099\n",
      "Epoch [423/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [424/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [425/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [426/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [427/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [428/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0099\n",
      "Epoch [429/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [430/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [431/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [432/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [433/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [434/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [435/1000] Fold 7, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [436/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [437/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [438/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [439/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0098\n",
      "Epoch [440/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [441/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [442/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [443/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [444/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [445/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [446/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0098\n",
      "Epoch [447/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [448/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [449/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [450/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [451/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [452/1000] Fold 7, Train Loss: 0.0116, Val Loss: 0.0098\n",
      "Epoch [453/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [454/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [455/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [456/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [457/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0098\n",
      "Epoch [458/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [459/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [460/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [461/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [462/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [463/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [464/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [465/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [466/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [467/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [468/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [469/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [470/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [471/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [472/1000] Fold 7, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [473/1000] Fold 7, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [474/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [475/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0098\n",
      "Epoch [476/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [477/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [478/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [479/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [480/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [481/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [482/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0098\n",
      "Epoch [483/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [484/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [485/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [486/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [487/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [488/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [489/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [490/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [491/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [492/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [493/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [494/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [495/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [496/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [497/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [498/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [499/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [500/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [501/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [502/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [503/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [504/1000] Fold 7, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [505/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [506/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0098\n",
      "Epoch [507/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [508/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [509/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [510/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [511/1000] Fold 7, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [512/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [513/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [514/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0098\n",
      "Epoch [515/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [516/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [517/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0098\n",
      "Epoch [518/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [519/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [520/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [521/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [522/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [523/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [524/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [525/1000] Fold 7, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [526/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [527/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [528/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [529/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [530/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [531/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [532/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [533/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [534/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [535/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0096\n",
      "Epoch [536/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [537/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [538/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [539/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [540/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [541/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [542/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [543/1000] Fold 7, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [544/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [545/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [546/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [547/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [548/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [549/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [550/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [551/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [552/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [553/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [554/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [555/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [556/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [557/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [558/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0096\n",
      "Epoch [559/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [560/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [561/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [562/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [563/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [564/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0097\n",
      "Epoch [565/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [566/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [567/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [568/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [569/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0097\n",
      "Epoch [570/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [571/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [572/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [573/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [574/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [575/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [576/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [577/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [578/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [579/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [580/1000] Fold 7, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [581/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [582/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [583/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [584/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [585/1000] Fold 7, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [586/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [587/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [588/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [589/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [590/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [591/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [592/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [593/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [594/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [595/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [596/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [597/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [598/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [599/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [600/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [601/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [602/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [603/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [604/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [605/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [606/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [607/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [608/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [609/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [610/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [611/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [612/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [613/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [614/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [615/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [616/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [617/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [618/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [619/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [620/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [621/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [622/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [623/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [624/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [625/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [626/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [627/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [628/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [629/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [630/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [631/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0096\n",
      "Epoch [632/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [633/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [634/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [635/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [636/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [637/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [638/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [639/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [640/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [641/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0095\n",
      "Epoch [642/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [643/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [644/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [645/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [646/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [647/1000] Fold 7, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [648/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [649/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [650/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [651/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [652/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [653/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [654/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [655/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [656/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [657/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [658/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [659/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [660/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [661/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [662/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [663/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [664/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [665/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [666/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [667/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [668/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [669/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0095\n",
      "Epoch [670/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [671/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [672/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [673/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [674/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [675/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [676/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [677/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [678/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [679/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [680/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [681/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [682/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [683/1000] Fold 7, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [684/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [685/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [686/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [687/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [688/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [689/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [690/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [691/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [692/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [693/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [694/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [695/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [696/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [697/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [698/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [699/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [700/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [701/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [702/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [703/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [704/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [705/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [706/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [707/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [708/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [709/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [710/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [711/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [712/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [713/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [714/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [715/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [716/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [717/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [718/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [719/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [720/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [721/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [722/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [723/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [724/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [725/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [726/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [727/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [728/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [729/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [730/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [731/1000] Fold 7, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [732/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [733/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [734/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [735/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [736/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [737/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [738/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [739/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [740/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [741/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [742/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [743/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [744/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [745/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [746/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [747/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [748/1000] Fold 7, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [749/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [750/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [751/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [752/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [753/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [754/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [755/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [756/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [757/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [758/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [759/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [760/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [761/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [762/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [763/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [764/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [765/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [766/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [767/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [768/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [769/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [770/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [771/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [772/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [773/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [774/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0090\n",
      "Epoch [775/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [776/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [777/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [778/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [779/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [780/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [781/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [782/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [783/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [784/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [785/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [786/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [787/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [788/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [789/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [790/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [791/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [792/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [793/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [794/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [795/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [796/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [797/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [798/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [799/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [800/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [801/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [802/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [803/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [804/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [805/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [806/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [807/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [808/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [809/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [810/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [811/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [812/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [813/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [814/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [815/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [816/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [817/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [818/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [819/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [820/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [821/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [822/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [823/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [824/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [825/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [826/1000] Fold 7, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [827/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [828/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [829/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [830/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [831/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [832/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [833/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [834/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [835/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [836/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [837/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [838/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [839/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [840/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [841/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [842/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [843/1000] Fold 7, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [844/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [845/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [846/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [847/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [848/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0089\n",
      "Epoch [849/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [850/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [851/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [852/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [853/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [854/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [855/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [856/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [857/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [858/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [859/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [860/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [861/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [862/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [863/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [864/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [865/1000] Fold 7, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [866/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [867/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [868/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [869/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [870/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [871/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [872/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [873/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [874/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [875/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [876/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [877/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [878/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [879/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [880/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [881/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [882/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [883/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [884/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [885/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [886/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [887/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [888/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [889/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [890/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [891/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [892/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [893/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [894/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [895/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [896/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [897/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [898/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [899/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [900/1000] Fold 7, Train Loss: 0.0102, Val Loss: 0.0090\n",
      "Epoch [901/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [902/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [903/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [904/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [905/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [906/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [907/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [908/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [909/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [910/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [911/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [912/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [913/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [914/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [915/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [916/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [917/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [918/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [919/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [920/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [921/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0089\n",
      "Epoch [922/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [923/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [924/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [925/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [926/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [927/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [928/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [929/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [930/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [931/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [932/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [933/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [934/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [935/1000] Fold 7, Train Loss: 0.0096, Val Loss: 0.0091\n",
      "Epoch [936/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [937/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [938/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [939/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [940/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [941/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [942/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [943/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [944/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [945/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [946/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [947/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [948/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [949/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [950/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [951/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [952/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [953/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [954/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [955/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [956/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [957/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [958/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [959/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [960/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [961/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [962/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [963/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [964/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [965/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [966/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [967/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [968/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [969/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [970/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [971/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [972/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [973/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [974/1000] Fold 7, Train Loss: 0.0100, Val Loss: 0.0090\n",
      "Epoch [975/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0090\n",
      "Epoch [976/1000] Fold 7, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [977/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [978/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [979/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [980/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [981/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [982/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [983/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [984/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0090\n",
      "Epoch [985/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [986/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [987/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [988/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [989/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [990/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [991/1000] Fold 7, Train Loss: 0.0096, Val Loss: 0.0091\n",
      "Epoch [992/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0088\n",
      "Epoch [993/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [994/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [995/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0089\n",
      "Epoch [996/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [997/1000] Fold 7, Train Loss: 0.0098, Val Loss: 0.0089\n",
      "Epoch [998/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [999/1000] Fold 7, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [1000/1000] Fold 7, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Fold 8/10\n",
      "Epoch [1/1000] Fold 8, Train Loss: 0.1409, Val Loss: 0.0693\n",
      "Epoch [2/1000] Fold 8, Train Loss: 0.0717, Val Loss: 0.0460\n",
      "Epoch [3/1000] Fold 8, Train Loss: 0.0496, Val Loss: 0.0365\n",
      "Epoch [4/1000] Fold 8, Train Loss: 0.0407, Val Loss: 0.0324\n",
      "Epoch [5/1000] Fold 8, Train Loss: 0.0360, Val Loss: 0.0304\n",
      "Epoch [6/1000] Fold 8, Train Loss: 0.0332, Val Loss: 0.0289\n",
      "Epoch [7/1000] Fold 8, Train Loss: 0.0316, Val Loss: 0.0281\n",
      "Epoch [8/1000] Fold 8, Train Loss: 0.0304, Val Loss: 0.0274\n",
      "Epoch [9/1000] Fold 8, Train Loss: 0.0293, Val Loss: 0.0269\n",
      "Epoch [10/1000] Fold 8, Train Loss: 0.0285, Val Loss: 0.0266\n",
      "Epoch [11/1000] Fold 8, Train Loss: 0.0281, Val Loss: 0.0262\n",
      "Epoch [12/1000] Fold 8, Train Loss: 0.0280, Val Loss: 0.0261\n",
      "Epoch [13/1000] Fold 8, Train Loss: 0.0271, Val Loss: 0.0257\n",
      "Epoch [14/1000] Fold 8, Train Loss: 0.0269, Val Loss: 0.0253\n",
      "Epoch [15/1000] Fold 8, Train Loss: 0.0268, Val Loss: 0.0254\n",
      "Epoch [16/1000] Fold 8, Train Loss: 0.0264, Val Loss: 0.0251\n",
      "Epoch [17/1000] Fold 8, Train Loss: 0.0257, Val Loss: 0.0246\n",
      "Epoch [18/1000] Fold 8, Train Loss: 0.0260, Val Loss: 0.0243\n",
      "Epoch [19/1000] Fold 8, Train Loss: 0.0256, Val Loss: 0.0238\n",
      "Epoch [20/1000] Fold 8, Train Loss: 0.0254, Val Loss: 0.0237\n",
      "Epoch [21/1000] Fold 8, Train Loss: 0.0251, Val Loss: 0.0235\n",
      "Epoch [22/1000] Fold 8, Train Loss: 0.0252, Val Loss: 0.0228\n",
      "Epoch [23/1000] Fold 8, Train Loss: 0.0244, Val Loss: 0.0225\n",
      "Epoch [24/1000] Fold 8, Train Loss: 0.0242, Val Loss: 0.0226\n",
      "Epoch [25/1000] Fold 8, Train Loss: 0.0239, Val Loss: 0.0219\n",
      "Epoch [26/1000] Fold 8, Train Loss: 0.0236, Val Loss: 0.0220\n",
      "Epoch [27/1000] Fold 8, Train Loss: 0.0235, Val Loss: 0.0213\n",
      "Epoch [28/1000] Fold 8, Train Loss: 0.0232, Val Loss: 0.0209\n",
      "Epoch [29/1000] Fold 8, Train Loss: 0.0229, Val Loss: 0.0207\n",
      "Epoch [30/1000] Fold 8, Train Loss: 0.0228, Val Loss: 0.0204\n",
      "Epoch [31/1000] Fold 8, Train Loss: 0.0223, Val Loss: 0.0198\n",
      "Epoch [32/1000] Fold 8, Train Loss: 0.0225, Val Loss: 0.0197\n",
      "Epoch [33/1000] Fold 8, Train Loss: 0.0221, Val Loss: 0.0196\n",
      "Epoch [34/1000] Fold 8, Train Loss: 0.0219, Val Loss: 0.0191\n",
      "Epoch [35/1000] Fold 8, Train Loss: 0.0215, Val Loss: 0.0193\n",
      "Epoch [36/1000] Fold 8, Train Loss: 0.0214, Val Loss: 0.0188\n",
      "Epoch [37/1000] Fold 8, Train Loss: 0.0214, Val Loss: 0.0187\n",
      "Epoch [38/1000] Fold 8, Train Loss: 0.0208, Val Loss: 0.0182\n",
      "Epoch [39/1000] Fold 8, Train Loss: 0.0208, Val Loss: 0.0182\n",
      "Epoch [40/1000] Fold 8, Train Loss: 0.0207, Val Loss: 0.0182\n",
      "Epoch [41/1000] Fold 8, Train Loss: 0.0203, Val Loss: 0.0178\n",
      "Epoch [42/1000] Fold 8, Train Loss: 0.0205, Val Loss: 0.0178\n",
      "Epoch [43/1000] Fold 8, Train Loss: 0.0204, Val Loss: 0.0176\n",
      "Epoch [44/1000] Fold 8, Train Loss: 0.0204, Val Loss: 0.0173\n",
      "Epoch [45/1000] Fold 8, Train Loss: 0.0203, Val Loss: 0.0172\n",
      "Epoch [46/1000] Fold 8, Train Loss: 0.0200, Val Loss: 0.0170\n",
      "Epoch [47/1000] Fold 8, Train Loss: 0.0200, Val Loss: 0.0170\n",
      "Epoch [48/1000] Fold 8, Train Loss: 0.0200, Val Loss: 0.0172\n",
      "Epoch [49/1000] Fold 8, Train Loss: 0.0198, Val Loss: 0.0169\n",
      "Epoch [50/1000] Fold 8, Train Loss: 0.0193, Val Loss: 0.0164\n",
      "Epoch [51/1000] Fold 8, Train Loss: 0.0195, Val Loss: 0.0164\n",
      "Epoch [52/1000] Fold 8, Train Loss: 0.0192, Val Loss: 0.0165\n",
      "Epoch [53/1000] Fold 8, Train Loss: 0.0190, Val Loss: 0.0165\n",
      "Epoch [54/1000] Fold 8, Train Loss: 0.0193, Val Loss: 0.0165\n",
      "Epoch [55/1000] Fold 8, Train Loss: 0.0192, Val Loss: 0.0162\n",
      "Epoch [56/1000] Fold 8, Train Loss: 0.0187, Val Loss: 0.0160\n",
      "Epoch [57/1000] Fold 8, Train Loss: 0.0188, Val Loss: 0.0159\n",
      "Epoch [58/1000] Fold 8, Train Loss: 0.0193, Val Loss: 0.0156\n",
      "Epoch [59/1000] Fold 8, Train Loss: 0.0188, Val Loss: 0.0159\n",
      "Epoch [60/1000] Fold 8, Train Loss: 0.0187, Val Loss: 0.0157\n",
      "Epoch [61/1000] Fold 8, Train Loss: 0.0186, Val Loss: 0.0156\n",
      "Epoch [62/1000] Fold 8, Train Loss: 0.0185, Val Loss: 0.0157\n",
      "Epoch [63/1000] Fold 8, Train Loss: 0.0183, Val Loss: 0.0154\n",
      "Epoch [64/1000] Fold 8, Train Loss: 0.0182, Val Loss: 0.0154\n",
      "Epoch [65/1000] Fold 8, Train Loss: 0.0181, Val Loss: 0.0152\n",
      "Epoch [66/1000] Fold 8, Train Loss: 0.0185, Val Loss: 0.0153\n",
      "Epoch [67/1000] Fold 8, Train Loss: 0.0181, Val Loss: 0.0151\n",
      "Epoch [68/1000] Fold 8, Train Loss: 0.0177, Val Loss: 0.0152\n",
      "Epoch [69/1000] Fold 8, Train Loss: 0.0179, Val Loss: 0.0151\n",
      "Epoch [70/1000] Fold 8, Train Loss: 0.0181, Val Loss: 0.0152\n",
      "Epoch [71/1000] Fold 8, Train Loss: 0.0176, Val Loss: 0.0147\n",
      "Epoch [72/1000] Fold 8, Train Loss: 0.0177, Val Loss: 0.0147\n",
      "Epoch [73/1000] Fold 8, Train Loss: 0.0178, Val Loss: 0.0145\n",
      "Epoch [74/1000] Fold 8, Train Loss: 0.0178, Val Loss: 0.0147\n",
      "Epoch [75/1000] Fold 8, Train Loss: 0.0178, Val Loss: 0.0144\n",
      "Epoch [76/1000] Fold 8, Train Loss: 0.0180, Val Loss: 0.0146\n",
      "Epoch [77/1000] Fold 8, Train Loss: 0.0172, Val Loss: 0.0144\n",
      "Epoch [78/1000] Fold 8, Train Loss: 0.0173, Val Loss: 0.0144\n",
      "Epoch [79/1000] Fold 8, Train Loss: 0.0174, Val Loss: 0.0142\n",
      "Epoch [80/1000] Fold 8, Train Loss: 0.0173, Val Loss: 0.0144\n",
      "Epoch [81/1000] Fold 8, Train Loss: 0.0173, Val Loss: 0.0142\n",
      "Epoch [82/1000] Fold 8, Train Loss: 0.0171, Val Loss: 0.0142\n",
      "Epoch [83/1000] Fold 8, Train Loss: 0.0170, Val Loss: 0.0142\n",
      "Epoch [84/1000] Fold 8, Train Loss: 0.0170, Val Loss: 0.0141\n",
      "Epoch [85/1000] Fold 8, Train Loss: 0.0173, Val Loss: 0.0139\n",
      "Epoch [86/1000] Fold 8, Train Loss: 0.0170, Val Loss: 0.0140\n",
      "Epoch [87/1000] Fold 8, Train Loss: 0.0169, Val Loss: 0.0141\n",
      "Epoch [88/1000] Fold 8, Train Loss: 0.0168, Val Loss: 0.0141\n",
      "Epoch [89/1000] Fold 8, Train Loss: 0.0169, Val Loss: 0.0139\n",
      "Epoch [90/1000] Fold 8, Train Loss: 0.0171, Val Loss: 0.0138\n",
      "Epoch [91/1000] Fold 8, Train Loss: 0.0168, Val Loss: 0.0136\n",
      "Epoch [92/1000] Fold 8, Train Loss: 0.0162, Val Loss: 0.0137\n",
      "Epoch [93/1000] Fold 8, Train Loss: 0.0167, Val Loss: 0.0134\n",
      "Epoch [94/1000] Fold 8, Train Loss: 0.0167, Val Loss: 0.0135\n",
      "Epoch [95/1000] Fold 8, Train Loss: 0.0165, Val Loss: 0.0137\n",
      "Epoch [96/1000] Fold 8, Train Loss: 0.0166, Val Loss: 0.0136\n",
      "Epoch [97/1000] Fold 8, Train Loss: 0.0165, Val Loss: 0.0132\n",
      "Epoch [98/1000] Fold 8, Train Loss: 0.0161, Val Loss: 0.0132\n",
      "Epoch [99/1000] Fold 8, Train Loss: 0.0164, Val Loss: 0.0131\n",
      "Epoch [100/1000] Fold 8, Train Loss: 0.0161, Val Loss: 0.0134\n",
      "Epoch [101/1000] Fold 8, Train Loss: 0.0161, Val Loss: 0.0133\n",
      "Epoch [102/1000] Fold 8, Train Loss: 0.0159, Val Loss: 0.0131\n",
      "Epoch [103/1000] Fold 8, Train Loss: 0.0163, Val Loss: 0.0133\n",
      "Epoch [104/1000] Fold 8, Train Loss: 0.0160, Val Loss: 0.0133\n",
      "Epoch [105/1000] Fold 8, Train Loss: 0.0162, Val Loss: 0.0129\n",
      "Epoch [106/1000] Fold 8, Train Loss: 0.0161, Val Loss: 0.0131\n",
      "Epoch [107/1000] Fold 8, Train Loss: 0.0163, Val Loss: 0.0131\n",
      "Epoch [108/1000] Fold 8, Train Loss: 0.0161, Val Loss: 0.0129\n",
      "Epoch [109/1000] Fold 8, Train Loss: 0.0159, Val Loss: 0.0130\n",
      "Epoch [110/1000] Fold 8, Train Loss: 0.0160, Val Loss: 0.0129\n",
      "Epoch [111/1000] Fold 8, Train Loss: 0.0158, Val Loss: 0.0128\n",
      "Epoch [112/1000] Fold 8, Train Loss: 0.0158, Val Loss: 0.0126\n",
      "Epoch [113/1000] Fold 8, Train Loss: 0.0155, Val Loss: 0.0128\n",
      "Epoch [114/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0128\n",
      "Epoch [115/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0126\n",
      "Epoch [116/1000] Fold 8, Train Loss: 0.0153, Val Loss: 0.0126\n",
      "Epoch [117/1000] Fold 8, Train Loss: 0.0157, Val Loss: 0.0125\n",
      "Epoch [118/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0126\n",
      "Epoch [119/1000] Fold 8, Train Loss: 0.0155, Val Loss: 0.0126\n",
      "Epoch [120/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0124\n",
      "Epoch [121/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0126\n",
      "Epoch [122/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0124\n",
      "Epoch [123/1000] Fold 8, Train Loss: 0.0151, Val Loss: 0.0124\n",
      "Epoch [124/1000] Fold 8, Train Loss: 0.0156, Val Loss: 0.0124\n",
      "Epoch [125/1000] Fold 8, Train Loss: 0.0154, Val Loss: 0.0123\n",
      "Epoch [126/1000] Fold 8, Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [127/1000] Fold 8, Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [128/1000] Fold 8, Train Loss: 0.0152, Val Loss: 0.0120\n",
      "Epoch [129/1000] Fold 8, Train Loss: 0.0153, Val Loss: 0.0122\n",
      "Epoch [130/1000] Fold 8, Train Loss: 0.0149, Val Loss: 0.0122\n",
      "Epoch [131/1000] Fold 8, Train Loss: 0.0153, Val Loss: 0.0123\n",
      "Epoch [132/1000] Fold 8, Train Loss: 0.0152, Val Loss: 0.0121\n",
      "Epoch [133/1000] Fold 8, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [134/1000] Fold 8, Train Loss: 0.0150, Val Loss: 0.0121\n",
      "Epoch [135/1000] Fold 8, Train Loss: 0.0150, Val Loss: 0.0122\n",
      "Epoch [136/1000] Fold 8, Train Loss: 0.0150, Val Loss: 0.0121\n",
      "Epoch [137/1000] Fold 8, Train Loss: 0.0150, Val Loss: 0.0121\n",
      "Epoch [138/1000] Fold 8, Train Loss: 0.0149, Val Loss: 0.0120\n",
      "Epoch [139/1000] Fold 8, Train Loss: 0.0151, Val Loss: 0.0121\n",
      "Epoch [140/1000] Fold 8, Train Loss: 0.0147, Val Loss: 0.0120\n",
      "Epoch [141/1000] Fold 8, Train Loss: 0.0150, Val Loss: 0.0117\n",
      "Epoch [142/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0117\n",
      "Epoch [143/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0118\n",
      "Epoch [144/1000] Fold 8, Train Loss: 0.0147, Val Loss: 0.0116\n",
      "Epoch [145/1000] Fold 8, Train Loss: 0.0147, Val Loss: 0.0118\n",
      "Epoch [146/1000] Fold 8, Train Loss: 0.0147, Val Loss: 0.0118\n",
      "Epoch [147/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0118\n",
      "Epoch [148/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0121\n",
      "Epoch [149/1000] Fold 8, Train Loss: 0.0145, Val Loss: 0.0118\n",
      "Epoch [150/1000] Fold 8, Train Loss: 0.0147, Val Loss: 0.0117\n",
      "Epoch [151/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0116\n",
      "Epoch [152/1000] Fold 8, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [153/1000] Fold 8, Train Loss: 0.0149, Val Loss: 0.0117\n",
      "Epoch [154/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0117\n",
      "Epoch [155/1000] Fold 8, Train Loss: 0.0145, Val Loss: 0.0117\n",
      "Epoch [156/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0116\n",
      "Epoch [157/1000] Fold 8, Train Loss: 0.0146, Val Loss: 0.0111\n",
      "Epoch [158/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0116\n",
      "Epoch [159/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0115\n",
      "Epoch [160/1000] Fold 8, Train Loss: 0.0143, Val Loss: 0.0115\n",
      "Epoch [161/1000] Fold 8, Train Loss: 0.0145, Val Loss: 0.0117\n",
      "Epoch [162/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0114\n",
      "Epoch [163/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [164/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0118\n",
      "Epoch [165/1000] Fold 8, Train Loss: 0.0141, Val Loss: 0.0113\n",
      "Epoch [166/1000] Fold 8, Train Loss: 0.0141, Val Loss: 0.0113\n",
      "Epoch [167/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0114\n",
      "Epoch [168/1000] Fold 8, Train Loss: 0.0144, Val Loss: 0.0115\n",
      "Epoch [169/1000] Fold 8, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [170/1000] Fold 8, Train Loss: 0.0143, Val Loss: 0.0113\n",
      "Epoch [171/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0115\n",
      "Epoch [172/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0115\n",
      "Epoch [173/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0112\n",
      "Epoch [174/1000] Fold 8, Train Loss: 0.0141, Val Loss: 0.0113\n",
      "Epoch [175/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [176/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0114\n",
      "Epoch [177/1000] Fold 8, Train Loss: 0.0141, Val Loss: 0.0112\n",
      "Epoch [178/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0114\n",
      "Epoch [179/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [180/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [181/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0110\n",
      "Epoch [182/1000] Fold 8, Train Loss: 0.0141, Val Loss: 0.0111\n",
      "Epoch [183/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0112\n",
      "Epoch [184/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0113\n",
      "Epoch [185/1000] Fold 8, Train Loss: 0.0138, Val Loss: 0.0113\n",
      "Epoch [186/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [187/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0111\n",
      "Epoch [188/1000] Fold 8, Train Loss: 0.0142, Val Loss: 0.0112\n",
      "Epoch [189/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [190/1000] Fold 8, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [191/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [192/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0112\n",
      "Epoch [193/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [194/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0108\n",
      "Epoch [195/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0108\n",
      "Epoch [196/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [197/1000] Fold 8, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [198/1000] Fold 8, Train Loss: 0.0138, Val Loss: 0.0111\n",
      "Epoch [199/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0109\n",
      "Epoch [200/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [201/1000] Fold 8, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [202/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [203/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0110\n",
      "Epoch [204/1000] Fold 8, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [205/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [206/1000] Fold 8, Train Loss: 0.0136, Val Loss: 0.0110\n",
      "Epoch [207/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [208/1000] Fold 8, Train Loss: 0.0138, Val Loss: 0.0109\n",
      "Epoch [209/1000] Fold 8, Train Loss: 0.0138, Val Loss: 0.0110\n",
      "Epoch [210/1000] Fold 8, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [211/1000] Fold 8, Train Loss: 0.0136, Val Loss: 0.0109\n",
      "Epoch [212/1000] Fold 8, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [213/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [214/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [215/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [216/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0108\n",
      "Epoch [217/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [218/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0108\n",
      "Epoch [219/1000] Fold 8, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [220/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [221/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0108\n",
      "Epoch [222/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [223/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0106\n",
      "Epoch [224/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [225/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [226/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [227/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [228/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [229/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0102\n",
      "Epoch [230/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0107\n",
      "Epoch [231/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [232/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0106\n",
      "Epoch [233/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [234/1000] Fold 8, Train Loss: 0.0135, Val Loss: 0.0104\n",
      "Epoch [235/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [236/1000] Fold 8, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [237/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0107\n",
      "Epoch [238/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0106\n",
      "Epoch [239/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [240/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0106\n",
      "Epoch [241/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [242/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [243/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [244/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [245/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [246/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [247/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [248/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [249/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [250/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [251/1000] Fold 8, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [252/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [253/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [254/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0104\n",
      "Epoch [255/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [256/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0104\n",
      "Epoch [257/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [258/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [259/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [260/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [261/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [262/1000] Fold 8, Train Loss: 0.0131, Val Loss: 0.0103\n",
      "Epoch [263/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [264/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0100\n",
      "Epoch [265/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [266/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [267/1000] Fold 8, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [268/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [269/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [270/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [271/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [272/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [273/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [274/1000] Fold 8, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [275/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [276/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [277/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [278/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [279/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [280/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [281/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [282/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [283/1000] Fold 8, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [284/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [285/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [286/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [287/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0098\n",
      "Epoch [288/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [289/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [290/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [291/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [292/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [293/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [294/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [295/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0104\n",
      "Epoch [296/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [297/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0101\n",
      "Epoch [298/1000] Fold 8, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [299/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [300/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [301/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [302/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [303/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [304/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [305/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [306/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [307/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [308/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [309/1000] Fold 8, Train Loss: 0.0128, Val Loss: 0.0098\n",
      "Epoch [310/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [311/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0101\n",
      "Epoch [312/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [313/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [314/1000] Fold 8, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [315/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [316/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [317/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [318/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [319/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [320/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [321/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [322/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [323/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [324/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [325/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [326/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [327/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [328/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [329/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [330/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [331/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [332/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [333/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [334/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [335/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [336/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [337/1000] Fold 8, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [338/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [339/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [340/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [341/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [342/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [343/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [344/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [345/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [346/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [347/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [348/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [349/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [350/1000] Fold 8, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [351/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [352/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [353/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [354/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [355/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [356/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [357/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [358/1000] Fold 8, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [359/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [360/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [361/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [362/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [363/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [364/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [365/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [366/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [367/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [368/1000] Fold 8, Train Loss: 0.0122, Val Loss: 0.0094\n",
      "Epoch [369/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [370/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [371/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [372/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [373/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [374/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [375/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [376/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [377/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [378/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [379/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [380/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [381/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [382/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [383/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [384/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [385/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [386/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [387/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [388/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [389/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [390/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [391/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [392/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [393/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [394/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [395/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [396/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [397/1000] Fold 8, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [398/1000] Fold 8, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [399/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [400/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [401/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [402/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [403/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [404/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [405/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [406/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [407/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [408/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [409/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [410/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [411/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [412/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [413/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [414/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [415/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [416/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [417/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [418/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [419/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [420/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [421/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [422/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [423/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [424/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [425/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [426/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [427/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [428/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [429/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [430/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [431/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [432/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [433/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [434/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [435/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [436/1000] Fold 8, Train Loss: 0.0119, Val Loss: 0.0091\n",
      "Epoch [437/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [438/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [439/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [440/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [441/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [442/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [443/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [444/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [445/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [446/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [447/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [448/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [449/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [450/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0093\n",
      "Epoch [451/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0089\n",
      "Epoch [452/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [453/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [454/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [455/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [456/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [457/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [458/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [459/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [460/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [461/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [462/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [463/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [464/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [465/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [466/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [467/1000] Fold 8, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [468/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [469/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [470/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [471/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [472/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [473/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [474/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [475/1000] Fold 8, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [476/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [477/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [478/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [479/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [480/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [481/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [482/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [483/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [484/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [485/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [486/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [487/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [488/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [489/1000] Fold 8, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [490/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [491/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [492/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [493/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [494/1000] Fold 8, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [495/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [496/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [497/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [498/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [499/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [500/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [501/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [502/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [503/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [504/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [505/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [506/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [507/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [508/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [509/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [510/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [511/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0088\n",
      "Epoch [512/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [513/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [514/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [515/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [516/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [517/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [518/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [519/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [520/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [521/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [522/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [523/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [524/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [525/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [526/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [527/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [528/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [529/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [530/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [531/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [532/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [533/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [534/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [535/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [536/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [537/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [538/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [539/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [540/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [541/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [542/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [543/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [544/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [545/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [546/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [547/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [548/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [549/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [550/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [551/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [552/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [553/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [554/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [555/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [556/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0087\n",
      "Epoch [557/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [558/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [559/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [560/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [561/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [562/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [563/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0091\n",
      "Epoch [564/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [565/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [566/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [567/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [568/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0091\n",
      "Epoch [569/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [570/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [571/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [572/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [573/1000] Fold 8, Train Loss: 0.0112, Val Loss: 0.0086\n",
      "Epoch [574/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [575/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [576/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [577/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [578/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [579/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [580/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [581/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [582/1000] Fold 8, Train Loss: 0.0114, Val Loss: 0.0089\n",
      "Epoch [583/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [584/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [585/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [586/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [587/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [588/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [589/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [590/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [591/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0086\n",
      "Epoch [592/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [593/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [594/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [595/1000] Fold 8, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [596/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [597/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [598/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [599/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [600/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [601/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [602/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [603/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [604/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [605/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [606/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [607/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [608/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [609/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [610/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [611/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [612/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0090\n",
      "Epoch [613/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [614/1000] Fold 8, Train Loss: 0.0113, Val Loss: 0.0088\n",
      "Epoch [615/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [616/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [617/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [618/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [619/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [620/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [621/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [622/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [623/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0091\n",
      "Epoch [624/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [625/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [626/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [627/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [628/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [629/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [630/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [631/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [632/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [633/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [634/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [635/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [636/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0089\n",
      "Epoch [637/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [638/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [639/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [640/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [641/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [642/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [643/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [644/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [645/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [646/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0090\n",
      "Epoch [647/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [648/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [649/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [650/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [651/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [652/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [653/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [654/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [655/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [656/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [657/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [658/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [659/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [660/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [661/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [662/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0089\n",
      "Epoch [663/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [664/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [665/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [666/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [667/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [668/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [669/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [670/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [671/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [672/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [673/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [674/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [675/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [676/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [677/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [678/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [679/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [680/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [681/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [682/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [683/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [684/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [685/1000] Fold 8, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [686/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [687/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [688/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [689/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [690/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [691/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [692/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [693/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [694/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [695/1000] Fold 8, Train Loss: 0.0110, Val Loss: 0.0085\n",
      "Epoch [696/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [697/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [698/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [699/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [700/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [701/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [702/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [703/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [704/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [705/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [706/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [707/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [708/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [709/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [710/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [711/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [712/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [713/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [714/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [715/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [716/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [717/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [718/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [719/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [720/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [721/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [722/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [723/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [724/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [725/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [726/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [727/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [728/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [729/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [730/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [731/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [732/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [733/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [734/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [735/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [736/1000] Fold 8, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [737/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [738/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [739/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [740/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [741/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [742/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [743/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [744/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [745/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [746/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [747/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [748/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [749/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [750/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [751/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [752/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [753/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [754/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [755/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [756/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [757/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [758/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [759/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [760/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [761/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [762/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [763/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [764/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [765/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [766/1000] Fold 8, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [767/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [768/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [769/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [770/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [771/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [772/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [773/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [774/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0088\n",
      "Epoch [775/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [776/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [777/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [778/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [779/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [780/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [781/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [782/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [783/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [784/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [785/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [786/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [787/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [788/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [789/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0089\n",
      "Epoch [790/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [791/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [792/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [793/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [794/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [795/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [796/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [797/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [798/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [799/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [800/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [801/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [802/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [803/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [804/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [805/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [806/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [807/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [808/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [809/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0088\n",
      "Epoch [810/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [811/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [812/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [813/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [814/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [815/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [816/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [817/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [818/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [819/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [820/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [821/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [822/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [823/1000] Fold 8, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [824/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [825/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [826/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [827/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [828/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [829/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [830/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [831/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [832/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [833/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [834/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [835/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [836/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [837/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [838/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [839/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [840/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [841/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [842/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [843/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [844/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [845/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [846/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [847/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [848/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [849/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [850/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [851/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [852/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [853/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [854/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0087\n",
      "Epoch [855/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [856/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [857/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [858/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [859/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [860/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [861/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [862/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [863/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [864/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [865/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [866/1000] Fold 8, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [867/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [868/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [869/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [870/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [871/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [872/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [873/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [874/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [875/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [876/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [877/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [878/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [879/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [880/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [881/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [882/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [883/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [884/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [885/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [886/1000] Fold 8, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [887/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [888/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [889/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [890/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [891/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [892/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [893/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [894/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [895/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [896/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [897/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [898/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [899/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [900/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [901/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [902/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [903/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [904/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0089\n",
      "Epoch [905/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [906/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [907/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [908/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [909/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [910/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [911/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [912/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [913/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [914/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0087\n",
      "Epoch [915/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [916/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [917/1000] Fold 8, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [918/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0087\n",
      "Epoch [919/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [920/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [921/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [922/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [923/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [924/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [925/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [926/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [927/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [928/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [929/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [930/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [931/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [932/1000] Fold 8, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [933/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [934/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [935/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [936/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [937/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [938/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [939/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [940/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [941/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [942/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [943/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0086\n",
      "Epoch [944/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [945/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [946/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [947/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [948/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [949/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [950/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [951/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [952/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [953/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [954/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [955/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [956/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [957/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [958/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [959/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [960/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [961/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [962/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [963/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [964/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [965/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [966/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [967/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [968/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [969/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [970/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [971/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [972/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [973/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [974/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [975/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [976/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [977/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [978/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [979/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [980/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [981/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [982/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [983/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0086\n",
      "Epoch [984/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [985/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [986/1000] Fold 8, Train Loss: 0.0101, Val Loss: 0.0086\n",
      "Epoch [987/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [988/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [989/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [990/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [991/1000] Fold 8, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [992/1000] Fold 8, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [993/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [994/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [995/1000] Fold 8, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [996/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [997/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [998/1000] Fold 8, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [999/1000] Fold 8, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [1000/1000] Fold 8, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Fold 9/10\n",
      "Epoch [1/1000] Fold 9, Train Loss: 0.1454, Val Loss: 0.0697\n",
      "Epoch [2/1000] Fold 9, Train Loss: 0.0763, Val Loss: 0.0453\n",
      "Epoch [3/1000] Fold 9, Train Loss: 0.0521, Val Loss: 0.0361\n",
      "Epoch [4/1000] Fold 9, Train Loss: 0.0421, Val Loss: 0.0315\n",
      "Epoch [5/1000] Fold 9, Train Loss: 0.0380, Val Loss: 0.0297\n",
      "Epoch [6/1000] Fold 9, Train Loss: 0.0348, Val Loss: 0.0281\n",
      "Epoch [7/1000] Fold 9, Train Loss: 0.0329, Val Loss: 0.0273\n",
      "Epoch [8/1000] Fold 9, Train Loss: 0.0317, Val Loss: 0.0264\n",
      "Epoch [9/1000] Fold 9, Train Loss: 0.0305, Val Loss: 0.0259\n",
      "Epoch [10/1000] Fold 9, Train Loss: 0.0297, Val Loss: 0.0253\n",
      "Epoch [11/1000] Fold 9, Train Loss: 0.0290, Val Loss: 0.0247\n",
      "Epoch [12/1000] Fold 9, Train Loss: 0.0285, Val Loss: 0.0243\n",
      "Epoch [13/1000] Fold 9, Train Loss: 0.0277, Val Loss: 0.0238\n",
      "Epoch [14/1000] Fold 9, Train Loss: 0.0273, Val Loss: 0.0235\n",
      "Epoch [15/1000] Fold 9, Train Loss: 0.0270, Val Loss: 0.0231\n",
      "Epoch [16/1000] Fold 9, Train Loss: 0.0270, Val Loss: 0.0227\n",
      "Epoch [17/1000] Fold 9, Train Loss: 0.0262, Val Loss: 0.0227\n",
      "Epoch [18/1000] Fold 9, Train Loss: 0.0262, Val Loss: 0.0222\n",
      "Epoch [19/1000] Fold 9, Train Loss: 0.0259, Val Loss: 0.0219\n",
      "Epoch [20/1000] Fold 9, Train Loss: 0.0254, Val Loss: 0.0216\n",
      "Epoch [21/1000] Fold 9, Train Loss: 0.0253, Val Loss: 0.0214\n",
      "Epoch [22/1000] Fold 9, Train Loss: 0.0253, Val Loss: 0.0211\n",
      "Epoch [23/1000] Fold 9, Train Loss: 0.0245, Val Loss: 0.0208\n",
      "Epoch [24/1000] Fold 9, Train Loss: 0.0246, Val Loss: 0.0208\n",
      "Epoch [25/1000] Fold 9, Train Loss: 0.0246, Val Loss: 0.0204\n",
      "Epoch [26/1000] Fold 9, Train Loss: 0.0240, Val Loss: 0.0201\n",
      "Epoch [27/1000] Fold 9, Train Loss: 0.0237, Val Loss: 0.0200\n",
      "Epoch [28/1000] Fold 9, Train Loss: 0.0236, Val Loss: 0.0197\n",
      "Epoch [29/1000] Fold 9, Train Loss: 0.0234, Val Loss: 0.0197\n",
      "Epoch [30/1000] Fold 9, Train Loss: 0.0233, Val Loss: 0.0193\n",
      "Epoch [31/1000] Fold 9, Train Loss: 0.0227, Val Loss: 0.0191\n",
      "Epoch [32/1000] Fold 9, Train Loss: 0.0229, Val Loss: 0.0188\n",
      "Epoch [33/1000] Fold 9, Train Loss: 0.0227, Val Loss: 0.0188\n",
      "Epoch [34/1000] Fold 9, Train Loss: 0.0223, Val Loss: 0.0184\n",
      "Epoch [35/1000] Fold 9, Train Loss: 0.0224, Val Loss: 0.0182\n",
      "Epoch [36/1000] Fold 9, Train Loss: 0.0219, Val Loss: 0.0179\n",
      "Epoch [37/1000] Fold 9, Train Loss: 0.0219, Val Loss: 0.0177\n",
      "Epoch [38/1000] Fold 9, Train Loss: 0.0214, Val Loss: 0.0179\n",
      "Epoch [39/1000] Fold 9, Train Loss: 0.0215, Val Loss: 0.0174\n",
      "Epoch [40/1000] Fold 9, Train Loss: 0.0213, Val Loss: 0.0172\n",
      "Epoch [41/1000] Fold 9, Train Loss: 0.0211, Val Loss: 0.0172\n",
      "Epoch [42/1000] Fold 9, Train Loss: 0.0211, Val Loss: 0.0171\n",
      "Epoch [43/1000] Fold 9, Train Loss: 0.0209, Val Loss: 0.0166\n",
      "Epoch [44/1000] Fold 9, Train Loss: 0.0205, Val Loss: 0.0167\n",
      "Epoch [45/1000] Fold 9, Train Loss: 0.0204, Val Loss: 0.0164\n",
      "Epoch [46/1000] Fold 9, Train Loss: 0.0203, Val Loss: 0.0164\n",
      "Epoch [47/1000] Fold 9, Train Loss: 0.0201, Val Loss: 0.0163\n",
      "Epoch [48/1000] Fold 9, Train Loss: 0.0201, Val Loss: 0.0161\n",
      "Epoch [49/1000] Fold 9, Train Loss: 0.0199, Val Loss: 0.0161\n",
      "Epoch [50/1000] Fold 9, Train Loss: 0.0197, Val Loss: 0.0160\n",
      "Epoch [51/1000] Fold 9, Train Loss: 0.0195, Val Loss: 0.0158\n",
      "Epoch [52/1000] Fold 9, Train Loss: 0.0197, Val Loss: 0.0158\n",
      "Epoch [53/1000] Fold 9, Train Loss: 0.0194, Val Loss: 0.0158\n",
      "Epoch [54/1000] Fold 9, Train Loss: 0.0192, Val Loss: 0.0154\n",
      "Epoch [55/1000] Fold 9, Train Loss: 0.0190, Val Loss: 0.0154\n",
      "Epoch [56/1000] Fold 9, Train Loss: 0.0192, Val Loss: 0.0153\n",
      "Epoch [57/1000] Fold 9, Train Loss: 0.0190, Val Loss: 0.0151\n",
      "Epoch [58/1000] Fold 9, Train Loss: 0.0189, Val Loss: 0.0149\n",
      "Epoch [59/1000] Fold 9, Train Loss: 0.0188, Val Loss: 0.0148\n",
      "Epoch [60/1000] Fold 9, Train Loss: 0.0189, Val Loss: 0.0151\n",
      "Epoch [61/1000] Fold 9, Train Loss: 0.0190, Val Loss: 0.0148\n",
      "Epoch [62/1000] Fold 9, Train Loss: 0.0186, Val Loss: 0.0149\n",
      "Epoch [63/1000] Fold 9, Train Loss: 0.0186, Val Loss: 0.0146\n",
      "Epoch [64/1000] Fold 9, Train Loss: 0.0185, Val Loss: 0.0147\n",
      "Epoch [65/1000] Fold 9, Train Loss: 0.0184, Val Loss: 0.0146\n",
      "Epoch [66/1000] Fold 9, Train Loss: 0.0180, Val Loss: 0.0144\n",
      "Epoch [67/1000] Fold 9, Train Loss: 0.0181, Val Loss: 0.0144\n",
      "Epoch [68/1000] Fold 9, Train Loss: 0.0177, Val Loss: 0.0143\n",
      "Epoch [69/1000] Fold 9, Train Loss: 0.0180, Val Loss: 0.0142\n",
      "Epoch [70/1000] Fold 9, Train Loss: 0.0178, Val Loss: 0.0139\n",
      "Epoch [71/1000] Fold 9, Train Loss: 0.0178, Val Loss: 0.0141\n",
      "Epoch [72/1000] Fold 9, Train Loss: 0.0178, Val Loss: 0.0141\n",
      "Epoch [73/1000] Fold 9, Train Loss: 0.0174, Val Loss: 0.0138\n",
      "Epoch [74/1000] Fold 9, Train Loss: 0.0176, Val Loss: 0.0138\n",
      "Epoch [75/1000] Fold 9, Train Loss: 0.0171, Val Loss: 0.0138\n",
      "Epoch [76/1000] Fold 9, Train Loss: 0.0172, Val Loss: 0.0136\n",
      "Epoch [77/1000] Fold 9, Train Loss: 0.0173, Val Loss: 0.0135\n",
      "Epoch [78/1000] Fold 9, Train Loss: 0.0174, Val Loss: 0.0137\n",
      "Epoch [79/1000] Fold 9, Train Loss: 0.0168, Val Loss: 0.0136\n",
      "Epoch [80/1000] Fold 9, Train Loss: 0.0172, Val Loss: 0.0133\n",
      "Epoch [81/1000] Fold 9, Train Loss: 0.0170, Val Loss: 0.0134\n",
      "Epoch [82/1000] Fold 9, Train Loss: 0.0171, Val Loss: 0.0135\n",
      "Epoch [83/1000] Fold 9, Train Loss: 0.0167, Val Loss: 0.0133\n",
      "Epoch [84/1000] Fold 9, Train Loss: 0.0172, Val Loss: 0.0135\n",
      "Epoch [85/1000] Fold 9, Train Loss: 0.0168, Val Loss: 0.0132\n",
      "Epoch [86/1000] Fold 9, Train Loss: 0.0167, Val Loss: 0.0131\n",
      "Epoch [87/1000] Fold 9, Train Loss: 0.0169, Val Loss: 0.0131\n",
      "Epoch [88/1000] Fold 9, Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Epoch [89/1000] Fold 9, Train Loss: 0.0166, Val Loss: 0.0132\n",
      "Epoch [90/1000] Fold 9, Train Loss: 0.0167, Val Loss: 0.0129\n",
      "Epoch [91/1000] Fold 9, Train Loss: 0.0162, Val Loss: 0.0129\n",
      "Epoch [92/1000] Fold 9, Train Loss: 0.0164, Val Loss: 0.0128\n",
      "Epoch [93/1000] Fold 9, Train Loss: 0.0163, Val Loss: 0.0129\n",
      "Epoch [94/1000] Fold 9, Train Loss: 0.0161, Val Loss: 0.0126\n",
      "Epoch [95/1000] Fold 9, Train Loss: 0.0161, Val Loss: 0.0127\n",
      "Epoch [96/1000] Fold 9, Train Loss: 0.0162, Val Loss: 0.0126\n",
      "Epoch [97/1000] Fold 9, Train Loss: 0.0160, Val Loss: 0.0126\n",
      "Epoch [98/1000] Fold 9, Train Loss: 0.0161, Val Loss: 0.0128\n",
      "Epoch [99/1000] Fold 9, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [100/1000] Fold 9, Train Loss: 0.0161, Val Loss: 0.0125\n",
      "Epoch [101/1000] Fold 9, Train Loss: 0.0156, Val Loss: 0.0126\n",
      "Epoch [102/1000] Fold 9, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [103/1000] Fold 9, Train Loss: 0.0159, Val Loss: 0.0122\n",
      "Epoch [104/1000] Fold 9, Train Loss: 0.0159, Val Loss: 0.0122\n",
      "Epoch [105/1000] Fold 9, Train Loss: 0.0160, Val Loss: 0.0122\n",
      "Epoch [106/1000] Fold 9, Train Loss: 0.0158, Val Loss: 0.0122\n",
      "Epoch [107/1000] Fold 9, Train Loss: 0.0157, Val Loss: 0.0122\n",
      "Epoch [108/1000] Fold 9, Train Loss: 0.0153, Val Loss: 0.0120\n",
      "Epoch [109/1000] Fold 9, Train Loss: 0.0154, Val Loss: 0.0120\n",
      "Epoch [110/1000] Fold 9, Train Loss: 0.0158, Val Loss: 0.0120\n",
      "Epoch [111/1000] Fold 9, Train Loss: 0.0156, Val Loss: 0.0120\n",
      "Epoch [112/1000] Fold 9, Train Loss: 0.0157, Val Loss: 0.0120\n",
      "Epoch [113/1000] Fold 9, Train Loss: 0.0156, Val Loss: 0.0119\n",
      "Epoch [114/1000] Fold 9, Train Loss: 0.0153, Val Loss: 0.0119\n",
      "Epoch [115/1000] Fold 9, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [116/1000] Fold 9, Train Loss: 0.0154, Val Loss: 0.0119\n",
      "Epoch [117/1000] Fold 9, Train Loss: 0.0152, Val Loss: 0.0118\n",
      "Epoch [118/1000] Fold 9, Train Loss: 0.0153, Val Loss: 0.0118\n",
      "Epoch [119/1000] Fold 9, Train Loss: 0.0152, Val Loss: 0.0117\n",
      "Epoch [120/1000] Fold 9, Train Loss: 0.0153, Val Loss: 0.0118\n",
      "Epoch [121/1000] Fold 9, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [122/1000] Fold 9, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [123/1000] Fold 9, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [124/1000] Fold 9, Train Loss: 0.0152, Val Loss: 0.0115\n",
      "Epoch [125/1000] Fold 9, Train Loss: 0.0152, Val Loss: 0.0116\n",
      "Epoch [126/1000] Fold 9, Train Loss: 0.0152, Val Loss: 0.0116\n",
      "Epoch [127/1000] Fold 9, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [128/1000] Fold 9, Train Loss: 0.0150, Val Loss: 0.0115\n",
      "Epoch [129/1000] Fold 9, Train Loss: 0.0149, Val Loss: 0.0115\n",
      "Epoch [130/1000] Fold 9, Train Loss: 0.0150, Val Loss: 0.0115\n",
      "Epoch [131/1000] Fold 9, Train Loss: 0.0150, Val Loss: 0.0114\n",
      "Epoch [132/1000] Fold 9, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [133/1000] Fold 9, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [134/1000] Fold 9, Train Loss: 0.0147, Val Loss: 0.0112\n",
      "Epoch [135/1000] Fold 9, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [136/1000] Fold 9, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [137/1000] Fold 9, Train Loss: 0.0147, Val Loss: 0.0112\n",
      "Epoch [138/1000] Fold 9, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [139/1000] Fold 9, Train Loss: 0.0149, Val Loss: 0.0113\n",
      "Epoch [140/1000] Fold 9, Train Loss: 0.0147, Val Loss: 0.0112\n",
      "Epoch [141/1000] Fold 9, Train Loss: 0.0144, Val Loss: 0.0111\n",
      "Epoch [142/1000] Fold 9, Train Loss: 0.0145, Val Loss: 0.0112\n",
      "Epoch [143/1000] Fold 9, Train Loss: 0.0145, Val Loss: 0.0112\n",
      "Epoch [144/1000] Fold 9, Train Loss: 0.0146, Val Loss: 0.0112\n",
      "Epoch [145/1000] Fold 9, Train Loss: 0.0144, Val Loss: 0.0113\n",
      "Epoch [146/1000] Fold 9, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [147/1000] Fold 9, Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [148/1000] Fold 9, Train Loss: 0.0145, Val Loss: 0.0111\n",
      "Epoch [149/1000] Fold 9, Train Loss: 0.0146, Val Loss: 0.0114\n",
      "Epoch [150/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0110\n",
      "Epoch [151/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [152/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [153/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [154/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [155/1000] Fold 9, Train Loss: 0.0144, Val Loss: 0.0109\n",
      "Epoch [156/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [157/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [158/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0108\n",
      "Epoch [159/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [160/1000] Fold 9, Train Loss: 0.0141, Val Loss: 0.0108\n",
      "Epoch [161/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [162/1000] Fold 9, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [163/1000] Fold 9, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [164/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0107\n",
      "Epoch [165/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [166/1000] Fold 9, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [167/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0106\n",
      "Epoch [168/1000] Fold 9, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [169/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0105\n",
      "Epoch [170/1000] Fold 9, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [171/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0106\n",
      "Epoch [172/1000] Fold 9, Train Loss: 0.0138, Val Loss: 0.0106\n",
      "Epoch [173/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0107\n",
      "Epoch [174/1000] Fold 9, Train Loss: 0.0136, Val Loss: 0.0108\n",
      "Epoch [175/1000] Fold 9, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [176/1000] Fold 9, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [177/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0106\n",
      "Epoch [178/1000] Fold 9, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [179/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [180/1000] Fold 9, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [181/1000] Fold 9, Train Loss: 0.0138, Val Loss: 0.0105\n",
      "Epoch [182/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [183/1000] Fold 9, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [184/1000] Fold 9, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [185/1000] Fold 9, Train Loss: 0.0136, Val Loss: 0.0103\n",
      "Epoch [186/1000] Fold 9, Train Loss: 0.0135, Val Loss: 0.0103\n",
      "Epoch [187/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [188/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [189/1000] Fold 9, Train Loss: 0.0136, Val Loss: 0.0104\n",
      "Epoch [190/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [191/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0102\n",
      "Epoch [192/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [193/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [194/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [195/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [196/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [197/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [198/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [199/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [200/1000] Fold 9, Train Loss: 0.0135, Val Loss: 0.0103\n",
      "Epoch [201/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [202/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0102\n",
      "Epoch [203/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0101\n",
      "Epoch [204/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0102\n",
      "Epoch [205/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [206/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [207/1000] Fold 9, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [208/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0102\n",
      "Epoch [209/1000] Fold 9, Train Loss: 0.0133, Val Loss: 0.0100\n",
      "Epoch [210/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [211/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [212/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [213/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0102\n",
      "Epoch [214/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [215/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [216/1000] Fold 9, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [217/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [218/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [219/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0103\n",
      "Epoch [220/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [221/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0102\n",
      "Epoch [222/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [223/1000] Fold 9, Train Loss: 0.0132, Val Loss: 0.0101\n",
      "Epoch [224/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0102\n",
      "Epoch [225/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0100\n",
      "Epoch [226/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [227/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [228/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0100\n",
      "Epoch [229/1000] Fold 9, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [230/1000] Fold 9, Train Loss: 0.0131, Val Loss: 0.0100\n",
      "Epoch [231/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0100\n",
      "Epoch [232/1000] Fold 9, Train Loss: 0.0130, Val Loss: 0.0099\n",
      "Epoch [233/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [234/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0100\n",
      "Epoch [235/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [236/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [237/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0099\n",
      "Epoch [238/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [239/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [240/1000] Fold 9, Train Loss: 0.0129, Val Loss: 0.0099\n",
      "Epoch [241/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [242/1000] Fold 9, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [243/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [244/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0099\n",
      "Epoch [245/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [246/1000] Fold 9, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [247/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [248/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [249/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [250/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [251/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [252/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [253/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [254/1000] Fold 9, Train Loss: 0.0127, Val Loss: 0.0097\n",
      "Epoch [255/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [256/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [257/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [258/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [259/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [260/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [261/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [262/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [263/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [264/1000] Fold 9, Train Loss: 0.0128, Val Loss: 0.0097\n",
      "Epoch [265/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [266/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [267/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [268/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [269/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [270/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0097\n",
      "Epoch [271/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0097\n",
      "Epoch [272/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [273/1000] Fold 9, Train Loss: 0.0125, Val Loss: 0.0096\n",
      "Epoch [274/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [275/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0096\n",
      "Epoch [276/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [277/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [278/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [279/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [280/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [281/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [282/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [283/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0096\n",
      "Epoch [284/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [285/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [286/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [287/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [288/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [289/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [290/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [291/1000] Fold 9, Train Loss: 0.0124, Val Loss: 0.0097\n",
      "Epoch [292/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [293/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0095\n",
      "Epoch [294/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [295/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [296/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0096\n",
      "Epoch [297/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [298/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0095\n",
      "Epoch [299/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0094\n",
      "Epoch [300/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [301/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [302/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [303/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [304/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [305/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [306/1000] Fold 9, Train Loss: 0.0126, Val Loss: 0.0094\n",
      "Epoch [307/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [308/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [309/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0094\n",
      "Epoch [310/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [311/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [312/1000] Fold 9, Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [313/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [314/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [315/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [316/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [317/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [318/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [319/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [320/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [321/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [322/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0095\n",
      "Epoch [323/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [324/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [325/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [326/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0093\n",
      "Epoch [327/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [328/1000] Fold 9, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [329/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [330/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [331/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [332/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0094\n",
      "Epoch [333/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [334/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [335/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [336/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0092\n",
      "Epoch [337/1000] Fold 9, Train Loss: 0.0120, Val Loss: 0.0094\n",
      "Epoch [338/1000] Fold 9, Train Loss: 0.0122, Val Loss: 0.0093\n",
      "Epoch [339/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [340/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [341/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [342/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [343/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [344/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0093\n",
      "Epoch [345/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [346/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0093\n",
      "Epoch [347/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [348/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [349/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [350/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [351/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0094\n",
      "Epoch [352/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [353/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [354/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [355/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [356/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [357/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [358/1000] Fold 9, Train Loss: 0.0119, Val Loss: 0.0092\n",
      "Epoch [359/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [360/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [361/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [362/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0093\n",
      "Epoch [363/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [364/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0093\n",
      "Epoch [365/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [366/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [367/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [368/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0092\n",
      "Epoch [369/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [370/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [371/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [372/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [373/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [374/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [375/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [376/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [377/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [378/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [379/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0092\n",
      "Epoch [380/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0092\n",
      "Epoch [381/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [382/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [383/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [384/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [385/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [386/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0092\n",
      "Epoch [387/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [388/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [389/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0091\n",
      "Epoch [390/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0091\n",
      "Epoch [391/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [392/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0092\n",
      "Epoch [393/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [394/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [395/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [396/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0091\n",
      "Epoch [397/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [398/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [399/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [400/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [401/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [402/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0091\n",
      "Epoch [403/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [404/1000] Fold 9, Train Loss: 0.0116, Val Loss: 0.0090\n",
      "Epoch [405/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [406/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [407/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [408/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [409/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [410/1000] Fold 9, Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [411/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [412/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [413/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [414/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [415/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [416/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0089\n",
      "Epoch [417/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [418/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [419/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [420/1000] Fold 9, Train Loss: 0.0117, Val Loss: 0.0090\n",
      "Epoch [421/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [422/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [423/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [424/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [425/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [426/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [427/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [428/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [429/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [430/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [431/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0090\n",
      "Epoch [432/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [433/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [434/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [435/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0091\n",
      "Epoch [436/1000] Fold 9, Train Loss: 0.0115, Val Loss: 0.0090\n",
      "Epoch [437/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [438/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [439/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [440/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [441/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [442/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [443/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [444/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [445/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [446/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [447/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [448/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [449/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [450/1000] Fold 9, Train Loss: 0.0114, Val Loss: 0.0090\n",
      "Epoch [451/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [452/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0090\n",
      "Epoch [453/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [454/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [455/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [456/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [457/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [458/1000] Fold 9, Train Loss: 0.0113, Val Loss: 0.0089\n",
      "Epoch [459/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [460/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [461/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [462/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0090\n",
      "Epoch [463/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [464/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0090\n",
      "Epoch [465/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [466/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [467/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [468/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0090\n",
      "Epoch [469/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [470/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [471/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [472/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [473/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [474/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [475/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [476/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [477/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0089\n",
      "Epoch [478/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [479/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [480/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0089\n",
      "Epoch [481/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [482/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0089\n",
      "Epoch [483/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [484/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [485/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [486/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [487/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [488/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [489/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [490/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [491/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [492/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0088\n",
      "Epoch [493/1000] Fold 9, Train Loss: 0.0112, Val Loss: 0.0088\n",
      "Epoch [494/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [495/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [496/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [497/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [498/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [499/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [500/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [501/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [502/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [503/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [504/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [505/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [506/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [507/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [508/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [509/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [510/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [511/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0089\n",
      "Epoch [512/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0086\n",
      "Epoch [513/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [514/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [515/1000] Fold 9, Train Loss: 0.0111, Val Loss: 0.0087\n",
      "Epoch [516/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [517/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [518/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [519/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0088\n",
      "Epoch [520/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [521/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0087\n",
      "Epoch [522/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [523/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [524/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [525/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [526/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [527/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [528/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [529/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [530/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [531/1000] Fold 9, Train Loss: 0.0110, Val Loss: 0.0088\n",
      "Epoch [532/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [533/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [534/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [535/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [536/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [537/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [538/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [539/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [540/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [541/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [542/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [543/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [544/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0088\n",
      "Epoch [545/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [546/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [547/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [548/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [549/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [550/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [551/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [552/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [553/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [554/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [555/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [556/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [557/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [558/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0087\n",
      "Epoch [559/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [560/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [561/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0085\n",
      "Epoch [562/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [563/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [564/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [565/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [566/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [567/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [568/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [569/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0088\n",
      "Epoch [570/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [571/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [572/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [573/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [574/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [575/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [576/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [577/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0086\n",
      "Epoch [578/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [579/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [580/1000] Fold 9, Train Loss: 0.0108, Val Loss: 0.0087\n",
      "Epoch [581/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [582/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [583/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [584/1000] Fold 9, Train Loss: 0.0109, Val Loss: 0.0086\n",
      "Epoch [585/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [586/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [587/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [588/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [589/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [590/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [591/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [592/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0087\n",
      "Epoch [593/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [594/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0087\n",
      "Epoch [595/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [596/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [597/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [598/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [599/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [600/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [601/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [602/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [603/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [604/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [605/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0086\n",
      "Epoch [606/1000] Fold 9, Train Loss: 0.0107, Val Loss: 0.0085\n",
      "Epoch [607/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [608/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [609/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [610/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [611/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [612/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [613/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [614/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [615/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [616/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [617/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [618/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [619/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0087\n",
      "Epoch [620/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [621/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [622/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0084\n",
      "Epoch [623/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [624/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [625/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [626/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [627/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [628/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [629/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [630/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [631/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [632/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [633/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [634/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [635/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [636/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [637/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [638/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [639/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [640/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [641/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [642/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [643/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [644/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [645/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [646/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [647/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [648/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0086\n",
      "Epoch [649/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [650/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [651/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [652/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [653/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [654/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [655/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [656/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [657/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [658/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [659/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [660/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [661/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [662/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0084\n",
      "Epoch [663/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [664/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [665/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [666/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [667/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [668/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [669/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0085\n",
      "Epoch [670/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0086\n",
      "Epoch [671/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [672/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0087\n",
      "Epoch [673/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [674/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [675/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [676/1000] Fold 9, Train Loss: 0.0106, Val Loss: 0.0086\n",
      "Epoch [677/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [678/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [679/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [680/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [681/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [682/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [683/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [684/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [685/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [686/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [687/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [688/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [689/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [690/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [691/1000] Fold 9, Train Loss: 0.0105, Val Loss: 0.0085\n",
      "Epoch [692/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [693/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [694/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [695/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [696/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [697/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [698/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [699/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [700/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0086\n",
      "Epoch [701/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [702/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [703/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [704/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [705/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [706/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [707/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [708/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0085\n",
      "Epoch [709/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [710/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [711/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [712/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [713/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [714/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [715/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [716/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [717/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [718/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [719/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0084\n",
      "Epoch [720/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [721/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [722/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [723/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [724/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [725/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [726/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [727/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [728/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0085\n",
      "Epoch [729/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [730/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [731/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [732/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [733/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [734/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [735/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [736/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [737/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [738/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [739/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [740/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [741/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [742/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [743/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [744/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [745/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [746/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [747/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [748/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [749/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [750/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [751/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [752/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [753/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [754/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [755/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [756/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0084\n",
      "Epoch [757/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [758/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [759/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [760/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [761/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [762/1000] Fold 9, Train Loss: 0.0103, Val Loss: 0.0083\n",
      "Epoch [763/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [764/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [765/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [766/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0085\n",
      "Epoch [767/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [768/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [769/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [770/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [771/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [772/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [773/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [774/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [775/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [776/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [777/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [778/1000] Fold 9, Train Loss: 0.0104, Val Loss: 0.0083\n",
      "Epoch [779/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [780/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [781/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [782/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [783/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [784/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [785/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [786/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [787/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [788/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [789/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [790/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [791/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [792/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [793/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [794/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [795/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [796/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [797/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [798/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [799/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [800/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [801/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [802/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [803/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [804/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [805/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [806/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [807/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0084\n",
      "Epoch [808/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [809/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [810/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [811/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [812/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [813/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [814/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [815/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [816/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [817/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [818/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [819/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [820/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [821/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [822/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [823/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [824/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [825/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [826/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [827/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [828/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [829/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [830/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0083\n",
      "Epoch [831/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0082\n",
      "Epoch [832/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [833/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [834/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [835/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [836/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [837/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [838/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [839/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [840/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [841/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [842/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [843/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [844/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [845/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0085\n",
      "Epoch [846/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0085\n",
      "Epoch [847/1000] Fold 9, Train Loss: 0.0102, Val Loss: 0.0083\n",
      "Epoch [848/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [849/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [850/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [851/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [852/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [853/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [854/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [855/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [856/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [857/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [858/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [859/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [860/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [861/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [862/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [863/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [864/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [865/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [866/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [867/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [868/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [869/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [870/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [871/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0082\n",
      "Epoch [872/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [873/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [874/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [875/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [876/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [877/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [878/1000] Fold 9, Train Loss: 0.0101, Val Loss: 0.0084\n",
      "Epoch [879/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [880/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [881/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [882/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [883/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [884/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [885/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [886/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [887/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [888/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [889/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [890/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [891/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [892/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [893/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [894/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [895/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [896/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [897/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [898/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [899/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [900/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [901/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [902/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [903/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [904/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [905/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [906/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [907/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [908/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [909/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [910/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0085\n",
      "Epoch [911/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0084\n",
      "Epoch [912/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [913/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [914/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0082\n",
      "Epoch [915/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [916/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [917/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [918/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [919/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [920/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [921/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0081\n",
      "Epoch [922/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [923/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [924/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [925/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [926/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [927/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [928/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [929/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [930/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [931/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [932/1000] Fold 9, Train Loss: 0.0094, Val Loss: 0.0084\n",
      "Epoch [933/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [934/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [935/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [936/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [937/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [938/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [939/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [940/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [941/1000] Fold 9, Train Loss: 0.0094, Val Loss: 0.0083\n",
      "Epoch [942/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [943/1000] Fold 9, Train Loss: 0.0100, Val Loss: 0.0083\n",
      "Epoch [944/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0084\n",
      "Epoch [945/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [946/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [947/1000] Fold 9, Train Loss: 0.0099, Val Loss: 0.0083\n",
      "Epoch [948/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0084\n",
      "Epoch [949/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [950/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0084\n",
      "Epoch [951/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [952/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [953/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [954/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [955/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0084\n",
      "Epoch [956/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [957/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0082\n",
      "Epoch [958/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [959/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0081\n",
      "Epoch [960/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [961/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [962/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [963/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [964/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0082\n",
      "Epoch [965/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [966/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [967/1000] Fold 9, Train Loss: 0.0098, Val Loss: 0.0083\n",
      "Epoch [968/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [969/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [970/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [971/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [972/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [973/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [974/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [975/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [976/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [977/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0082\n",
      "Epoch [978/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [979/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [980/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [981/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0083\n",
      "Epoch [982/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [983/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [984/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [985/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [986/1000] Fold 9, Train Loss: 0.0094, Val Loss: 0.0083\n",
      "Epoch [987/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [988/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0083\n",
      "Epoch [989/1000] Fold 9, Train Loss: 0.0095, Val Loss: 0.0081\n",
      "Epoch [990/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0083\n",
      "Epoch [991/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [992/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Epoch [993/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0084\n",
      "Epoch [994/1000] Fold 9, Train Loss: 0.0093, Val Loss: 0.0082\n",
      "Epoch [995/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0081\n",
      "Epoch [996/1000] Fold 9, Train Loss: 0.0094, Val Loss: 0.0081\n",
      "Epoch [997/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0081\n",
      "Epoch [998/1000] Fold 9, Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [999/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0080\n",
      "Epoch [1000/1000] Fold 9, Train Loss: 0.0096, Val Loss: 0.0082\n",
      "Fold 10/10\n",
      "Epoch [1/1000] Fold 10, Train Loss: 0.1464, Val Loss: 0.0710\n",
      "Epoch [2/1000] Fold 10, Train Loss: 0.0756, Val Loss: 0.0460\n",
      "Epoch [3/1000] Fold 10, Train Loss: 0.0530, Val Loss: 0.0365\n",
      "Epoch [4/1000] Fold 10, Train Loss: 0.0434, Val Loss: 0.0322\n",
      "Epoch [5/1000] Fold 10, Train Loss: 0.0382, Val Loss: 0.0297\n",
      "Epoch [6/1000] Fold 10, Train Loss: 0.0356, Val Loss: 0.0285\n",
      "Epoch [7/1000] Fold 10, Train Loss: 0.0329, Val Loss: 0.0272\n",
      "Epoch [8/1000] Fold 10, Train Loss: 0.0318, Val Loss: 0.0268\n",
      "Epoch [9/1000] Fold 10, Train Loss: 0.0309, Val Loss: 0.0258\n",
      "Epoch [10/1000] Fold 10, Train Loss: 0.0296, Val Loss: 0.0254\n",
      "Epoch [11/1000] Fold 10, Train Loss: 0.0292, Val Loss: 0.0248\n",
      "Epoch [12/1000] Fold 10, Train Loss: 0.0286, Val Loss: 0.0245\n",
      "Epoch [13/1000] Fold 10, Train Loss: 0.0277, Val Loss: 0.0240\n",
      "Epoch [14/1000] Fold 10, Train Loss: 0.0276, Val Loss: 0.0236\n",
      "Epoch [15/1000] Fold 10, Train Loss: 0.0272, Val Loss: 0.0233\n",
      "Epoch [16/1000] Fold 10, Train Loss: 0.0268, Val Loss: 0.0229\n",
      "Epoch [17/1000] Fold 10, Train Loss: 0.0262, Val Loss: 0.0225\n",
      "Epoch [18/1000] Fold 10, Train Loss: 0.0259, Val Loss: 0.0222\n",
      "Epoch [19/1000] Fold 10, Train Loss: 0.0257, Val Loss: 0.0221\n",
      "Epoch [20/1000] Fold 10, Train Loss: 0.0255, Val Loss: 0.0213\n",
      "Epoch [21/1000] Fold 10, Train Loss: 0.0252, Val Loss: 0.0210\n",
      "Epoch [22/1000] Fold 10, Train Loss: 0.0245, Val Loss: 0.0207\n",
      "Epoch [23/1000] Fold 10, Train Loss: 0.0242, Val Loss: 0.0203\n",
      "Epoch [24/1000] Fold 10, Train Loss: 0.0243, Val Loss: 0.0201\n",
      "Epoch [25/1000] Fold 10, Train Loss: 0.0237, Val Loss: 0.0197\n",
      "Epoch [26/1000] Fold 10, Train Loss: 0.0231, Val Loss: 0.0192\n",
      "Epoch [27/1000] Fold 10, Train Loss: 0.0231, Val Loss: 0.0189\n",
      "Epoch [28/1000] Fold 10, Train Loss: 0.0227, Val Loss: 0.0188\n",
      "Epoch [29/1000] Fold 10, Train Loss: 0.0227, Val Loss: 0.0184\n",
      "Epoch [30/1000] Fold 10, Train Loss: 0.0223, Val Loss: 0.0185\n",
      "Epoch [31/1000] Fold 10, Train Loss: 0.0223, Val Loss: 0.0181\n",
      "Epoch [32/1000] Fold 10, Train Loss: 0.0223, Val Loss: 0.0178\n",
      "Epoch [33/1000] Fold 10, Train Loss: 0.0215, Val Loss: 0.0176\n",
      "Epoch [34/1000] Fold 10, Train Loss: 0.0215, Val Loss: 0.0173\n",
      "Epoch [35/1000] Fold 10, Train Loss: 0.0217, Val Loss: 0.0174\n",
      "Epoch [36/1000] Fold 10, Train Loss: 0.0214, Val Loss: 0.0172\n",
      "Epoch [37/1000] Fold 10, Train Loss: 0.0210, Val Loss: 0.0169\n",
      "Epoch [38/1000] Fold 10, Train Loss: 0.0212, Val Loss: 0.0168\n",
      "Epoch [39/1000] Fold 10, Train Loss: 0.0211, Val Loss: 0.0166\n",
      "Epoch [40/1000] Fold 10, Train Loss: 0.0207, Val Loss: 0.0166\n",
      "Epoch [41/1000] Fold 10, Train Loss: 0.0206, Val Loss: 0.0164\n",
      "Epoch [42/1000] Fold 10, Train Loss: 0.0207, Val Loss: 0.0164\n",
      "Epoch [43/1000] Fold 10, Train Loss: 0.0204, Val Loss: 0.0162\n",
      "Epoch [44/1000] Fold 10, Train Loss: 0.0202, Val Loss: 0.0159\n",
      "Epoch [45/1000] Fold 10, Train Loss: 0.0196, Val Loss: 0.0157\n",
      "Epoch [46/1000] Fold 10, Train Loss: 0.0199, Val Loss: 0.0156\n",
      "Epoch [47/1000] Fold 10, Train Loss: 0.0197, Val Loss: 0.0157\n",
      "Epoch [48/1000] Fold 10, Train Loss: 0.0192, Val Loss: 0.0153\n",
      "Epoch [49/1000] Fold 10, Train Loss: 0.0192, Val Loss: 0.0153\n",
      "Epoch [50/1000] Fold 10, Train Loss: 0.0193, Val Loss: 0.0152\n",
      "Epoch [51/1000] Fold 10, Train Loss: 0.0192, Val Loss: 0.0150\n",
      "Epoch [52/1000] Fold 10, Train Loss: 0.0192, Val Loss: 0.0150\n",
      "Epoch [53/1000] Fold 10, Train Loss: 0.0191, Val Loss: 0.0148\n",
      "Epoch [54/1000] Fold 10, Train Loss: 0.0190, Val Loss: 0.0148\n",
      "Epoch [55/1000] Fold 10, Train Loss: 0.0186, Val Loss: 0.0145\n",
      "Epoch [56/1000] Fold 10, Train Loss: 0.0186, Val Loss: 0.0145\n",
      "Epoch [57/1000] Fold 10, Train Loss: 0.0185, Val Loss: 0.0145\n",
      "Epoch [58/1000] Fold 10, Train Loss: 0.0184, Val Loss: 0.0144\n",
      "Epoch [59/1000] Fold 10, Train Loss: 0.0182, Val Loss: 0.0145\n",
      "Epoch [60/1000] Fold 10, Train Loss: 0.0183, Val Loss: 0.0142\n",
      "Epoch [61/1000] Fold 10, Train Loss: 0.0184, Val Loss: 0.0142\n",
      "Epoch [62/1000] Fold 10, Train Loss: 0.0178, Val Loss: 0.0139\n",
      "Epoch [63/1000] Fold 10, Train Loss: 0.0180, Val Loss: 0.0141\n",
      "Epoch [64/1000] Fold 10, Train Loss: 0.0182, Val Loss: 0.0138\n",
      "Epoch [65/1000] Fold 10, Train Loss: 0.0177, Val Loss: 0.0138\n",
      "Epoch [66/1000] Fold 10, Train Loss: 0.0178, Val Loss: 0.0138\n",
      "Epoch [67/1000] Fold 10, Train Loss: 0.0182, Val Loss: 0.0137\n",
      "Epoch [68/1000] Fold 10, Train Loss: 0.0178, Val Loss: 0.0136\n",
      "Epoch [69/1000] Fold 10, Train Loss: 0.0177, Val Loss: 0.0135\n",
      "Epoch [70/1000] Fold 10, Train Loss: 0.0176, Val Loss: 0.0133\n",
      "Epoch [71/1000] Fold 10, Train Loss: 0.0175, Val Loss: 0.0133\n",
      "Epoch [72/1000] Fold 10, Train Loss: 0.0172, Val Loss: 0.0133\n",
      "Epoch [73/1000] Fold 10, Train Loss: 0.0174, Val Loss: 0.0133\n",
      "Epoch [74/1000] Fold 10, Train Loss: 0.0173, Val Loss: 0.0133\n",
      "Epoch [75/1000] Fold 10, Train Loss: 0.0172, Val Loss: 0.0131\n",
      "Epoch [76/1000] Fold 10, Train Loss: 0.0172, Val Loss: 0.0132\n",
      "Epoch [77/1000] Fold 10, Train Loss: 0.0170, Val Loss: 0.0132\n",
      "Epoch [78/1000] Fold 10, Train Loss: 0.0169, Val Loss: 0.0132\n",
      "Epoch [79/1000] Fold 10, Train Loss: 0.0168, Val Loss: 0.0129\n",
      "Epoch [80/1000] Fold 10, Train Loss: 0.0167, Val Loss: 0.0128\n",
      "Epoch [81/1000] Fold 10, Train Loss: 0.0166, Val Loss: 0.0130\n",
      "Epoch [82/1000] Fold 10, Train Loss: 0.0167, Val Loss: 0.0129\n",
      "Epoch [83/1000] Fold 10, Train Loss: 0.0168, Val Loss: 0.0126\n",
      "Epoch [84/1000] Fold 10, Train Loss: 0.0170, Val Loss: 0.0127\n",
      "Epoch [85/1000] Fold 10, Train Loss: 0.0166, Val Loss: 0.0128\n",
      "Epoch [86/1000] Fold 10, Train Loss: 0.0164, Val Loss: 0.0125\n",
      "Epoch [87/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0124\n",
      "Epoch [88/1000] Fold 10, Train Loss: 0.0166, Val Loss: 0.0124\n",
      "Epoch [89/1000] Fold 10, Train Loss: 0.0163, Val Loss: 0.0123\n",
      "Epoch [90/1000] Fold 10, Train Loss: 0.0162, Val Loss: 0.0123\n",
      "Epoch [91/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0124\n",
      "Epoch [92/1000] Fold 10, Train Loss: 0.0162, Val Loss: 0.0123\n",
      "Epoch [93/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0122\n",
      "Epoch [94/1000] Fold 10, Train Loss: 0.0162, Val Loss: 0.0123\n",
      "Epoch [95/1000] Fold 10, Train Loss: 0.0160, Val Loss: 0.0124\n",
      "Epoch [96/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0122\n",
      "Epoch [97/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0123\n",
      "Epoch [98/1000] Fold 10, Train Loss: 0.0162, Val Loss: 0.0122\n",
      "Epoch [99/1000] Fold 10, Train Loss: 0.0161, Val Loss: 0.0122\n",
      "Epoch [100/1000] Fold 10, Train Loss: 0.0158, Val Loss: 0.0121\n",
      "Epoch [101/1000] Fold 10, Train Loss: 0.0155, Val Loss: 0.0122\n",
      "Epoch [102/1000] Fold 10, Train Loss: 0.0160, Val Loss: 0.0123\n",
      "Epoch [103/1000] Fold 10, Train Loss: 0.0156, Val Loss: 0.0120\n",
      "Epoch [104/1000] Fold 10, Train Loss: 0.0158, Val Loss: 0.0121\n",
      "Epoch [105/1000] Fold 10, Train Loss: 0.0158, Val Loss: 0.0120\n",
      "Epoch [106/1000] Fold 10, Train Loss: 0.0158, Val Loss: 0.0121\n",
      "Epoch [107/1000] Fold 10, Train Loss: 0.0158, Val Loss: 0.0122\n",
      "Epoch [108/1000] Fold 10, Train Loss: 0.0157, Val Loss: 0.0116\n",
      "Epoch [109/1000] Fold 10, Train Loss: 0.0155, Val Loss: 0.0119\n",
      "Epoch [110/1000] Fold 10, Train Loss: 0.0155, Val Loss: 0.0119\n",
      "Epoch [111/1000] Fold 10, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [112/1000] Fold 10, Train Loss: 0.0152, Val Loss: 0.0117\n",
      "Epoch [113/1000] Fold 10, Train Loss: 0.0152, Val Loss: 0.0117\n",
      "Epoch [114/1000] Fold 10, Train Loss: 0.0150, Val Loss: 0.0118\n",
      "Epoch [115/1000] Fold 10, Train Loss: 0.0154, Val Loss: 0.0118\n",
      "Epoch [116/1000] Fold 10, Train Loss: 0.0151, Val Loss: 0.0118\n",
      "Epoch [117/1000] Fold 10, Train Loss: 0.0150, Val Loss: 0.0115\n",
      "Epoch [118/1000] Fold 10, Train Loss: 0.0152, Val Loss: 0.0116\n",
      "Epoch [119/1000] Fold 10, Train Loss: 0.0147, Val Loss: 0.0116\n",
      "Epoch [120/1000] Fold 10, Train Loss: 0.0149, Val Loss: 0.0118\n",
      "Epoch [121/1000] Fold 10, Train Loss: 0.0150, Val Loss: 0.0116\n",
      "Epoch [122/1000] Fold 10, Train Loss: 0.0151, Val Loss: 0.0115\n",
      "Epoch [123/1000] Fold 10, Train Loss: 0.0152, Val Loss: 0.0115\n",
      "Epoch [124/1000] Fold 10, Train Loss: 0.0148, Val Loss: 0.0115\n",
      "Epoch [125/1000] Fold 10, Train Loss: 0.0150, Val Loss: 0.0116\n",
      "Epoch [126/1000] Fold 10, Train Loss: 0.0149, Val Loss: 0.0115\n",
      "Epoch [127/1000] Fold 10, Train Loss: 0.0149, Val Loss: 0.0115\n",
      "Epoch [128/1000] Fold 10, Train Loss: 0.0150, Val Loss: 0.0116\n",
      "Epoch [129/1000] Fold 10, Train Loss: 0.0148, Val Loss: 0.0114\n",
      "Epoch [130/1000] Fold 10, Train Loss: 0.0146, Val Loss: 0.0115\n",
      "Epoch [131/1000] Fold 10, Train Loss: 0.0149, Val Loss: 0.0114\n",
      "Epoch [132/1000] Fold 10, Train Loss: 0.0147, Val Loss: 0.0115\n",
      "Epoch [133/1000] Fold 10, Train Loss: 0.0147, Val Loss: 0.0113\n",
      "Epoch [134/1000] Fold 10, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [135/1000] Fold 10, Train Loss: 0.0147, Val Loss: 0.0114\n",
      "Epoch [136/1000] Fold 10, Train Loss: 0.0149, Val Loss: 0.0115\n",
      "Epoch [137/1000] Fold 10, Train Loss: 0.0146, Val Loss: 0.0111\n",
      "Epoch [138/1000] Fold 10, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [139/1000] Fold 10, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [140/1000] Fold 10, Train Loss: 0.0147, Val Loss: 0.0113\n",
      "Epoch [141/1000] Fold 10, Train Loss: 0.0144, Val Loss: 0.0112\n",
      "Epoch [142/1000] Fold 10, Train Loss: 0.0145, Val Loss: 0.0111\n",
      "Epoch [143/1000] Fold 10, Train Loss: 0.0145, Val Loss: 0.0113\n",
      "Epoch [144/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [145/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0110\n",
      "Epoch [146/1000] Fold 10, Train Loss: 0.0144, Val Loss: 0.0110\n",
      "Epoch [147/1000] Fold 10, Train Loss: 0.0144, Val Loss: 0.0111\n",
      "Epoch [148/1000] Fold 10, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [149/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0111\n",
      "Epoch [150/1000] Fold 10, Train Loss: 0.0141, Val Loss: 0.0112\n",
      "Epoch [151/1000] Fold 10, Train Loss: 0.0143, Val Loss: 0.0111\n",
      "Epoch [152/1000] Fold 10, Train Loss: 0.0144, Val Loss: 0.0110\n",
      "Epoch [153/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0110\n",
      "Epoch [154/1000] Fold 10, Train Loss: 0.0140, Val Loss: 0.0109\n",
      "Epoch [155/1000] Fold 10, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [156/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [157/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0108\n",
      "Epoch [158/1000] Fold 10, Train Loss: 0.0143, Val Loss: 0.0110\n",
      "Epoch [159/1000] Fold 10, Train Loss: 0.0141, Val Loss: 0.0110\n",
      "Epoch [160/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [161/1000] Fold 10, Train Loss: 0.0142, Val Loss: 0.0109\n",
      "Epoch [162/1000] Fold 10, Train Loss: 0.0141, Val Loss: 0.0107\n",
      "Epoch [163/1000] Fold 10, Train Loss: 0.0141, Val Loss: 0.0109\n",
      "Epoch [164/1000] Fold 10, Train Loss: 0.0139, Val Loss: 0.0109\n",
      "Epoch [165/1000] Fold 10, Train Loss: 0.0139, Val Loss: 0.0108\n",
      "Epoch [166/1000] Fold 10, Train Loss: 0.0140, Val Loss: 0.0110\n",
      "Epoch [167/1000] Fold 10, Train Loss: 0.0138, Val Loss: 0.0108\n",
      "Epoch [168/1000] Fold 10, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [169/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0111\n",
      "Epoch [170/1000] Fold 10, Train Loss: 0.0138, Val Loss: 0.0107\n",
      "Epoch [171/1000] Fold 10, Train Loss: 0.0140, Val Loss: 0.0108\n",
      "Epoch [172/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0106\n",
      "Epoch [173/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [174/1000] Fold 10, Train Loss: 0.0140, Val Loss: 0.0107\n",
      "Epoch [175/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0109\n",
      "Epoch [176/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0107\n",
      "Epoch [177/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0108\n",
      "Epoch [178/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [179/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0107\n",
      "Epoch [180/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [181/1000] Fold 10, Train Loss: 0.0135, Val Loss: 0.0108\n",
      "Epoch [182/1000] Fold 10, Train Loss: 0.0137, Val Loss: 0.0106\n",
      "Epoch [183/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [184/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [185/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [186/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0107\n",
      "Epoch [187/1000] Fold 10, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [188/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [189/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0106\n",
      "Epoch [190/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0106\n",
      "Epoch [191/1000] Fold 10, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [192/1000] Fold 10, Train Loss: 0.0136, Val Loss: 0.0105\n",
      "Epoch [193/1000] Fold 10, Train Loss: 0.0135, Val Loss: 0.0105\n",
      "Epoch [194/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [195/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [196/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [197/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [198/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [199/1000] Fold 10, Train Loss: 0.0135, Val Loss: 0.0106\n",
      "Epoch [200/1000] Fold 10, Train Loss: 0.0133, Val Loss: 0.0105\n",
      "Epoch [201/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [202/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [203/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0104\n",
      "Epoch [204/1000] Fold 10, Train Loss: 0.0133, Val Loss: 0.0102\n",
      "Epoch [205/1000] Fold 10, Train Loss: 0.0133, Val Loss: 0.0103\n",
      "Epoch [206/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0103\n",
      "Epoch [207/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0105\n",
      "Epoch [208/1000] Fold 10, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [209/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [210/1000] Fold 10, Train Loss: 0.0134, Val Loss: 0.0104\n",
      "Epoch [211/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [212/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0105\n",
      "Epoch [213/1000] Fold 10, Train Loss: 0.0132, Val Loss: 0.0103\n",
      "Epoch [214/1000] Fold 10, Train Loss: 0.0133, Val Loss: 0.0104\n",
      "Epoch [215/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [216/1000] Fold 10, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [217/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [218/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0104\n",
      "Epoch [219/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [220/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [221/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0102\n",
      "Epoch [222/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0103\n",
      "Epoch [223/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [224/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [225/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [226/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [227/1000] Fold 10, Train Loss: 0.0130, Val Loss: 0.0103\n",
      "Epoch [228/1000] Fold 10, Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [229/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [230/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [231/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [232/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [233/1000] Fold 10, Train Loss: 0.0131, Val Loss: 0.0101\n",
      "Epoch [234/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [235/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [236/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0102\n",
      "Epoch [237/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [238/1000] Fold 10, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [239/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [240/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [241/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [242/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [243/1000] Fold 10, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [244/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [245/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [246/1000] Fold 10, Train Loss: 0.0128, Val Loss: 0.0102\n",
      "Epoch [247/1000] Fold 10, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [248/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [249/1000] Fold 10, Train Loss: 0.0128, Val Loss: 0.0101\n",
      "Epoch [250/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [251/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [252/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0103\n",
      "Epoch [253/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [254/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [255/1000] Fold 10, Train Loss: 0.0129, Val Loss: 0.0101\n",
      "Epoch [256/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [257/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0101\n",
      "Epoch [258/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [259/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [260/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [261/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0101\n",
      "Epoch [262/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [263/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [264/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [265/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0100\n",
      "Epoch [266/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0100\n",
      "Epoch [267/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0100\n",
      "Epoch [268/1000] Fold 10, Train Loss: 0.0127, Val Loss: 0.0099\n",
      "Epoch [269/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [270/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [271/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [272/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [273/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [274/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [275/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0099\n",
      "Epoch [276/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0101\n",
      "Epoch [277/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [278/1000] Fold 10, Train Loss: 0.0126, Val Loss: 0.0098\n",
      "Epoch [279/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0100\n",
      "Epoch [280/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0098\n",
      "Epoch [281/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [282/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [283/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [284/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [285/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [286/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [287/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0100\n",
      "Epoch [288/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0099\n",
      "Epoch [289/1000] Fold 10, Train Loss: 0.0125, Val Loss: 0.0098\n",
      "Epoch [290/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [291/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [292/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0099\n",
      "Epoch [293/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [294/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [295/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [296/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [297/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [298/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [299/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [300/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [301/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0098\n",
      "Epoch [302/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0099\n",
      "Epoch [303/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [304/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0100\n",
      "Epoch [305/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0099\n",
      "Epoch [306/1000] Fold 10, Train Loss: 0.0124, Val Loss: 0.0099\n",
      "Epoch [307/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [308/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [309/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0100\n",
      "Epoch [310/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [311/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [312/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [313/1000] Fold 10, Train Loss: 0.0123, Val Loss: 0.0097\n",
      "Epoch [314/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [315/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0097\n",
      "Epoch [316/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [317/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [318/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [319/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [320/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [321/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [322/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [323/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [324/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [325/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [326/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0098\n",
      "Epoch [327/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [328/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [329/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0097\n",
      "Epoch [330/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [331/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [332/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [333/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0099\n",
      "Epoch [334/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [335/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0098\n",
      "Epoch [336/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [337/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [338/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [339/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [340/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [341/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [342/1000] Fold 10, Train Loss: 0.0120, Val Loss: 0.0096\n",
      "Epoch [343/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0099\n",
      "Epoch [344/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [345/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [346/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0100\n",
      "Epoch [347/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [348/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0098\n",
      "Epoch [349/1000] Fold 10, Train Loss: 0.0122, Val Loss: 0.0098\n",
      "Epoch [350/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [351/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [352/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0097\n",
      "Epoch [353/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0098\n",
      "Epoch [354/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [355/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0098\n",
      "Epoch [356/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [357/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0096\n",
      "Epoch [358/1000] Fold 10, Train Loss: 0.0121, Val Loss: 0.0095\n",
      "Epoch [359/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [360/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0095\n",
      "Epoch [361/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [362/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [363/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [364/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [365/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0097\n",
      "Epoch [366/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [367/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [368/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [369/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [370/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [371/1000] Fold 10, Train Loss: 0.0119, Val Loss: 0.0096\n",
      "Epoch [372/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [373/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [374/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [375/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0097\n",
      "Epoch [376/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [377/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0095\n",
      "Epoch [378/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [379/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0095\n",
      "Epoch [380/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [381/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [382/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [383/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [384/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [385/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [386/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [387/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [388/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0096\n",
      "Epoch [389/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [390/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [391/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [392/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0096\n",
      "Epoch [393/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [394/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [395/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [396/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [397/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [398/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [399/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0097\n",
      "Epoch [400/1000] Fold 10, Train Loss: 0.0118, Val Loss: 0.0096\n",
      "Epoch [401/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [402/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [403/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [404/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [405/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0094\n",
      "Epoch [406/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0097\n",
      "Epoch [407/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [408/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [409/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [410/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0099\n",
      "Epoch [411/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [412/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [413/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0097\n",
      "Epoch [414/1000] Fold 10, Train Loss: 0.0116, Val Loss: 0.0095\n",
      "Epoch [415/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0095\n",
      "Epoch [416/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [417/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0096\n",
      "Epoch [418/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [419/1000] Fold 10, Train Loss: 0.0117, Val Loss: 0.0094\n",
      "Epoch [420/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0098\n",
      "Epoch [421/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [422/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0097\n",
      "Epoch [423/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [424/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [425/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [426/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [427/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [428/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [429/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [430/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [431/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [432/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [433/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [434/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [435/1000] Fold 10, Train Loss: 0.0115, Val Loss: 0.0094\n",
      "Epoch [436/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0096\n",
      "Epoch [437/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [438/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [439/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [440/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [441/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [442/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [443/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [444/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [445/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [446/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [447/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [448/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [449/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0095\n",
      "Epoch [450/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [451/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [452/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [453/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [454/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [455/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [456/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [457/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [458/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0094\n",
      "Epoch [459/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [460/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [461/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [462/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0095\n",
      "Epoch [463/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [464/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [465/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [466/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [467/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0094\n",
      "Epoch [468/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0096\n",
      "Epoch [469/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [470/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0096\n",
      "Epoch [471/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [472/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [473/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [474/1000] Fold 10, Train Loss: 0.0114, Val Loss: 0.0094\n",
      "Epoch [475/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [476/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [477/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [478/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [479/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [480/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0096\n",
      "Epoch [481/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [482/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [483/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [484/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [485/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [486/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [487/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0097\n",
      "Epoch [488/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [489/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [490/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [491/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [492/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [493/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [494/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [495/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [496/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [497/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [498/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [499/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [500/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [501/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0094\n",
      "Epoch [502/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0096\n",
      "Epoch [503/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [504/1000] Fold 10, Train Loss: 0.0113, Val Loss: 0.0093\n",
      "Epoch [505/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [506/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [507/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [508/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [509/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0097\n",
      "Epoch [510/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [511/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [512/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [513/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [514/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [515/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [516/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [517/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [518/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [519/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [520/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [521/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [522/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [523/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [524/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [525/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [526/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0096\n",
      "Epoch [527/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [528/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [529/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [530/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [531/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [532/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [533/1000] Fold 10, Train Loss: 0.0112, Val Loss: 0.0095\n",
      "Epoch [534/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [535/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [536/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [537/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [538/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0095\n",
      "Epoch [539/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [540/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [541/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0096\n",
      "Epoch [542/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [543/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [544/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [545/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [546/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [547/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [548/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [549/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [550/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [551/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0092\n",
      "Epoch [552/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0093\n",
      "Epoch [553/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [554/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [555/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0095\n",
      "Epoch [556/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [557/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [558/1000] Fold 10, Train Loss: 0.0110, Val Loss: 0.0094\n",
      "Epoch [559/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [560/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [561/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [562/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [563/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [564/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [565/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [566/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [567/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0094\n",
      "Epoch [568/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0092\n",
      "Epoch [569/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [570/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0096\n",
      "Epoch [571/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [572/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [573/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [574/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [575/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0096\n",
      "Epoch [576/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [577/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [578/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [579/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [580/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0095\n",
      "Epoch [581/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [582/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [583/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [584/1000] Fold 10, Train Loss: 0.0111, Val Loss: 0.0093\n",
      "Epoch [585/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [586/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0096\n",
      "Epoch [587/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [588/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [589/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [590/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [591/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [592/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0095\n",
      "Epoch [593/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [594/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0091\n",
      "Epoch [595/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0093\n",
      "Epoch [596/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0097\n",
      "Epoch [597/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [598/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [599/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [600/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [601/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [602/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [603/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [604/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [605/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0095\n",
      "Epoch [606/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [607/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0092\n",
      "Epoch [608/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [609/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [610/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0095\n",
      "Epoch [611/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [612/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [613/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [614/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [615/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [616/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [617/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [618/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [619/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0094\n",
      "Epoch [620/1000] Fold 10, Train Loss: 0.0109, Val Loss: 0.0093\n",
      "Epoch [621/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0093\n",
      "Epoch [622/1000] Fold 10, Train Loss: 0.0108, Val Loss: 0.0094\n",
      "Epoch [623/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [624/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [625/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [626/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [627/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [628/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [629/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [630/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [631/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [632/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [633/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [634/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [635/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0092\n",
      "Epoch [636/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0095\n",
      "Epoch [637/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [638/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0094\n",
      "Epoch [639/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [640/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [641/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [642/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [643/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [644/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [645/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [646/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [647/1000] Fold 10, Train Loss: 0.0107, Val Loss: 0.0092\n",
      "Epoch [648/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [649/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0091\n",
      "Epoch [650/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [651/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [652/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [653/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0093\n",
      "Epoch [654/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [655/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [656/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [657/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [658/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [659/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0095\n",
      "Epoch [660/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0092\n",
      "Epoch [661/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [662/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0095\n",
      "Epoch [663/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [664/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [665/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [666/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [667/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [668/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [669/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [670/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [671/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [672/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [673/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0095\n",
      "Epoch [674/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [675/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [676/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [677/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [678/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0095\n",
      "Epoch [679/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [680/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [681/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [682/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [683/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [684/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [685/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [686/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [687/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0090\n",
      "Epoch [688/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0091\n",
      "Epoch [689/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [690/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [691/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [692/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [693/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [694/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0092\n",
      "Epoch [695/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [696/1000] Fold 10, Train Loss: 0.0105, Val Loss: 0.0093\n",
      "Epoch [697/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [698/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [699/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [700/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [701/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [702/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [703/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [704/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [705/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [706/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [707/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [708/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0091\n",
      "Epoch [709/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [710/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0095\n",
      "Epoch [711/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0094\n",
      "Epoch [712/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [713/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [714/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [715/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [716/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [717/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [718/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [719/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [720/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [721/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [722/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [723/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [724/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [725/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [726/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [727/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [728/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [729/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [730/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [731/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [732/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [733/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0095\n",
      "Epoch [734/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0095\n",
      "Epoch [735/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0093\n",
      "Epoch [736/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [737/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [738/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [739/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [740/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0091\n",
      "Epoch [741/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [742/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [743/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0096\n",
      "Epoch [744/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [745/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [746/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [747/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0095\n",
      "Epoch [748/1000] Fold 10, Train Loss: 0.0106, Val Loss: 0.0096\n",
      "Epoch [749/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [750/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [751/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [752/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [753/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [754/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [755/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [756/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [757/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [758/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [759/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0091\n",
      "Epoch [760/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [761/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0095\n",
      "Epoch [762/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [763/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [764/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [765/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [766/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0095\n",
      "Epoch [767/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [768/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [769/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [770/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [771/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [772/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [773/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [774/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [775/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [776/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [777/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [778/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0090\n",
      "Epoch [779/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [780/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [781/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [782/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [783/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [784/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [785/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [786/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0096\n",
      "Epoch [787/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0093\n",
      "Epoch [788/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [789/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [790/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [791/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [792/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0093\n",
      "Epoch [793/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [794/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [795/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [796/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [797/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [798/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [799/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [800/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [801/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [802/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [803/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [804/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [805/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [806/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [807/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [808/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [809/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [810/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [811/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [812/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0092\n",
      "Epoch [813/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [814/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [815/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0091\n",
      "Epoch [816/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0096\n",
      "Epoch [817/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [818/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [819/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [820/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [821/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0095\n",
      "Epoch [822/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [823/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [824/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [825/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [826/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [827/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [828/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0095\n",
      "Epoch [829/1000] Fold 10, Train Loss: 0.0104, Val Loss: 0.0094\n",
      "Epoch [830/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [831/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0096\n",
      "Epoch [832/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0094\n",
      "Epoch [833/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [834/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [835/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [836/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [837/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [838/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [839/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [840/1000] Fold 10, Train Loss: 0.0102, Val Loss: 0.0094\n",
      "Epoch [841/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [842/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [843/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [844/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [845/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [846/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0095\n",
      "Epoch [847/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [848/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0093\n",
      "Epoch [849/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [850/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [851/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0090\n",
      "Epoch [852/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [853/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [854/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [855/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0096\n",
      "Epoch [856/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [857/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [858/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0095\n",
      "Epoch [859/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0092\n",
      "Epoch [860/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [861/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0097\n",
      "Epoch [862/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [863/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [864/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [865/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [866/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [867/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [868/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [869/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [870/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [871/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [872/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [873/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [874/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [875/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0095\n",
      "Epoch [876/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [877/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [878/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [879/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0095\n",
      "Epoch [880/1000] Fold 10, Train Loss: 0.0101, Val Loss: 0.0094\n",
      "Epoch [881/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0091\n",
      "Epoch [882/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [883/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [884/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [885/1000] Fold 10, Train Loss: 0.0103, Val Loss: 0.0095\n",
      "Epoch [886/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [887/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [888/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [889/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [890/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [891/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0095\n",
      "Epoch [892/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0096\n",
      "Epoch [893/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [894/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [895/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [896/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [897/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0096\n",
      "Epoch [898/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [899/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [900/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [901/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [902/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0094\n",
      "Epoch [903/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [904/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [905/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [906/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [907/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [908/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [909/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [910/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [911/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [912/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0094\n",
      "Epoch [913/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [914/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [915/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [916/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [917/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [918/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [919/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [920/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0093\n",
      "Epoch [921/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [922/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0091\n",
      "Epoch [923/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [924/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [925/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [926/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [927/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [928/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [929/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [930/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [931/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [932/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [933/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [934/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0094\n",
      "Epoch [935/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [936/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [937/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0094\n",
      "Epoch [938/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [939/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [940/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [941/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [942/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [943/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0095\n",
      "Epoch [944/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0094\n",
      "Epoch [945/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [946/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [947/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [948/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0096\n",
      "Epoch [949/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0095\n",
      "Epoch [950/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [951/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [952/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [953/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0094\n",
      "Epoch [954/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [955/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0094\n",
      "Epoch [956/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0092\n",
      "Epoch [957/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0094\n",
      "Epoch [958/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [959/1000] Fold 10, Train Loss: 0.0100, Val Loss: 0.0092\n",
      "Epoch [960/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [961/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [962/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0095\n",
      "Epoch [963/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [964/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0094\n",
      "Epoch [965/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [966/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0094\n",
      "Epoch [967/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [968/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [969/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [970/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0093\n",
      "Epoch [971/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [972/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [973/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [974/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0093\n",
      "Epoch [975/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0095\n",
      "Epoch [976/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [977/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0093\n",
      "Epoch [978/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [979/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [980/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0093\n",
      "Epoch [981/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0091\n",
      "Epoch [982/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0096\n",
      "Epoch [983/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [984/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0091\n",
      "Epoch [985/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0094\n",
      "Epoch [986/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [987/1000] Fold 10, Train Loss: 0.0098, Val Loss: 0.0092\n",
      "Epoch [988/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [989/1000] Fold 10, Train Loss: 0.0099, Val Loss: 0.0092\n",
      "Epoch [990/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0094\n",
      "Epoch [991/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0096\n",
      "Epoch [992/1000] Fold 10, Train Loss: 0.0096, Val Loss: 0.0094\n",
      "Epoch [993/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0095\n",
      "Epoch [994/1000] Fold 10, Train Loss: 0.0094, Val Loss: 0.0093\n",
      "Epoch [995/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0091\n",
      "Epoch [996/1000] Fold 10, Train Loss: 0.0094, Val Loss: 0.0091\n",
      "Epoch [997/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0093\n",
      "Epoch [998/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0090\n",
      "Epoch [999/1000] Fold 10, Train Loss: 0.0097, Val Loss: 0.0092\n",
      "Epoch [1000/1000] Fold 10, Train Loss: 0.0095, Val Loss: 0.0096\n"
     ]
    }
   ],
   "source": [
    "# MIN MAX\n",
    "dataset = TensorDataset(train_day7_feats_minmaxed, train_day10_feats_minmaxed)\n",
    "\n",
    "cv_histories, best_models = cross_validate_with_early_stopping(\n",
    "    model_class=FeaturePredictor,\n",
    "    dataset=dataset,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer_class=torch.optim.Adam,\n",
    "    num_epochs=1000,\n",
    "    patience=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Validation Loss: 0.0086\n"
     ]
    }
   ],
   "source": [
    "# Example: Average validation loss across folds\n",
    "avg_val_loss = np.mean([history['val_loss'][-1] for history in cv_histories])\n",
    "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "  Best Validation Loss = 0.0086\n",
      "  Corresponding Training Loss = 0.0098\n",
      "  Best Epoch = 935\n",
      "Fold 2:\n",
      "  Best Validation Loss = 0.0083\n",
      "  Corresponding Training Loss = 0.0097\n",
      "  Best Epoch = 997\n",
      "Fold 3:\n",
      "  Best Validation Loss = 0.0083\n",
      "  Corresponding Training Loss = 0.0098\n",
      "  Best Epoch = 1000\n",
      "Fold 4:\n",
      "  Best Validation Loss = 0.0080\n",
      "  Corresponding Training Loss = 0.0099\n",
      "  Best Epoch = 919\n",
      "Fold 5:\n",
      "  Best Validation Loss = 0.0085\n",
      "  Corresponding Training Loss = 0.0098\n",
      "  Best Epoch = 925\n",
      "Fold 6:\n",
      "  Best Validation Loss = 0.0084\n",
      "  Corresponding Training Loss = 0.0097\n",
      "  Best Epoch = 943\n",
      "Fold 7:\n",
      "  Best Validation Loss = 0.0088\n",
      "  Corresponding Training Loss = 0.0099\n",
      "  Best Epoch = 992\n",
      "Fold 8:\n",
      "  Best Validation Loss = 0.0082\n",
      "  Corresponding Training Loss = 0.0099\n",
      "  Best Epoch = 935\n",
      "Fold 9:\n",
      "  Best Validation Loss = 0.0080\n",
      "  Corresponding Training Loss = 0.0096\n",
      "  Best Epoch = 999\n",
      "Fold 10:\n",
      "  Best Validation Loss = 0.0090\n",
      "  Corresponding Training Loss = 0.0104\n",
      "  Best Epoch = 687\n"
     ]
    }
   ],
   "source": [
    "# Analyze the best results for each fold outside the training function\n",
    "for fold_idx, history in enumerate(cv_histories):\n",
    "    # Find the epoch with the lowest validation loss\n",
    "    best_epoch = int(np.argmin(history['val_loss'])) + 1  # Adding 1 because epochs are 1-indexed\n",
    "    best_val_loss = history['val_loss'][best_epoch - 1]  # Accessing the loss using 0-based index\n",
    "    best_train_loss = history['train_loss'][best_epoch - 1]\n",
    "\n",
    "    print(f\"Fold {fold_idx + 1}:\")\n",
    "    print(f\"  Best Validation Loss = {best_val_loss:.4f}\")\n",
    "    print(f\"  Corresponding Training Loss = {best_train_loss:.4f}\")\n",
    "    print(f\"  Best Epoch = {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves_separately(cv_histories):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss curves for each fold in separate figures.\n",
    "    \n",
    "    Args:\n",
    "        cv_histories (list): List of loss histories for each fold.\n",
    "    \"\"\"\n",
    "    # Plot Training Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for fold, history in enumerate(cv_histories):\n",
    "        epochs = range(1, len(history['train_loss']) + 1)  # Dynamically set range\n",
    "        plt.plot(epochs, history['train_loss'], label=f'Fold {fold + 1} Train Loss')\n",
    "    plt.title('Training Loss per Fold')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Validation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for fold, history in enumerate(cv_histories):\n",
    "        epochs = range(1, len(history['val_loss']) + 1)  # Dynamically set range\n",
    "        plt.plot(epochs, history['val_loss'], label=f'Fold {fold + 1} Val Loss', linestyle='--')\n",
    "    plt.title('Validation Loss per Fold')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plot_loss_curves_separately(cv_histories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model from fold 9 with the lowest validation loss (0.0080) saved to conall_9.pth.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Track the best validation loss across all folds\n",
    "lowest_val_loss = float('inf')\n",
    "best_model_idx = -1  # Index of the best model\n",
    "best_model_state = None\n",
    "\n",
    "# Find the fold with the lowest validation loss\n",
    "for fold_idx, history in enumerate(cv_histories):\n",
    "    # Get the best validation loss for this fold\n",
    "    best_val_loss = min(history['val_loss'])\n",
    "    \n",
    "    # Check if this is the lowest validation loss across all folds\n",
    "    if best_val_loss < lowest_val_loss:\n",
    "        lowest_val_loss = best_val_loss\n",
    "        best_model_idx = fold_idx\n",
    "        best_model_state = best_models[fold_idx]\n",
    "\n",
    "# Save the model with the lowest validation loss\n",
    "if best_model_state is not None:\n",
    "    save_path = f'conall_{best_model_idx + 1}.pth'\n",
    "    torch.save(best_model_state, save_path)\n",
    "    print(f\"Model from fold {best_model_idx + 1} with the lowest validation loss ({lowest_val_loss:.4f}) saved to {save_path}.\")\n",
    "else:\n",
    "    print(\"No model to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
