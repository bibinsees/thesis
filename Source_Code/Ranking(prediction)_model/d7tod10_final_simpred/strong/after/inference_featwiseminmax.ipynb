{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training features\n",
    "cond7 = torch.load('combined_cond7.pt')\n",
    "cond10 = torch.load('combined_cond10.pt')\n",
    "\n",
    "# Load training features\n",
    "ex7 = torch.load('combined_ex7.pt')\n",
    "ex10 = torch.load('combined_ex10.pt')\n",
    "# Load training features\n",
    "sd7 = torch.load('combined_sd7.pt')\n",
    "sd10 = torch.load('combined_sd10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cond7.shape)\n",
    "print(cond10.shape)\n",
    "\n",
    "print(sd7.shape)\n",
    "print(sd10.shape)\n",
    "\n",
    "print(ex7.shape)\n",
    "print(ex10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('ex7.pkl')\n",
    "scaler_day10 = joblib.load('ex10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(sd7.shape)\n",
    "print(sd10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('sd7.pkl')\n",
    "scaler_day10 = joblib.load('sd10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('cond7.pkl')\n",
    "scaler_day10 = joblib.load('cond10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_tensor(scaler, tensor):\n",
    "    \"\"\"\n",
    "    Scales a tensor using the provided MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        scaler (MinMaxScaler): The MinMaxScaler to use for scaling.\n",
    "        tensor (torch.Tensor): The tensor to scale.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The scaled tensor.\n",
    "    \"\"\"\n",
    "    tensor_numpy = tensor.numpy()  # Convert tensor to numpy\n",
    "    tensor_scaled_numpy = scaler.transform(tensor_numpy)  # Apply Min-Max scaling\n",
    "    tensor_scaled = torch.tensor(tensor_scaled_numpy)  # Convert back to torch tensor\n",
    "    return tensor_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for different datasets\n",
    "\n",
    "\n",
    "cond7_minmax = scale_tensor(scaler_day7, cond7)\n",
    "cond10_minmax = scale_tensor(scaler_day10, cond10)\n",
    "\n",
    "ex7_minmax = scale_tensor(scaler_day7, ex7)\n",
    "ex10_minmax = scale_tensor(scaler_day10, ex10)\n",
    "\n",
    "sd7_minmax = scale_tensor(scaler_day7, sd7)\n",
    "sd10_minmax = scale_tensor(scaler_day10, sd10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=20, output_size=20):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.BatchNorm1d(4),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z\n",
    "\n",
    "# Example usage\n",
    "model = FeaturePredictor()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    # Corrected indentation for forward method\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super(FeaturePredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where models are saved\n",
    "#load_dir = r'C:\\Users\\k54739\\saved_model\\ranking\\simclr\\minmax_scaled'\n",
    "\n",
    "# Load a specific fold's best model\n",
    "model = FeaturePredictor()\n",
    "load_path = os.path.join('bestmodel_sd_1.pth')\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all metrics added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = sd7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = sd10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"sdnorm_sd.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = cond7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = cond10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"sdnorm_cond.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = ex7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = ex10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"sdnorm_ex.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"explod\": pd.read_csv(\"sdnorm_ex.csv\"),\n",
    "    \"sd\": pd.read_csv(\"sdnorm_sd.csv\"),\n",
    "    \"control\": pd.read_csv(\"sdnorm_cond.csv\")\n",
    "}\n",
    "\n",
    "# Prepare all metrics\n",
    "metrics = [\n",
    "    \"MSE\",\n",
    "    \"Euclidean Distance\",\n",
    "    \"Cosine Distance\",\n",
    "    \"L1 Distance\",\n",
    "    \"Pearson Correlation\",\n",
    "    \"Dot Product\",\n",
    "    \"Jaccard Similarity\",\n",
    "    \"Hamming Distance\",\n",
    "    \"Mahalanobis Distance\"\n",
    "]\n",
    "\n",
    "# Accuracy calculation function\n",
    "def calculate_accuracy(datasets, metric_name):\n",
    "    # Step 1: Calculate mean inference metric for each class\n",
    "    class_means = {}\n",
    "    for class_name, df in datasets.items():\n",
    "        if metric_name in df.columns:\n",
    "            class_means[class_name] = df[metric_name].mean()\n",
    "\n",
    "    # Sort class means to find the middle class\n",
    "    sorted_means = sorted(class_means.items(), key=lambda x: x[1])\n",
    "    middle_class = sorted_means[1][0]  # The middle class is the second one in sorted order\n",
    "    middle_class_mean = sorted_means[1][1]\n",
    "\n",
    "    # Step 2: Find the min and max for the middle class\n",
    "    middle_class_min = datasets[middle_class][metric_name].min()\n",
    "    middle_class_max = datasets[middle_class][metric_name].max()\n",
    "\n",
    "    # Step 3: Calculate errors\n",
    "    error_1 = sum(datasets[sorted_means[0][0]][metric_name] > middle_class_min)\n",
    "    error_3 = sum(datasets[sorted_means[2][0]][metric_name] < middle_class_max)\n",
    "    total_errors = error_1 + error_3\n",
    "\n",
    "    # Step 4: Calculate accuracy\n",
    "    total_points = sum(len(df) for df in datasets.values())\n",
    "    accuracy = (total_points - total_errors) / total_points\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Metric: {metric_name}\")\n",
    "    print(f\"  Class Means: {class_means}\")\n",
    "    print(f\"  Middle Class: {middle_class} (Mean: {middle_class_mean})\")\n",
    "    print(f\"  Middle Class Min: {middle_class_min}, Max: {middle_class_max}\")\n",
    "    print(f\"  Errors (C1 > min): {error_1}, Errors (C3 < max): {error_3}\")\n",
    "    print(f\"  Total Errors: {total_errors}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Calculate and print accuracy for each metric\n",
    "results = {}\n",
    "for metric in metrics:\n",
    "    results[metric] = calculate_accuracy(datasets, metric)\n",
    "\n",
    "# Summary of accuracies\n",
    "print(\"\\nSummary of Accuracies:\")\n",
    "for metric, acc in results.items():\n",
    "    print(f\"{metric}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"explod\": pd.read_csv(\"sdnorm_ex.csv\"),\n",
    "    \"sd\": pd.read_csv(\"sdnorm_sd.csv\"),\n",
    "    \"control\": pd.read_csv(\"sdnorm_cond.csv\")\n",
    "}\n",
    "\n",
    "# Prepare all metrics\n",
    "metrics = [\n",
    "    \"MSE\",\n",
    "    \"Euclidean Distance\",\n",
    "    \"Cosine Distance\",\n",
    "    \"L1 Distance\",\n",
    "    \"Pearson Correlation\",\n",
    "    \"Dot Product\",\n",
    "    \"Jaccard Similarity\",\n",
    "    \"Hamming Distance\",\n",
    "    \"Mahalanobis Distance\"\n",
    "]\n",
    "\n",
    "# Define colors and hatching patterns for each dataset\n",
    "colors = {\"explod\": \"blue\", \"sd\": \"orange\", \"control\": \"green\"}\n",
    "hatch_styles = {\"explod\": \"o\", \"sd\": \"\\\\\", \"control\": \"*\"}\n",
    "\n",
    "# Plotting function\n",
    "def plot_metric_distribution(metric_name, bins=20):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for dataset_name, df in datasets.items():\n",
    "        if metric_name in df.columns:\n",
    "            values = df[metric_name].tolist()\n",
    "            plt.hist(\n",
    "                values,\n",
    "                bins=bins,\n",
    "                alpha=0.6,\n",
    "                label=f\"{dataset_name.capitalize()} ({metric_name})\",\n",
    "                color=colors[dataset_name],\n",
    "                hatch=hatch_styles[dataset_name],\n",
    "                edgecolor=\"black\",\n",
    "                density=True  # Normalize to density\n",
    "            )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f\"{metric_name} Distribution for Different Datasets (Density)\", fontsize=14)\n",
    "    plt.xlabel(metric_name, fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for all metrics\n",
    "for metric in metrics:\n",
    "    plot_metric_distribution(metric, bins=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
