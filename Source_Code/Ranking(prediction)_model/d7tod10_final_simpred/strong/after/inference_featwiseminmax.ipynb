{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cond7 = torch.load('combined_cond7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cond10 = torch.load('combined_cond10.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ex7 = torch.load('combined_ex7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ex10 = torch.load('combined_ex10.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd7 = torch.load('combined_sd7.pt')\n",
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\1147718468.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sd10 = torch.load('combined_sd10.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load training features\n",
    "cond7 = torch.load('combined_cond7.pt')\n",
    "cond10 = torch.load('combined_cond10.pt')\n",
    "\n",
    "# Load training features\n",
    "ex7 = torch.load('combined_ex7.pt')\n",
    "ex10 = torch.load('combined_ex10.pt')\n",
    "# Load training features\n",
    "sd7 = torch.load('combined_sd7.pt')\n",
    "sd10 = torch.load('combined_sd10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([650, 20])\n",
      "torch.Size([650, 20])\n",
      "torch.Size([638, 20])\n",
      "torch.Size([638, 20])\n",
      "torch.Size([646, 20])\n",
      "torch.Size([646, 20])\n"
     ]
    }
   ],
   "source": [
    "print(cond7.shape)\n",
    "print(cond10.shape)\n",
    "\n",
    "print(sd7.shape)\n",
    "print(sd10.shape)\n",
    "\n",
    "print(ex7.shape)\n",
    "print(ex10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers loaded!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('ex7.pkl')\n",
    "scaler_day10 = joblib.load('ex10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(sd7.shape)\n",
    "print(sd10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('sd7.pkl')\n",
    "scaler_day10 = joblib.load('sd10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Load the scalers from disk\n",
    "scaler_day7 = joblib.load('cond7.pkl')\n",
    "scaler_day10 = joblib.load('cond10.pkl')\n",
    "\n",
    "print(\"Scalers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_tensor(scaler, tensor):\n",
    "    \"\"\"\n",
    "    Scales a tensor using the provided MinMaxScaler.\n",
    "\n",
    "    Args:\n",
    "        scaler (MinMaxScaler): The MinMaxScaler to use for scaling.\n",
    "        tensor (torch.Tensor): The tensor to scale.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The scaled tensor.\n",
    "    \"\"\"\n",
    "    tensor_numpy = tensor.numpy()  # Convert tensor to numpy\n",
    "    tensor_scaled_numpy = scaler.transform(tensor_numpy)  # Apply Min-Max scaling\n",
    "    tensor_scaled = torch.tensor(tensor_scaled_numpy)  # Convert back to torch tensor\n",
    "    return tensor_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for different datasets\n",
    "\n",
    "\n",
    "cond7_minmax = scale_tensor(scaler_day7, cond7)\n",
    "cond10_minmax = scale_tensor(scaler_day10, cond10)\n",
    "\n",
    "ex7_minmax = scale_tensor(scaler_day7, ex7)\n",
    "ex10_minmax = scale_tensor(scaler_day10, ex10)\n",
    "\n",
    "sd7_minmax = scale_tensor(scaler_day7, sd7)\n",
    "sd10_minmax = scale_tensor(scaler_day10, sd10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturePredictor(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (5): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (9): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=8, out_features=16, bias=True)\n",
      "    (4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=16, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=20, output_size=20):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.BatchNorm1d(4),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4, 8),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z\n",
    "\n",
    "# Example usage\n",
    "model = FeaturePredictor()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    # Corrected indentation for forward method\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.decoder(x)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class FeaturePredictor(nn.Module):\n",
    "    def __init__(self, input_size=512, output_size=512):\n",
    "        super(FeaturePredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from bestmodel_ex_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_10344\\3015219247.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(load_path))\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where models are saved\n",
    "#load_dir = r'C:\\Users\\k54739\\saved_model\\ranking\\simclr\\minmax_scaled'\n",
    "\n",
    "# Load a specific fold's best model\n",
    "model = FeaturePredictor()\n",
    "load_path = os.path.join('bestmodel_ex_1.pth')\n",
    "model.load_state_dict(torch.load(load_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all metrics added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example metrics for the first feature vector:\n",
      "MSE: 0.1733\n",
      "Euclidean Distance: 1.8616\n",
      "Cosine Distance: 0.2089\n",
      "L1 Distance: 6.2473\n",
      "Pearson Correlation: 0.1304\n",
      "Dot Product: 5.2833\n",
      "Jaccard Similarity: 0.3333\n",
      "Hamming Distance: 8.0000\n",
      "Mahalanobis Distance: 41.2720\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = sd7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = sd10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics saved to exnorm_sd.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"exnorm_sd.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example metrics for the first feature vector:\n",
      "MSE: 0.1191\n",
      "Euclidean Distance: 1.5433\n",
      "Cosine Distance: 0.1659\n",
      "L1 Distance: 5.8528\n",
      "Pearson Correlation: 0.0277\n",
      "Dot Product: 5.2407\n",
      "Jaccard Similarity: 0.3333\n",
      "Hamming Distance: 10.0000\n",
      "Mahalanobis Distance: 7.2681\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = cond7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = cond10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics saved to exnorm_cond.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"exnorm_cond.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example metrics for the first feature vector:\n",
      "MSE: 0.0370\n",
      "Euclidean Distance: 0.8607\n",
      "Cosine Distance: 0.0676\n",
      "L1 Distance: 3.0249\n",
      "Pearson Correlation: 0.5599\n",
      "Dot Product: 5.0732\n",
      "Jaccard Similarity: 0.5000\n",
      "Hamming Distance: 7.0000\n",
      "Mahalanobis Distance: 6.0776\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume exploded40_day7_minmaxed and exploded40_day10_minmaxed are already loaded\n",
    "# Example shapes: [N, 512], where N is the number of feature vectors\n",
    "\n",
    "# Ensure the tensors are on the same device as the model\n",
    "train_day7_feats_minmaxed = ex7_minmax.to(device)\n",
    "train_day10_feats_minmaxed = ex10_minmax.to(device)\n",
    "\n",
    "# Initialize the MSE loss function\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "# Lists to store metrics for each feature vector\n",
    "mse_values = []\n",
    "euclidean_distances = []\n",
    "cosine_distances = []\n",
    "l1_distances = []\n",
    "pearson_correlations = []\n",
    "dot_products = []\n",
    "jaccard_similarities = []\n",
    "hamming_distances = []\n",
    "mahalanobis_distances = []\n",
    "\n",
    "# Compute covariance matrix for Mahalanobis distance (for day10 features)\n",
    "epsilon = 1e-6  # Stabilization constant for invertibility\n",
    "mean_vector = torch.mean(train_day10_feats_minmaxed, dim=0)  # Mean vector\n",
    "covariance_matrix = torch.cov(train_day10_feats_minmaxed.T)  # Covariance matrix\n",
    "covariance_matrix += torch.eye(covariance_matrix.size(0), device=device) * epsilon  # Stabilize\n",
    "covariance_matrix_inv = torch.inverse(covariance_matrix)  # Inverse covariance matrix\n",
    "\n",
    "# Loop through each feature vector\n",
    "# Loop through each feature vector\n",
    "for i in range(train_day7_feats_minmaxed.shape[0]):\n",
    "    train_day7_feats = train_day7_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    train_day10_feats = train_day10_feats_minmaxed[i].unsqueeze(0)  # Shape: [1, 512]\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predicted_day10 = model(train_day7_feats)\n",
    "    \n",
    "    # Compute MSE for this feature vector\n",
    "    mse = mse_loss_fn(predicted_day10, train_day10_feats).item()\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = torch.norm(predicted_day10 - train_day10_feats, p=2).item()\n",
    "    euclidean_distances.append(euclidean_distance)\n",
    "    \n",
    "    # Compute Cosine distance\n",
    "    cosine_distance = 1 - F.cosine_similarity(predicted_day10, train_day10_feats, dim=1).item()\n",
    "    cosine_distances.append(cosine_distance)\n",
    "    \n",
    "    # Compute L1 distance\n",
    "    l1_distance = torch.norm(predicted_day10 - train_day10_feats, p=1).item()\n",
    "    l1_distances.append(l1_distance)\n",
    "    \n",
    "    # Compute Pearson Correlation\n",
    "    combined = torch.stack((predicted_day10.flatten(), train_day10_feats.flatten()), dim=0)  # Shape: [2, N]\n",
    "    pearson_corr_matrix = torch.corrcoef(combined)  # Correlation matrix of shape [2, 2]\n",
    "    pearson_corr = pearson_corr_matrix[0, 1]  # Extract correlation between the two variables\n",
    "    pearson_correlations.append(pearson_corr.item())\n",
    "    \n",
    "    # Compute Inner Product (Dot Product)\n",
    "    dot_product = torch.dot(predicted_day10.flatten(), train_day10_feats.flatten()).item()\n",
    "    dot_products.append(dot_product)\n",
    "    \n",
    "    # Compute Jaccard Similarity (thresholding features to binary)\n",
    "    bin_pred = (predicted_day10.flatten() > 0.5).int()\n",
    "    bin_true = (train_day10_feats.flatten() > 0.5).int()\n",
    "    intersection = (bin_pred & bin_true).sum().item()\n",
    "    union = (bin_pred | bin_true).sum().item()\n",
    "    jaccard_similarity = intersection / union if union > 0 else 0\n",
    "    jaccard_similarities.append(jaccard_similarity)\n",
    "    \n",
    "    # Compute Hamming Distance (binary representation)\n",
    "    hamming_distance = (bin_pred != bin_true).sum().item()\n",
    "    hamming_distances.append(hamming_distance)\n",
    "    \n",
    "    # Compute Mahalanobis Distance\n",
    "    delta = (predicted_day10.flatten() - mean_vector)  # Difference from mean\n",
    "    mahalanobis_distance = torch.sqrt(torch.dot(delta, torch.matmul(covariance_matrix_inv, delta))).item()\n",
    "    mahalanobis_distances.append(mahalanobis_distance)\n",
    "\n",
    "# Print some example metrics for the first feature vector\n",
    "print(f\"Example metrics for the first feature vector:\")\n",
    "print(f\"MSE: {mse_values[0]:.4f}\")\n",
    "print(f\"Euclidean Distance: {euclidean_distances[0]:.4f}\")\n",
    "print(f\"Cosine Distance: {cosine_distances[0]:.4f}\")\n",
    "print(f\"L1 Distance: {l1_distances[0]:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_correlations[0]:.4f}\")\n",
    "print(f\"Dot Product: {dot_products[0]:.4f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarities[0]:.4f}\")\n",
    "print(f\"Hamming Distance: {hamming_distances[0]:.4f}\")\n",
    "print(f\"Mahalanobis Distance: {mahalanobis_distances[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics saved to exnorm_ex.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with all the metrics\n",
    "data = {\n",
    "    \"MSE\": mse_values,  # List of MSE values\n",
    "    \"Euclidean Distance\": euclidean_distances,  # List of Euclidean distances\n",
    "    \"Cosine Distance\": cosine_distances,  # List of Cosine distances\n",
    "    \"L1 Distance\": l1_distances,  # List of L1 distances\n",
    "    \"Pearson Correlation\": pearson_correlations,  # List of Pearson correlation values\n",
    "    \"Dot Product\": dot_products,  # List of dot product values\n",
    "    \"Jaccard Similarity\": jaccard_similarities,  # List of Jaccard similarities\n",
    "    \"Hamming Distance\": hamming_distances,  # List of Hamming distances\n",
    "    \"Mahalanobis Distance\": mahalanobis_distances  # List of Mahalanobis distances\n",
    "}\n",
    "\n",
    "# Convert the data dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the desired name for the CSV file\n",
    "csv_file_name = \"exnorm_ex.csv\"  # Replace this with your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(csv_file_name, index=False)  # Save without an index column\n",
    "print(f\"All metrics saved to {csv_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: MSE\n",
      "  Class Means: {'explod': 0.02812437968034489, 'sd': 0.1448124129336829, 'control': 0.10463834984944413}\n",
      "  Middle Class: control (Mean: 0.10463834984944413)\n",
      "  Middle Class Min: 0.0256066862493753, Max: 0.2052597105503082\n",
      "  Errors (C1 > min): 304, Errors (C3 < max): 634\n",
      "  Total Errors: 938\n",
      "  Accuracy: 0.5150\n",
      "\n",
      "Metric: Euclidean Distance\n",
      "  Class Means: {'explod': 0.717570737258015, 'sd': 1.6954156717162894, 'control': 1.4316329297652612}\n",
      "  Middle Class: control (Mean: 1.4316329297652612)\n",
      "  Middle Class Min: 0.7156351804733276, Max: 2.026127815246582\n",
      "  Errors (C1 > min): 304, Errors (C3 < max): 634\n",
      "  Total Errors: 938\n",
      "  Accuracy: 0.5150\n",
      "\n",
      "Metric: Cosine Distance\n",
      "  Class Means: {'explod': 0.049773130808083224, 'sd': 0.18114589729279187, 'control': 0.14496552577385532}\n",
      "  Middle Class: control (Mean: 0.14496552577385532)\n",
      "  Middle Class Min: 0.0405097007751464, Max: 0.3246803283691406\n",
      "  Errors (C1 > min): 360, Errors (C3 < max): 638\n",
      "  Total Errors: 998\n",
      "  Accuracy: 0.4840\n",
      "\n",
      "Metric: L1 Distance\n",
      "  Class Means: {'explod': 2.636775102500945, 'sd': 6.1419627367515925, 'control': 5.189857128583468}\n",
      "  Middle Class: control (Mean: 5.189857128583468)\n",
      "  Middle Class Min: 2.46954607963562, Max: 7.922432899475098\n",
      "  Errors (C1 > min): 354, Errors (C3 < max): 638\n",
      "  Total Errors: 992\n",
      "  Accuracy: 0.4871\n",
      "\n",
      "Metric: Pearson Correlation\n",
      "  Class Means: {'explod': 0.600740153851397, 'sd': 0.09030966409529045, 'control': 0.1863720824659014}\n",
      "  Middle Class: control (Mean: 0.1863720824659014)\n",
      "  Middle Class Min: -0.3000653088092804, Max: 0.7551953792572021\n",
      "  Errors (C1 > min): 638, Errors (C3 < max): 425\n",
      "  Total Errors: 1063\n",
      "  Accuracy: 0.4504\n",
      "\n",
      "Metric: Dot Product\n",
      "  Class Means: {'explod': 4.9845014408276915, 'sd': 5.303663581889999, 'control': 5.212151741247911}\n",
      "  Middle Class: control (Mean: 5.212151741247911)\n",
      "  Middle Class Min: 3.5471527576446533, Max: 5.972013473510742\n",
      "  Errors (C1 > min): 646, Errors (C3 < max): 627\n",
      "  Total Errors: 1273\n",
      "  Accuracy: 0.3418\n",
      "\n",
      "Metric: Jaccard Similarity\n",
      "  Class Means: {'explod': 0.5463487118923287, 'sd': 0.35412937017906587, 'control': 0.3953309187504829}\n",
      "  Middle Class: control (Mean: 0.3953309187504829)\n",
      "  Middle Class Min: 0.0666666666666666, Max: 0.7142857142857143\n",
      "  Errors (C1 > min): 638, Errors (C3 < max): 539\n",
      "  Total Errors: 1177\n",
      "  Accuracy: 0.3914\n",
      "\n",
      "Metric: Hamming Distance\n",
      "  Class Means: {'explod': 5.7972136222910216, 'sd': 9.788401253918495, 'control': 8.906153846153845}\n",
      "  Middle Class: control (Mean: 8.906153846153845)\n",
      "  Middle Class Min: 4, Max: 14\n",
      "  Errors (C1 > min): 435, Errors (C3 < max): 624\n",
      "  Total Errors: 1059\n",
      "  Accuracy: 0.4524\n",
      "\n",
      "Metric: Mahalanobis Distance\n",
      "  Class Means: {'explod': 4.638468708047188, 'sd': 40.418863003530475, 'control': 9.028214044570923}\n",
      "  Middle Class: control (Mean: 9.028214044570923)\n",
      "  Middle Class Min: 6.849713325500488, Max: 15.087985038757324\n",
      "  Errors (C1 > min): 104, Errors (C3 < max): 0\n",
      "  Total Errors: 104\n",
      "  Accuracy: 0.9462\n",
      "\n",
      "\n",
      "Summary of Accuracies:\n",
      "MSE: 0.5150\n",
      "Euclidean Distance: 0.5150\n",
      "Cosine Distance: 0.4840\n",
      "L1 Distance: 0.4871\n",
      "Pearson Correlation: 0.4504\n",
      "Dot Product: 0.3418\n",
      "Jaccard Similarity: 0.3914\n",
      "Hamming Distance: 0.4524\n",
      "Mahalanobis Distance: 0.9462\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"explod\": pd.read_csv(\"exnorm_ex.csv\"),\n",
    "    \"sd\": pd.read_csv(\"exnorm_sd.csv\"),\n",
    "    \"control\": pd.read_csv(\"exnorm_cond.csv\")\n",
    "}\n",
    "\n",
    "# Prepare all metrics\n",
    "metrics = [\n",
    "    \"MSE\",\n",
    "    \"Euclidean Distance\",\n",
    "    \"Cosine Distance\",\n",
    "    \"L1 Distance\",\n",
    "    \"Pearson Correlation\",\n",
    "    \"Dot Product\",\n",
    "    \"Jaccard Similarity\",\n",
    "    \"Hamming Distance\",\n",
    "    \"Mahalanobis Distance\"\n",
    "]\n",
    "\n",
    "# Accuracy calculation function\n",
    "def calculate_accuracy(datasets, metric_name):\n",
    "    # Step 1: Calculate mean inference metric for each class\n",
    "    class_means = {}\n",
    "    for class_name, df in datasets.items():\n",
    "        if metric_name in df.columns:\n",
    "            class_means[class_name] = df[metric_name].mean()\n",
    "\n",
    "    # Sort class means to find the middle class\n",
    "    sorted_means = sorted(class_means.items(), key=lambda x: x[1])\n",
    "    middle_class = sorted_means[1][0]  # The middle class is the second one in sorted order\n",
    "    middle_class_mean = sorted_means[1][1]\n",
    "\n",
    "    # Step 2: Find the min and max for the middle class\n",
    "    middle_class_min = datasets[middle_class][metric_name].min()\n",
    "    middle_class_max = datasets[middle_class][metric_name].max()\n",
    "\n",
    "    # Step 3: Calculate errors\n",
    "    error_1 = sum(datasets[sorted_means[0][0]][metric_name] > middle_class_min)\n",
    "    error_3 = sum(datasets[sorted_means[2][0]][metric_name] < middle_class_max)\n",
    "    total_errors = error_1 + error_3\n",
    "\n",
    "    # Step 4: Calculate accuracy\n",
    "    total_points = sum(len(df) for df in datasets.values())\n",
    "    accuracy = (total_points - total_errors) / total_points\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Metric: {metric_name}\")\n",
    "    print(f\"  Class Means: {class_means}\")\n",
    "    print(f\"  Middle Class: {middle_class} (Mean: {middle_class_mean})\")\n",
    "    print(f\"  Middle Class Min: {middle_class_min}, Max: {middle_class_max}\")\n",
    "    print(f\"  Errors (C1 > min): {error_1}, Errors (C3 < max): {error_3}\")\n",
    "    print(f\"  Total Errors: {total_errors}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Calculate and print accuracy for each metric\n",
    "results = {}\n",
    "for metric in metrics:\n",
    "    results[metric] = calculate_accuracy(datasets, metric)\n",
    "\n",
    "# Summary of accuracies\n",
    "print(\"\\nSummary of Accuracies:\")\n",
    "for metric, acc in results.items():\n",
    "    print(f\"{metric}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"explod\": pd.read_csv(\"sdnorm_ex.csv\"),\n",
    "    \"sd\": pd.read_csv(\"sdnorm_sd.csv\"),\n",
    "    \"control\": pd.read_csv(\"sdnorm_cond.csv\")\n",
    "}\n",
    "\n",
    "# Prepare all metrics\n",
    "metrics = [\n",
    "    \"MSE\",\n",
    "    \"Euclidean Distance\",\n",
    "    \"Cosine Distance\",\n",
    "    \"L1 Distance\",\n",
    "    \"Pearson Correlation\",\n",
    "    \"Dot Product\",\n",
    "    \"Jaccard Similarity\",\n",
    "    \"Hamming Distance\",\n",
    "    \"Mahalanobis Distance\"\n",
    "]\n",
    "\n",
    "# Define colors and hatching patterns for each dataset\n",
    "colors = {\"explod\": \"blue\", \"sd\": \"orange\", \"control\": \"green\"}\n",
    "hatch_styles = {\"explod\": \"o\", \"sd\": \"\\\\\", \"control\": \"*\"}\n",
    "\n",
    "# Plotting function\n",
    "def plot_metric_distribution(metric_name, bins=20):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for dataset_name, df in datasets.items():\n",
    "        if metric_name in df.columns:\n",
    "            values = df[metric_name].tolist()\n",
    "            plt.hist(\n",
    "                values,\n",
    "                bins=bins,\n",
    "                alpha=0.6,\n",
    "                label=f\"{dataset_name.capitalize()} ({metric_name})\",\n",
    "                color=colors[dataset_name],\n",
    "                hatch=hatch_styles[dataset_name],\n",
    "                edgecolor=\"black\",\n",
    "                density=True  # Normalize to density\n",
    "            )\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.title(f\"{metric_name} Distribution for Different Datasets (Density)\", fontsize=14)\n",
    "    plt.xlabel(metric_name, fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for all metrics\n",
    "for metric in metrics:\n",
    "    plot_metric_distribution(metric, bins=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
