{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "#from helper import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Now you can use the `device` variable\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay,max_epochs):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        \n",
    "        # Modify the fully connected layer\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, 20)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=max_epochs, eta_min=lr / 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)\n",
    "\n",
    "    def info_nce_loss(self, imgs1, imgs2, device):\n",
    "\n",
    "        imgs = torch.cat((imgs1, imgs2), dim=0)  # Concatenate along the batch dimension\n",
    "        imgs = imgs.to(device)  # Move images to the device\n",
    "\n",
    "        # Encode all images\n",
    "        feats = self.forward(imgs)\n",
    "    \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = nn.functional.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
    "    \n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "    \n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
    "    \n",
    "        # Normalize similarity scores by temperature\n",
    "        cos_sim = cos_sim / self.temperature\n",
    "\n",
    "        # InfoNCE loss\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Accuracy calculations\n",
    "        # Create a combination of positive and negative similarities for ranking\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:, None],  # Positive example in first position\n",
    "                          cos_sim.masked_fill(pos_mask, -9e15)], dim=-1)\n",
    "    \n",
    "        # Sort and get the ranking position of the positive example\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "    \n",
    "        # Compute accuracy metrics\n",
    "        top1_acc = (sim_argsort == 0).float().mean()  # Top-1 accuracy\n",
    "        top5_acc = (sim_argsort < 5).float().mean()   # Top-5 accuracy\n",
    "        mean_pos = 1 + sim_argsort.float().mean()     # Mean position of the positive example\n",
    "\n",
    "        return nll, top1_acc, top5_acc, mean_pos\n",
    "\n",
    "    def train_epoch(self, train_loader, device):\n",
    "        self.train()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            imgs1, imgs2, _ = batch\n",
    "            imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate loss and accuracy metrics\n",
    "            loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #self.lr_scheduler.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_top1_acc += top1_acc.item()\n",
    "            total_top5_acc += top5_acc.item()\n",
    "            total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(train_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(train_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(train_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos\n",
    "\n",
    "    def validate_epoch(self, val_loader, device):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                imgs1, imgs2, _ = batch\n",
    "                imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "\n",
    "                # Calculate loss and accuracy metrics\n",
    "                loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "                # Accumulate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_top1_acc += top1_acc.item()\n",
    "                total_top5_acc += top5_acc.item()\n",
    "                total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(val_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(val_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(val_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos\n",
    "    \n",
    "\n",
    "    def inference_epoch(self, inference_loader, device):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(inference_loader, desc=\"Inferencing\", leave=False):\n",
    "                imgs1, imgs2, _ = batch\n",
    "                imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "\n",
    "                # Calculate loss and accuracy metrics\n",
    "                loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "                # Accumulate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_top1_acc += top1_acc.item()\n",
    "                total_top5_acc += top5_acc.item()\n",
    "                total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(inference_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(inference_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(inference_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(inference_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "model_path = r'C:\\Users\\k54739\\saved_model\\ohneContrastSweetcrop_simclr_model_epoch_245.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_22448\\2711482870.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  simclr_model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (convnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=512, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinitialize the model\n",
    "simclr_model = SimCLR(hidden_dim=128, lr=5e-4, temperature=0.07, weight_decay=1e-4,max_epochs=245)\n",
    "simclr_model.load_state_dict(torch.load(model_path))\n",
    "simclr_model.to(device)\n",
    "simclr_model.eval()\n",
    "# Load the metrics\n",
    "#loaded_metrics = torch.load(metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference before projection head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def load_and_split_data(root_dir, test_size=0.2):\n",
    "    #classes = ['untreated', 'single_dose', 'drug_screened']\n",
    "    classes = ['control', 'treated']\n",
    "    image_files = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        labels.extend([idx] * len(files))\n",
    "    \n",
    "    # Check if the labels correctly reflect the classes\n",
    "    #print(\"Label distribution:\", {classes[i]: labels.count(i) for i in range(len(classes))})\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "        image_files, labels, test_size=test_size, stratify=labels, random_state=42)\n",
    "\n",
    "    return train_files, test_files, train_labels, test_labels\n",
    "\n",
    "\n",
    "# Directories for labeled data\n",
    "image_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\train_sd\"\n",
    "\n",
    "# Load and split the data\n",
    "train_files, test_files, train_labels, test_labels = load_and_split_data(image_dir, test_size=0.2)\n",
    "\n",
    "# Create the labeled datasets\n",
    "train_labeled_dataset = LabeledImageDataset(train_files, train_labels)\n",
    "test_labeled_dataset = LabeledImageDataset(test_files, test_labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader_labeled = DataLoader(train_labeled_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=0)\n",
    "test_loader_labeled = DataLoader(test_labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for anchor,label in train_loader_labeled:\n",
    "    print(anchor.shape, label.shape, anchor.dtype)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter\n",
    "total_images = 0\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for anchor, label in train_loader_labeled:\n",
    "    total_images += anchor.size(0)  # Increment by the batch size (number of images in the current batch)\n",
    "\n",
    "# Print the total number of images\n",
    "print(f\"Total number of images in train_loader_labeled: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a counter\n",
    "total_images = 0\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for anchor, label in test_loader_labeled:\n",
    "    total_images += anchor.size(0)  # Increment by the batch size (number of images in the current batch)\n",
    "\n",
    "# Print the total number of images\n",
    "print(f\"Total number of images in test_loader_labeled: {total_images}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        print(f\"Batch features shape: {batch_feats.shape}\")\n",
    "        print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    print(f\"Features shape after concatenation: {feats.shape}\")\n",
    "    print(f\"Labels shape after concatenation: {labels.shape}\")\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for train and test datasets\n",
    "train_feats_simclr = prepare_data_features(simclr_model, train_loader_labeled)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_loader_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model definition\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_logistic_regression(model, train_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_feats, batch_labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch_feats, batch_labels = batch_feats.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_feats)\n",
    "        loss = nn.functional.cross_entropy(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_logistic_regression(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_feats, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            batch_feats, batch_labels = batch_feats.to(device), batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_feats)\n",
    "            loss = nn.functional.cross_entropy(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg_simplified(batch_size, train_feats_data, test_feats_data, feature_dim, num_classes, lr=5e-4, weight_decay=1e-4, max_epochs=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                              drop_last=False, pin_memory=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                             drop_last=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "    # Model, loss, and optimizer\n",
    "    model = LogisticRegression(feature_dim, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                               milestones=[int(max_epochs * 0.6), int(max_epochs * 0.8)],\n",
    "                                               gamma=0.1)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Track best by accuracy\n",
    "    best_test_acc = -1.0\n",
    "    best_model_state_acc = None\n",
    "\n",
    "    # Track best by loss (with accuracy as a tiebreaker)\n",
    "    best_test_loss = float('inf')\n",
    "    best_test_loss_acc = -1.0\n",
    "    best_model_state_loss = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_logistic_regression(model, train_loader, optimizer, scheduler, device)\n",
    "        test_loss, test_acc = evaluate_logistic_regression(model, test_loader, criterion, device)\n",
    "\n",
    "        print(f\"Training loss: {train_loss:.4f}, Training accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        # Check for best accuracy model\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_model_state_acc = model.state_dict()\n",
    "\n",
    "        # Check for best loss model\n",
    "        # Condition: strictly lower loss OR equal loss but higher accuracy\n",
    "        if (test_loss < best_test_loss) or (test_loss == best_test_loss and test_acc > best_test_loss_acc):\n",
    "            best_test_loss = test_loss\n",
    "            best_test_loss_acc = test_acc\n",
    "            best_model_state_loss = model.state_dict()\n",
    "\n",
    "    # Now we have two best states: best_model_state_acc and best_model_state_loss\n",
    "    # Create two separate model instances for them\n",
    "    best_acc_model = LogisticRegression(feature_dim, num_classes).to(device)\n",
    "    best_acc_model.load_state_dict(best_model_state_acc)\n",
    "    best_acc_model.eval()\n",
    "\n",
    "    best_loss_model = LogisticRegression(feature_dim, num_classes).to(device)\n",
    "    best_loss_model.load_state_dict(best_model_state_loss)\n",
    "    best_loss_model.eval()\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Loss curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(max_epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(max_epochs), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(max_epochs), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(max_epochs), test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return both models and the final results (e.g., last train_acc and test_acc recorded)\n",
    "    return best_acc_model, best_loss_model, {\"train_acc\": train_acc, \"test_acc\": test_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model\n",
    "# Example usage:\n",
    "best_acc_model, best_loss_model, results = train_logreg_simplified(\n",
    "    batch_size=8,\n",
    "    train_feats_data=train_feats_simclr,\n",
    "    test_feats_data=test_feats_simclr,\n",
    "    feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
    "    num_classes=2,\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=750\n",
    ")\n",
    "#print(f\"Final Train Accuracy: {results['train_acc']:.4f}, Final Test Accuracy: {results['test_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model= best_loss_model\n",
    "logreg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the logistic regression model\n",
    "# Example usage:\n",
    "best_acc_model, best_loss_model, results = train_logreg_simplified(\n",
    "    batch_size=8,\n",
    "    train_feats_data=train_feats_simclr,\n",
    "    test_feats_data=test_feats_simclr,\n",
    "    feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
    "    num_classes=2,\n",
    "    lr=5e-4,\n",
    "    weight_decay=1e-4,\n",
    "    max_epochs=100\n",
    ")\n",
    "#print(f\"Final Train Accuracy: {results['train_acc']:.4f}, Final Test Accuracy: {results['test_acc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models by accuracy and loss\n",
    "torch.save(best_acc_model.state_dict(), \"1000epoch_best_acc_model.pth\")\n",
    "torch.save(best_loss_model.state_dict(), \"1000epoch_best_loss_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(512,2)\n",
    "logreg_model = logreg_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_22448\\4036842466.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"1000epoch_best_loss_model.pth\")  # Load weights from the file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 2: Load the saved state dict\n",
    "state_dict = torch.load(\"1000epoch_best_loss_model.pth\")  # Load weights from the file\n",
    "logreg_model.load_state_dict(state_dict)  # Load state dict into the model\n",
    "\n",
    "# Step 3: Set the model to evaluation mode (if not training)\n",
    "logreg_model.eval()  # This disables dropout and batchnorm for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 512= feature_dim = train_feats_simclr.tensors[0].shape[1] =  before projection head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model state_dict\n",
    "torch.save(logreg_model.state_dict(), \"logreg_model_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1000 epochs: no outlier amoung exploded, control7, single dose\n",
    "#500 epochs, one outlier of exploder in ds closer to sd: which is ds41_g9_C10-T01   inbetween 4 ds closer\n",
    "# 100 epochs, one outlier of exploder also one outlier from ds_clos_tosd ds41_g5_E04-T01\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is inference for ex85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 6/6 [00:03<00:00,  1.55it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAHUCAYAAAAEKdj3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKDElEQVR4nO3dd3wVVf7/8fdN4aZfCJAmMfQSlKIUCSIgHUUpChaayP5EQFYQXIpIABWNiqwF1t2FgCxtVURckCJIE1iBBVFAFpSmECIBksCF1Pn94Td3vZOE5MYk9wKv5+MxD52ZMzOfe+ck3HfOzFyLYRiGAAAAAAAOXu4uAAAAAAA8DUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQmA22VlZal+/fp69dVX3V2KFi9erFmzZpXJvufPny+LxaLjx487lg0YMEA9e/Ys9j6qV68ui8Uii8UiLy8v2Ww2NWjQQAMHDtS6desK3MZisSg+Pt6lWlevXu3yNgUdK+8179692+V9Feb06dOKj4/Xvn378q2Lj4+XxWIptWOVRFZWliIiImSxWPTRRx+5tZbysHLlSlksFlWuXFkZGRkFtqlevboGDx7smD9+/LgsFovmz59frGOcPXtWEydOVJMmTRQSEqIKFSqoWrVq6t27t1auXKmcnJxSeCUA4IygBMDtZs+erQsXLuiZZ55xdyllGpQKEh8fr1WrVmnjxo3F3qZ169basWOHtm/fro8//lgjR47UsWPH1KVLFz300EPKyspyar9jxw4NHTrUpbpWr16tqVOnurRNSY/lqtOnT2vq1KkFBqWhQ4dqx44dZXr8ovzrX//S2bNnJUlz5851ay3lIe81nj9/XitWrCj1/e/cuVO33367/va3v+mBBx7Q0qVL9cUXX+jVV1+Vr6+vevfuXezABQCu8HF3AQBubtnZ2Xr99dc1ZMgQBQYGurscl+Tk5Cg7O1tWq7XE+6hVq5a6du2qV199Vffee2+xtqlYsaLuuusux3zHjh01YsQIxcfHa+rUqXrhhRf02muvOdb/tm1ZMAxDV69elb+/f5kfqyjVqlVTtWrV3FrD3LlzVaFCBbVt21br1q3TTz/9VGo12e12BQQElMq+SkNSUpJWr16te++9V9u3b9fcuXPVr1+/Utv/xYsX1bNnTwUFBemrr75SZGSk0/r+/ftr//79SklJueZ+rly5Ij8/P7ePNgK4vjCiBKDU5V3+tHfvXvXu3VshISGy2Wzq37+/fvnlF6e2K1eu1M8//6wBAwbk28/333+vRx99VOHh4bJarbr11ls1cOBAp8t7vvvuOz344IOqVKmS/Pz81KRJEy1YsMBpP5s2bZLFYtGSJUs0adIkRUVFKSQkRB07dtThw4cd7dq1a6dVq1bpxIkTjsvb8j5Y5V0qlJCQoJdeekk1atSQ1WrVl19+6XgdrVq1UkBAgIKDg9WpU6dij2wMGDBAX3zxhX744YfivcGFiI+PV8OGDfXuu+/q6tWrjuXmy+HsdrvGjh2rGjVqyM/PT6GhoWrWrJmWLFkiSRo8eLDee+89x7Z5U94lgxaLRSNHjtRf/vIXNWjQQFar1fGeF3aZ34ULF/TEE08oNDRUgYGB6tGjh3788UenNubLs/K0a9dO7dq1k/TruWzevLkk6YknnnDUlnfMgi69y83NVUJCgurXry+r1aqwsDANHDhQP/30U77j3Hbbbdq1a5fatGmjgIAA1axZU6+++qpyc3MLf+N/4/Tp01qzZo169OihcePGKTc3t9DRjsWLF6tVq1YKCgpSUFCQmjRp4jQClVfPli1bFBcXp4CAAA0ZMkSSdPLkSfXv319hYWGyWq1q0KCB3nzzzXx1zpkzR40bN1ZQUJCCg4NVv359TZw40bG+qL5QlAULFig7O1ujR49W7969tWHDBp04caJY2xbH3/72N509e1YJCQn5QlKeRo0aqX379o75vMs9161bpyFDhqhq1aoKCAhQRkZGsftCcfqi9L/fLf/4xz80ZswYRUREyN/fX23bttXevXudtv3xxx/1yCOPKCoqSlarVeHh4erQoUOBI6MAPANBCUCZ6dWrl2rXrq2PPvpI8fHxWrFihbp06eJ0adiqVasUFham2NhYp22/+eYbNW/eXDt37tS0adP0+eefa8aMGcrIyFBmZqYk6fDhw4qLi9OBAwf09ttva/ny5YqNjdXgwYOVkJCQr56JEyfqxIkT+vvf/66//vWvOnLkiHr06OG4v2H27Nlq3bq1IiIitGPHDsf0W2+//bY2btyoN954Q59//rnq16+vxYsX68EHH1RISIiWLFmiuXPn6sKFC2rXrp22bdtW5PvUrl07GYah1atXu/wem/Xo0UN2u/2a9wSNGTNGc+bM0ahRo7RmzRotXLhQDz/8sOOv8pMnT9ZDDz0kSU7vw28/qK5YsUJz5szRiy++qLVr16pNmzbXrOvJJ5+Ul5eX49LGr7/+Wu3atdPFixdden133HGHEhMTJUkvvPCCo7ZrXe739NNP609/+pM6deqklStXavr06VqzZo3i4uJ07tw5p7ZJSUl6/PHH1b9/f61cuVLdunXThAkT9I9//KNY9c2fP185OTkaMmSIOnbsqJiYGM2bN0+GYTi1e/HFF/X4448rKipK8+fP1yeffKJBgwblCxlnzpxR//799dhjj2n16tUaPny4fvnlF8XFxWndunWaPn26Vq5cqY4dO2rs2LEaOXKkY9ulS5dq+PDhatu2rT755BOtWLFCo0eP1uXLlx1tiuoLRZk3b54iIyPVrVs3DRky5JrBsCTWr18vb29vde/e3eVthwwZIl9fXy1cuFAfffSRfH19XeoLrpg4caJ+/PFH/f3vf9ff//53nT59Wu3atXP6Y0D37t21Z88eJSQkaP369ZozZ46aNm3q8s8AgHJkAEApmzJliiHJGD16tNPyRYsWGZKMf/zjH45lDRo0MLp27ZpvH/fee69RsWJFIzk5udDjPPLII4bVajVOnjzptLxbt25GQECAcfHiRcMwDOPLL780JBndu3d3avfPf/7TkGTs2LHDsey+++4zYmJi8h3r2LFjhiSjVq1aRmZmpmN5Tk6OERUVZdx+++1GTk6OY3l6eroRFhZmxMXFOZYlJiYakoxjx47l2/8tt9xi9OvXr9DXmicmJsa47777Cl0/Z84cQ5KxbNkyxzJJxpQpUxzzt912m9GzZ89rHmfEiBFGYf9ESDJsNptx/vz5Atf99lh5r7lXr15O7b766itDkvHSSy85vbZBgwbl22fbtm2Ntm3bOuZ37dplSDISExPztc3re3kOHTpkSDKGDx/u1O7f//63IcmYOHGi03EkGf/+97+d2sbGxhpdunTJdyyz3Nxco3bt2sYtt9xiZGdnO9WzYcMGR7sff/zR8Pb2Nh5//PFr7i+vnt9uaxiGMX78+ALrfPrppw2LxWIcPnzYMAzDGDlypFGxYsVrHqM4faEwW7ZsMSQZ48ePNwzj19dfo0YNIyYmxsjNzXVqaz63eT9PBZ3D36pfv74RERGRb3lOTo6RlZXlmH77s5fX5wYOHOi0jSt9obh9Me93yx133OH0mo8fP274+voaQ4cONQzDMM6dO2dIMmbNmnXN1wvAszCiBKDMPP74407zffv2lY+Pj+NyNenXS5XCwsKc2tntdm3evFl9+/ZV1apVC93/xo0b1aFDB0VHRzstHzx4sOx2e77RoAceeMBpvlGjRpLk0qVCDzzwgHx9fR3zhw8f1unTpzVgwAB5ef3vV2pQUJD69OmjnTt3ym63F7nfsLAw/fzzz8WuozCGaeSiIC1atNDnn3+u8ePHa9OmTbpy5YrLx7n33ntVqVKlYrc394W4uDjFxMQ49YWykLd/82VULVq0UIMGDbRhwwan5REREWrRooXTskaNGhWrj2zevFlHjx7VoEGD5O3tLel/lwfOmzfP0W79+vXKycnRiBEjitxnpUqV8t27tnHjRsXGxuarc/DgwTIMw/FgkBYtWujixYt69NFH9emnnxY4YvJ7+kLeZYJ5lwNaLBYNHjxYJ06cyPe+lrYxY8bI19fXMZl/tiWpT58+TvOu9gVXPPbYY06XfMbExCguLs5xzNDQUNWqVUuvv/66Zs6cqb179xb7ck4A7kNQAlBmIiIinOZ9fHxUuXJlp8t68m6y/q0LFy4oJyenyBvgU1JSCrxvISoqyrH+typXruw0n/cQBlc+HJqPl3eMwurIzc3VhQsXityvn59fiQKLWd4H+rz3oCBvv/22/vSnP2nFihVq3769QkND1bNnTx05cqTYxynsfpHCmPtC3rLiXuJVUkWdn6L6iPRrPynOuckLDr169dLFixd18eJF2Ww23X333fr4448dl1jl3adXnAc8FFR3cfv9gAEDNG/ePJ04cUJ9+vRRWFiYWrZsqfXr1zu2KWlfSE9P14cffqgWLVqoatWqjtfbq1cvWSyWUnva36233qpffvkl3x8bnnvuOe3atUu7du0qtC+6+rP6e/piUf3bYrFow4YN6tKlixISEnTHHXeoatWqGjVqlNLT00t8XABli6AEoMwkJSU5zWdnZyslJcXpw2iVKlV0/vx5p3ahoaHy9vbOd4O1WeXKlXXmzJl8y0+fPu3Yd2kzPygg77UUVoeXl1exRl7Onz//u+s1DEOfffaZAgMD1axZs0LbBQYGaurUqfr++++VlJSkOXPmaOfOnerRo0exj+Xq08PMfSFv2W/7gp+fX4Hfw/N77h0p6vyUVh9JTU3Vxx9/LElq3ry5KlWq5Ji2bt2qq1evavHixZLkGCUtqn9LBb/PrvT7J554Qtu3b1dqaqpWrVolwzB0//33OwJ1SfvCkiVLZLfb9fXXXzu91kaNGskwDH3yySfF+gNBUTp16qScnJx89+9FR0erWbNmatasmSpUqFDgtq7+rP72fXO1Lxanf8fExGju3LlKSkrS4cOHNXr0aM2ePVvjxo0rcJ8A3I+gBKDMLFq0yGn+n//8p7Kzs52eGlW/fv18T3vLe2rUhx9+eM0PyR06dNDGjRsdHxDzfPDBBwoICCjRo6qLO3qQp169errlllu0ePFip8veLl++rI8//tjxJLxryc7O1qlTp/I90MJVU6dO1cGDB/XHP/4x3yhdYcLDwzV48GA9+uijOnz4sOMv9yUZbbsWc1/Yvn27Tpw44dQXqlevrv379zu1++9//+v0ZEJXa8u7bM38MIZdu3bp0KFD6tChQ7Ffw7UsXrxYV65c0fTp0/Xll1/mm6pUqeK4/K5z587y9vbWnDlzSnSsDh066ODBg/rPf/7jtPyDDz6QxWJxegJcnsDAQHXr1k2TJk1SZmamDhw4kK9NYX2hIHPnzlVwcLA2bNiQ77W+/vrrysjIyHfOS2Lo0KEKDw/X888/X2DAcYUrfaG4fTHPkiVLnH7+T5w4oe3btzv179+qW7euXnjhBd1+++35ziMAz8H3KAEoM8uXL5ePj486deqkAwcOaPLkyWrcuLH69u3raNOuXTtNmzYt3/fDzJw5U3fffbdatmyp8ePHq3bt2jp79qxWrlyp999/X8HBwZoyZYr+9a9/qX379nrxxRcVGhqqRYsWadWqVUpISJDNZnO55ttvv13Lly/XnDlzdOedd8rLy+uaozNeXl5KSEjQ448/rvvvv19PPfWUMjIy9Prrr+vixYt69dVXizzm/v37ZbfbC/yAW5CLFy9q586dkn4NZIcPH9bSpUu1detW9e3bt8gvim3ZsqXuv/9+NWrUSJUqVdKhQ4e0cOFCp1B3++23S5Jee+01devWTd7e3mrUqFGhf70vyu7duzV06FA9/PDDOnXqlCZNmqRbbrlFw4cPd7QZMGCA+vfvr+HDh6tPnz46ceKEEhIS8t2nVqtWLfn7+2vRokVq0KCBgoKCFBUVVeDlhvXq1dP/+3//T++88468vLzUrVs3HT9+XJMnT1Z0dLRGjx5dotdjNnfuXFWqVEljx44tMKQOHDhQM2fO1DfffKPGjRtr4sSJmj59uq5cuaJHH31UNptNBw8e1Llz54o8f6NHj9YHH3yg++67T9OmTVNMTIxWrVql2bNn6+mnn1bdunUlSX/4wx/k7++v1q1bKzIyUklJSZoxY4ZsNpvjEevF6Qtm3333nb7++ms9/fTTBX73V+vWrfXmm29q7ty5Tk/hK4mKFStqxYoV6tGjhxo3bqynn35ad911l4KCgpSSkqItW7YoKSlJcXFxRe7Llb5Q3L6YJzk5Wb169dIf/vAHpaamasqUKfLz89OECRMk/fozPnLkSD388MOqU6eOKlSooI0bN2r//v0aP37873qPAJQhNz5IAsANKu9JX3v27DF69OhhBAUFGcHBwcajjz5qnD171qnt0aNHDYvFYvzzn//Mt5+DBw8aDz/8sFG5cmWjQoUKxq233moMHjzYuHr1qqPNt99+a/To0cOw2WxGhQoVjMaNG+d7klbek6k+/PBDp+UFPXnr/PnzxkMPPWRUrFjRsFgsjieo5bV9/fXXC3zNK1asMFq2bGn4+fkZgYGBRocOHYyvvvrKqU1hT72bPHmyUaVKFafXVZiYmBhDkiHJsFgsRlBQkFGvXj1jwIABxtq1awvcRqYn0Y0fP95o1qyZUalSJcNqtRo1a9Y0Ro8ebZw7d87RJiMjwxg6dKhRtWpVx/uQV7ckY8SIEcU6Vt5rXrdunTFgwACjYsWKhr+/v9G9e3fjyJEjTtvm5uYaCQkJRs2aNQ0/Pz+jWbNmxsaNG/M9acwwDGPJkiVG/fr1DV9fX6djmp96Zxi/PiHttddeM+rWrWv4+voaVapUMfr372+cOnXKqV3btm2Nhg0b5ntNgwYNKvBJiHm++eYbQ5Lx7LPPFtrm+++/NyQZzzzzjGPZBx98YDRv3tzw8/MzgoKCjKZNmzr1xcLqMQzDOHHihPHYY48ZlStXNnx9fY169eoZr7/+utPT3xYsWGC0b9/eCA8PNypUqGBERUUZffv2Nfbv3+9oU5y+YPbss88akox9+/YV2ibvyXx79uwxDKPkT73Lk5SUZEyYMMFo1KiRERgYaPj6+hpRUVFGjx49jA8++MDIyspytM3rc7t27cq3n+L2heL2xbzfLQsXLjRGjRplVK1a1bBarUabNm2M3bt3O9qdPXvWGDx4sFG/fn0jMDDQCAoKMho1amS89dZbjickAvA8FsMoxiOSAMAF8fHxmjp1qn755Zdi3QPSo0cPZWdn6/PPPy+H6jxLTk6Oateurccee0wvv/yyu8sB4IJNmzapffv2+vDDDx3fPQbgxsE9SgDcbsaMGfriiy+0a9cud5dS7v7xj3/o0qVL3NANAICHISgBcLvbbrtNiYmJBT456kaXm5urRYsWqWLFiu4uBQAA/AaX3gEAAACACSNKAAAAAGBCUAIAAAAAE4ISAAAAAJi49QtnZ8yYoeXLl+v777+Xv7+/4uLi9Nprr6levXqONoMHD9aCBQuctmvZsqXjyxaLkpubq9OnTys4OFgWi6VU6wcAAABw/TAMQ+np6YqKipKX17XHjNwalDZv3qwRI0aoefPmys7O1qRJk9S5c2cdPHhQgYGBjnZdu3ZVYmKiY96Vb4Y/ffq0oqOjS7VuAAAAANevU6dOqVq1atds49agtGbNGqf5xMREhYWFac+ePbrnnnscy61WqyIiIkp0jODgYEm/vhkhISElLxYAAADAdS0tLU3R0dGOjHAtbg1KZqmpqZKk0NBQp+WbNm1SWFiYKlasqLZt2+rll19WWFhYgfvIyMhQRkaGYz49PV2SFBISQlACAAAAUKxbcjzme5QMw9CDDz6oCxcuaOvWrY7ly5YtU1BQkGJiYnTs2DFNnjxZ2dnZ2rNnj6xWa779xMfHa+rUqfmWp6amEpQAAACAm1haWppsNluxsoHHBKURI0Zo1apV2rZt2zWvFzxz5oxiYmK0dOlS9e7dO99684hS3vAaQQkAAAC4ubkSlDzi0rtnnnlGK1eu1JYtW4q8qSoyMlIxMTE6cuRIgeutVmuBI00AAAAAUFxuDUqGYeiZZ57RJ598ok2bNqlGjRpFbpOSkqJTp04pMjKyHCoEAACAYRjKzs5WTk6Ou0sBrsnb21s+Pj6l8rVAbg1KI0aM0OLFi/Xpp58qODhYSUlJkiSbzSZ/f39dunRJ8fHx6tOnjyIjI3X8+HFNnDhRVapUUa9evdxZOgAAwE0hMzNTZ86ckd1ud3cpQLEEBAQoMjLSpa8UKohb71EqLOklJiZq8ODBunLlinr27Km9e/fq4sWLioyMVPv27TV9+vRifzeSK9chAgAA4H9yc3N15MgReXt7q2rVqqpQoUKp/KUeKAuGYSgzM1O//PKLcnJyVKdOnXxfKnvd3KNUVEbz9/fX2rVry6kaAAAA/FZmZqZyc3MVHR2tgIAAd5cDFMnf31++vr46ceKEMjMz5efnV+J9eRXdBAAAADcz81/lAU9WWv2VXg8AAAAAJgQlAAAAADDxiO9RAgAAwPUlNTW13J6EFxAQIJvNVi7HQtmZP3++nn32WV28eNHdpRQLQQkAAAAuSU1N1csJbyklvXyCUuXgAE16frTHhKVdu3Zp/Pjx2rNnjywWi5o3b66EhAQ1adJEknT8+PECvx/0888/V9euXcu5Ws915swZPffcc9qzZ4+OHDmiUaNGadasWfnaffzxx5o8ebJ++OEH1apVSy+//HK5fFUQQQkAAAAusdvtSkm3K7Th3QqyhZbpsS6lnlfKgW2y2+0eEZTS09PVpUsXPfjgg5o9e7ays7M1ZcoUdenSRT/99JN8fX0dbb/44gs1bNjQMR8aWrbv1fUmIyNDVatW1aRJk/TWW28V2GbHjh3q16+fpk+frl69eumTTz5R3759tW3bNrVs2bJM6+MeJQAAAJRIkC1UIZXDynQqaRAzDEMJCQmqWbOm/P391bhxY3300UcyDEMdO3ZU165dHV9Vc/HiRd16662aNGlSkfs9fPiwLly4oGnTpqlevXpq2LChpkyZouTkZJ08edKpbeXKlRUREeGYivsFqPHx8WrSpIkWLlyo6tWry2az6ZFHHlF6erqjTUZGhkaNGqWwsDD5+fnp7rvv1q5du4r9/hw4cED33XefQkJCFBwcrDZt2uiHH36Q9Ov3Z02bNk3VqlWT1WpVkyZNtGbNGse2x48fl8Vi0fLly9W+fXsFBASocePG2rFjh9Mx5s+fr1tvvVUBAQHq1auXUlJSnNZXr15df/7znzVw4MBCQ/CsWbPUqVMnTZgwQfXr19eECRPUoUOHAkeeShsjSuWsPK/nLQrX+wIAgBvVCy+8oOXLl2vOnDmqU6eOtmzZov79+6tq1apasGCBbr/9dr399tv64x//qGHDhik8PFzx8fFF7rdevXqqUqWK5s6dq4kTJyonJ0dz585Vw4YNFRMT49T2gQce0NWrV1WnTh2NHj1aDz30ULHr/+GHH7RixQr961//0oULF9S3b1+9+uqrevnllyVJzz//vD7++GMtWLBAMTExSkhIUJcuXXT06NEiR65+/vln3XPPPWrXrp02btyokJAQffXVV8rOzpYk/fnPf9abb76p999/X02bNtW8efP0wAMP6MCBA6pTp45jP5MmTdIbb7yhOnXqaNKkSXr00Ud19OhR+fj46N///reGDBmiV155Rb1799aaNWs0ZcqUYr/+PDt27NDo0aOdlnXp0oWgdKMp7+t5i+Jp1/sCAACUhsuXL2vmzJnauHGjWrVqJUmqWbOmtm3bpvfff1+LFy/W+++/rwEDBujs2bP67LPPtHfvXqfL5goTHBysTZs26cEHH9T06dMlSXXr1tXatWvl4/PrR+ugoCDNnDlTrVu3lpeXl1auXKl+/fppwYIF6t+/f7FeQ25urubPn6/g4GBJ0oABA7Rhwwa9/PLLunz5subMmaP58+erW7dukqS//e1vWr9+vebOnatx48Zdc9/vvfeebDabli5d6njNdevWdax/44039Kc//UmPPPKIJOm1117Tl19+qVmzZum9995ztBs7dqzuu+8+SdLUqVPVsGFDHT16VPXr19ef//xndenSRePHj3fsf/v27U4jU8WRlJSk8PBwp2Xh4eFKSkpyaT8lQVAqR+V5PW9RPO16XwAAgNJy8OBBXb16VZ06dXJanpmZqaZNm0qSHn74YX3yySeaMWOG5syZ4xQUruXKlSsaMmSIWrdurSVLlignJ0dvvPGGunfvrl27dsnf319VqlRxGgVp1qyZLly4oISEhGIHperVqztCkiRFRkYqOTlZ0q+jTVlZWWrdurVjva+vr1q0aKFDhw4Vue99+/apTZs2BQbDtLQ0nT592mnfktS6dWt98803TssaNWrkVJ8kJScnq379+jp06FC+By60atXK5aAkSRaLxWneMIx8y8oCQckN8q7ndbfz7i4AAACgDOTm5kqSVq1apVtuucVpndVqlfTrH7D37Nkjb29vHTlypNj7Xrx4sY4fP64dO3bIy8vLsaxSpUr69NNPHaMwZnfddZf+/ve/F/s45hBjsVgcryvv3qqSBgh/f/8i2xRn37+tMW+ducbfKyIiIt/oUXJycr5RprLAwxwAAABwQ4mNjZXVatXJkydVu3Ztpyk6OlqS9Nxzz8nLy0uff/653n77bW3cuLFY+7bb7fLy8nIKDXnzeSGhIHv37nWMuvxetWvXVoUKFbRt2zbHsqysLO3evVsNGjQocvtGjRpp69atysrKyrcuJCREUVFRTvuWpO3btxdr33liY2O1c+dOp2Xm+eJo1aqV1q9f77Rs3bp1iouLc3lfrmJECQAAACVyKbXsr08pyTGCg4M1duxYjR49Wrm5ubr77ruVlpam7du3KygoSFWqVNG8efO0Y8cO3XHHHRo/frwGDRqk/fv3q1KlStfcd6dOnTRu3DiNGDFCzzzzjHJzc/Xqq6/Kx8dH7du3lyQtWLBAvr6+atq0qby8vPTZZ5/p7bff1muvvVai98AsMDBQTz/9tMaNG6fQ0FDdeuutSkhIkN1u15NPPlnk9iNHjtQ777yjRx55RBMmTJDNZtPOnTvVokUL1atXT+PGjdOUKVNUq1YtNWnSRImJidq3b58WLVpU7BpHjRqluLg4JSQkqGfPnlq3bl2Bl93t27dPknTp0iX98ssv2rdvnypUqKDY2FhJ0h//+Efdc889eu211/Tggw/q008/1RdffJEvyJUFghIAAABcEhAQoMrBAUo5sK1cLuWvHByggIAAl7aZPn26wsLCNGPGDP3444+qWLGi7rjjDk2YMEH9+vVTfHy87rjjDknSlClTtG7dOg0bNkzLli275n7r16+vzz77TFOnTlWrVq3k5eWlpk2bas2aNU4jRi+99JJOnDghb29v1a1bV/PmzSv2/UnF8eqrryo3N1cDBgxQenq6mjVrprVr1xYZ9KRfH1u+ceNGjRs3Tm3btpW3t7eaNGniuC9p1KhRSktL03PPPafk5GTFxsZq5cqVTk+8K0repYZTpkxRfHy8OnbsqBdeeMHxAIw8efeMSdKePXu0ePFixcTE6Pjx45KkuLg4LV26VC+88IImT56sWrVqadmyZWX+HUqSZDFK6wJCD5WWliabzabU1FSFhIS4tZYzZ87ohRlv6da4B9x+j1JaSrJObl+plyaMLrVhYAAAcGO5evWqjh07pho1asjPz89pXXl+5QlfaQJXXKvfupINGFECAACAy2w2G+EFNzQe5gAAAAD8n2HDhikoKKjAadiwYaVyjIYNGxZ6DFfuAypMebyGmwEjSgAAAMD/mTZtmsaOHVvgutK6jWP16tUFPnFOUqk89ro8XsPNgKAEAAAA/J+wsDCFhZXtveQxMTFluv/yeA03Ay69AwAAwDXd4M/+wg2mtPorQQkAAAAF8vX1laRye7odUBry+mte/y0pLr0DAABAgby9vVWxYkUlJydL+vUx3RaLxc1VAQUzDEN2u13JycmqWLGivL29f9f+CEoAAAAoVEREhCQ5whLg6SpWrOjot78HQQkAAACFslgsioyMVFhYWKFPagM8ha+v7+8eScpDUAIAAECRvL29S+0DKHA94GEOAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJj7uLgAAAABA6UtNTZXdbnd3GZKkgIAA2Ww2d5fhEoISAAAAcINJTU3VywlvKSXdM4JS5eAATXp+9HUVlghKAAAAwA3GbrcrJd2u0IZ3K8gW6tZaLqWeV8qBbbLb7QQlAAAAAO4XZAtVSOUwd5eh8+4uoAR4mAMAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYOLWoDRjxgw1b95cwcHBCgsLU8+ePXX48GGnNoZhKD4+XlFRUfL391e7du104MABN1UMAAAA4Gbg1qC0efNmjRgxQjt37tT69euVnZ2tzp076/Lly442CQkJmjlzpt59913t2rVLERER6tSpk9LT091YOQAAAIAbmY87D75mzRqn+cTERIWFhWnPnj265557ZBiGZs2apUmTJql3796SpAULFig8PFyLFy/WU0895Y6yAQAAANzgPOoepdTUVElSaGioJOnYsWNKSkpS586dHW2sVqvatm2r7du3F7iPjIwMpaWlOU0AAAAA4AqPCUqGYWjMmDG6++67ddttt0mSkpKSJEnh4eFObcPDwx3rzGbMmCGbzeaYoqOjy7ZwAAAAADccjwlKI0eO1P79+7VkyZJ86ywWi9O8YRj5luWZMGGCUlNTHdOpU6fKpF4AAAAANy633qOU55lnntHKlSu1ZcsWVatWzbE8IiJC0q8jS5GRkY7lycnJ+UaZ8litVlmt1rItGAAAAMANza0jSoZhaOTIkVq+fLk2btyoGjVqOK2vUaOGIiIitH79eseyzMxMbd68WXFxceVdLgAAAICbhFtHlEaMGKHFixfr008/VXBwsOO+I5vNJn9/f1ksFj377LN65ZVXVKdOHdWpU0evvPKKAgIC9Nhjj7mzdAAAAAA3MLcGpTlz5kiS2rVr57Q8MTFRgwcPliQ9//zzunLlioYPH64LFy6oZcuWWrdunYKDg8u5WgAAAAA3C7cGJcMwimxjsVgUHx+v+Pj4si8IAAAAAORBT70DAAAAAE9BUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABO3BqUtW7aoR48eioqKksVi0YoVK5zWDx48WBaLxWm666673FMsAAAAgJuGW4PS5cuX1bhxY7377ruFtunatavOnDnjmFavXl2OFQIAAAC4Gfm48+DdunVTt27drtnGarUqIiKinCoCAAAAgOvgHqVNmzYpLCxMdevW1R/+8AclJydfs31GRobS0tKcJgAAAABwhUcHpW7dumnRokXauHGj3nzzTe3atUv33nuvMjIyCt1mxowZstlsjik6OrocKwYAAABwI3DrpXdF6devn+P/b7vtNjVr1kwxMTFatWqVevfuXeA2EyZM0JgxYxzzaWlphCUAAAAALvHooGQWGRmpmJgYHTlypNA2VqtVVqu1HKsCAAAAcKPx6EvvzFJSUnTq1ClFRka6uxQAAAAANzC3jihdunRJR48edcwfO3ZM+/btU2hoqEJDQxUfH68+ffooMjJSx48f18SJE1WlShX16tXLjVUDAAAAuNG5NSjt3r1b7du3d8zn3Vs0aNAgzZkzR99++60++OADXbx4UZGRkWrfvr2WLVum4OBgd5UMAAAA4Cbg1qDUrl07GYZR6Pq1a9eWYzUAAAAA8Kvr6h4lAAAAACgPBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgEmJglLNmjWVkpKSb/nFixdVs2bN310UAAAAALhTiYLS8ePHlZOTk295RkaGfv75599dFAAAAAC4k48rjVeuXOn4/7Vr18pmsznmc3JytGHDBlWvXr3UigMAAAAAd3ApKPXs2VOSZLFYNGjQIKd1vr6+ql69ut58881SKw4AAAAA3MGloJSbmytJqlGjhnbt2qUqVaqUSVEAAAAA4E4uBaU8x44dK+06AAAAAMBjlCgoSdKGDRu0YcMGJScnO0aa8sybN+93FwYAAAAA7lKioDR16lRNmzZNzZo1U2RkpCwWS2nXBQAAAABuU6Kg9Je//EXz58/XgAEDSrseAAAAAHC7En2PUmZmpuLi4kq7FgAAAADwCCUKSkOHDtXixYtLuxYAAAAA8AgluvTu6tWr+utf/6ovvvhCjRo1kq+vr9P6mTNnlkpxAAAAAOAOJQpK+/fvV5MmTSRJ3333ndM6HuwAAAAA4HpXoqD05ZdflnYdAAAAAOAxSnSPEgAAAADcyEo0otS+fftrXmK3cePGEhcEAAAAAO5WoqCUd39SnqysLO3bt0/fffedBg0aVBp1AQAAAIDblCgovfXWWwUuj4+P16VLl35XQQAAAADgbqV6j1L//v01b9680twlAAAAAJS7Ug1KO3bskJ+fX2nuEgAAAADKXYkuvevdu7fTvGEYOnPmjHbv3q3JkyeXSmEAAAAA4C4lCko2m81p3svLS/Xq1dO0adPUuXPnUikMAAAAANylREEpMTGxtOsAAAAAAI9RoqCUZ8+ePTp06JAsFotiY2PVtGnT0qoLAAAAANymREEpOTlZjzzyiDZt2qSKFSvKMAylpqaqffv2Wrp0qapWrVradQIAAABAuSnRU++eeeYZpaWl6cCBAzp//rwuXLig7777TmlpaRo1alRp1wgAAAAA5apEI0pr1qzRF198oQYNGjiWxcbG6r333uNhDgAAAACueyUaUcrNzZWvr2++5b6+vsrNzf3dRQEAAACAO5UoKN1777364x//qNOnTzuW/fzzzxo9erQ6dOhQasUBAAAAgDuUKCi9++67Sk9PV/Xq1VWrVi3Vrl1bNWrUUHp6ut55553SrhEAAAAAylWJ7lGKjo7Wf/7zH61fv17ff/+9DMNQbGysOnbsWNr1AQAAAEC5c2lEaePGjYqNjVVaWpokqVOnTnrmmWc0atQoNW/eXA0bNtTWrVvLpFAAAAAAKC8uBaVZs2bpD3/4g0JCQvKts9lseuqppzRz5sxSKw4AAAAA3MGloPTNN9+oa9euha7v3Lmz9uzZ87uLAgAAAAB3cikonT17tsDHgufx8fHRL7/88ruLAgAAAAB3ciko3XLLLfr2228LXb9//35FRkb+7qIAAAAAwJ1cCkrdu3fXiy++qKtXr+Zbd+XKFU2ZMkX3339/qRUHAAAAAO7g0uPBX3jhBS1fvlx169bVyJEjVa9ePVksFh06dEjvvfeecnJyNGnSpLKqFQAAAADKhUtBKTw8XNu3b9fTTz+tCRMmyDAMSZLFYlGXLl00e/ZshYeHl0mhAAAAAFBeXP7C2ZiYGK1evVoXLlzQ0aNHZRiG6tSpo0qVKpVFfQAAAABQ7lwOSnkqVaqk5s2bl2YtAAAAAOARXHqYAwAAAADcDAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMDErUFpy5Yt6tGjh6KiomSxWLRixQqn9YZhKD4+XlFRUfL391e7du104MAB9xQLAAAA4Kbh1qB0+fJlNW7cWO+++26B6xMSEjRz5ky9++672rVrlyIiItSpUyelp6eXc6UAAAAAbiY+7jx4t27d1K1btwLXGYahWbNmadKkSerdu7ckacGCBQoPD9fixYv11FNPlWepAAAAAG4iHnuP0rFjx5SUlKTOnTs7llmtVrVt21bbt28vdLuMjAylpaU5TQAAAADgCo8NSklJSZKk8PBwp+Xh4eGOdQWZMWOGbDabY4qOji7TOgEAAADceDw2KOWxWCxO84Zh5Fv2WxMmTFBqaqpjOnXqVFmXCAAAAOAG49Z7lK4lIiJC0q8jS5GRkY7lycnJ+UaZfstqtcpqtZZ5fQAAAABuXB47olSjRg1FRERo/fr1jmWZmZnavHmz4uLi3FgZAAAAgBudW0eULl26pKNHjzrmjx07pn379ik0NFS33nqrnn32Wb3yyiuqU6eO6tSpo1deeUUBAQF67LHH3Fg1AAAAgBudW4PS7t271b59e8f8mDFjJEmDBg3S/Pnz9fzzz+vKlSsaPny4Lly4oJYtW2rdunUKDg52V8kAAAAAbgJuDUrt2rWTYRiFrrdYLIqPj1d8fHz5FQUAAADgpuex9ygBAAAAgLsQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgImPuwuA+2RmZOjs2bPuLsMhICBANpvN3WUAAAAABKWb1VX7Je3/dr8S3psrf39/d5cjSaocHKBJz48mLAEAAMDtCEo3qayMq8rMtahSbGuFRVZzdzm6lHpeKQe2yW63E5QAAADgdgSlm1xgSCWFVA5zdxmSpPPuLgAAAAD4PzzMAQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACAiUcHpfj4eFksFqcpIiLC3WUBAAAAuMH5uLuAojRs2FBffPGFY97b29uN1QAAAAC4GXh8UPLx8WEUCQAAAEC58uhL7yTpyJEjioqKUo0aNfTII4/oxx9/vGb7jIwMpaWlOU0AAAAA4AqPDkotW7bUBx98oLVr1+pvf/ubkpKSFBcXp5SUlEK3mTFjhmw2m2OKjo4ux4oBAAAA3Ag8Oih169ZNffr00e23366OHTtq1apVkqQFCxYUus2ECROUmprqmE6dOlVe5QIAAAC4QXj8PUq/FRgYqNtvv11HjhwptI3VapXVai3HqgAAAADcaDx6RMksIyNDhw4dUmRkpLtLAQAAAHAD8+igNHbsWG3evFnHjh3Tv//9bz300ENKS0vToEGD3F0aAAAAgBuYR19699NPP+nRRx/VuXPnVLVqVd11113auXOnYmJi3F0aAAAAgBuYRwelpUuXursEAAAAADchj770DgAAAADcgaAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmHj0F87i5pKZkaGzZ8+6uwxJUkBAgGw2m7vLAADAI6Wmpsput7u7DEn8m42yQ1CCR7hqv6T93+5Xwntz5e/v7+5yVDk4QJOeH80vXgAATFJTU/VywltKSfeMoMS/2SgrBCV4hKyMq8rMtahSbGuFRVZzay2XUs8r5cA22e12fukCAGBit9uVkm5XaMO7FWQLdWst/JuNskRQgkcJDKmkkMph7i5D591dAAAAHi7IFsq/2bih8TAHAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATHzcXQAAlFRqaqrsdru7y5AkBQQEyGazubsMj+NJ50jiPME19N/rQ2ZGhs6ePevuMiRxjm40BCUA16XU1FS9nPCWUtI940NM5eAATXp+NP9A/oannSOJ84Tio/9eH67aL2n/t/uV8N5c+fv7u7scztENhqAE4Lpkt9uVkm5XaMO7FWQLdWstl1LPK+XANtntdv5x/A1POkcS5wmuof9eH7Iyrioz16JKsa0VFlnNrbVwjm48BCUA17UgW6hCKoe5uwydd3cBHsxTzpHEeYLr6L/Xh8CQSh5xnjhHNxYe5gAAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAICJj7sLAHBtqampstvt7i5DkhQQECCbzebuMoAbAj/bcFVmRobOnj3r7jJ09uxZZWVlursMj+Qp50jiPJUGghLgwVJTU/VywltKSfeMD1OVgwM06fnRfKACfid+tuGqq/ZL2v/tfiW8N1f+/v5urcV++ZIO/feoqrXKcGsdnsaTzpHEeSoNBCXAg9ntdqWk2xXa8G4F2ULdWsul1PNKObBNdrudD1PA78TPNlyVlXFVmbkWVYptrbDIam6tJenkUWUc+F7ZWdlurcPTeNI5kjhPpYGgBFwHgmyhCqkc5u4ydN7dBQA3GH624arAkEpu7zPpF8659fiezhPOkcR5Kg08zAEAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATH3cXAHiizIwMnT171t1l6OzZs8rKynR3GQ6e8r5IvDfXkpWVJV9fX3eX4XHnSPKc8+SJ7w0AwBlBCTC5ar+k/d/uV8J7c+Xv7+/WWuyXL+nQf4+qWqsMt9Yhedb7IvHeFCYzI0P//f6g6sU2lK9vBbfW4knnSPKs8+Rp7w0AID+CEmCSlXFVmbkWVYptrbDIam6tJenkUWUc+F7ZWdlurUPyrPdF4r0pTNLJo0r75lsF173LI2rxlHMked558qT3BgCQH0EJKERgSCWFVA5zaw3pF8659fgF8YT3ReK9KUze++JJtXga3hsAQHHwMAcAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwuS6C0uzZs1WjRg35+fnpzjvv1NatW91dEgAAAIAbmMcHpWXLlunZZ5/VpEmTtHfvXrVp00bdunXTyZMn3V0aAAAAgBuUxwelmTNn6sknn9TQoUPVoEEDzZo1S9HR0ZozZ467SwMAAABwg/JxdwHXkpmZqT179mj8+PFOyzt37qzt27cXuE1GRoYyMjIc86mpqZKktLS0siu0mNLT05WZmaGUpJ901X7ZrbVc+OWMcrKzdSH5Z3lb3FqKx9VDLZ5fi6fVQy2eX4un1eNJtVxOu6DLl9L1ww8/KD093b3FSDIMQxaL+ztMcnKy7PZLHvFvtuRZfYZaPL8WT6vnctoFZWZmKD09XYGBgW6tJS8TGIZRdGPDg/3888+GJOOrr75yWv7yyy8bdevWLXCbKVOmGJKYmJiYmJiYmJiYmJgKnE6dOlVkFvHoEaU85r8sGdf4a9OECRM0ZswYx3xubq7Onz+vypUru/0vVGlpaYqOjtapU6cUEhLi1lpwfaDPwFX0GbiKPgNX0WfgKk/qM4ZhKD09XVFRUUW29eigVKVKFXl7eyspKclpeXJyssLDwwvcxmq1ymq1Oi2rWLFiWZVYIiEhIW7vJLi+0GfgKvoMXEWfgavoM3CVp/QZm81WrHYe/TCHChUq6M4779T69eudlq9fv15xcXFuqgoAAADAjc6jR5QkacyYMRowYICaNWumVq1a6a9//atOnjypYcOGubs0AAAAADcojw9K/fr1U0pKiqZNm6YzZ87otttu0+rVqxUTE+Pu0lxmtVo1ZcqUfJcGAoWhz8BV9Bm4ij4DV9Fn4Krrtc9YDKM4z8YDAAAAgJuHR9+jBAAAAADuQFACAAAAABOCEgAAAACYEJQAAAAAwISgVMpmz56tGjVqyM/PT3feeae2bt16zfabN2/WnXfeKT8/P9WsWVN/+ctfyqlSeApX+szy5cvVqVMnVa1aVSEhIWrVqpXWrl1bjtXCE7j6eybPV199JR8fHzVp0qRsC4THcbXPZGRkaNKkSYqJiZHValWtWrU0b968cqoWnsDVPrNo0SI1btxYAQEBioyM1BNPPKGUlJRyqhbutmXLFvXo0UNRUVGyWCxasWJFkdtcD5+BCUqlaNmyZXr22Wc1adIk7d27V23atFG3bt108uTJAtsfO3ZM3bt3V5s2bbR3715NnDhRo0aN0scff1zOlcNdXO0zW7ZsUadOnbR69Wrt2bNH7du3V48ePbR3795yrhzu4mqfyZOamqqBAweqQ4cO5VQpPEVJ+kzfvn21YcMGzZ07V4cPH9aSJUtUv379cqwa7uRqn9m2bZsGDhyoJ598UgcOHNCHH36oXbt2aejQoeVcOdzl8uXLaty4sd59991itb9uPgMbKDUtWrQwhg0b5rSsfv36xvjx4wts//zzzxv169d3WvbUU08Zd911V5nVCM/iap8pSGxsrDF16tTSLg0eqqR9pl+/fsYLL7xgTJkyxWjcuHEZVghP42qf+fzzzw2bzWakpKSUR3nwQK72mddff92oWbOm07K3337bqFatWpnVCM8lyfjkk0+u2eZ6+QzMiFIpyczM1J49e9S5c2en5Z07d9b27dsL3GbHjh352nfp0kW7d+9WVlZWmdUKz1CSPmOWm5ur9PR0hYaGlkWJ8DAl7TOJiYn64YcfNGXKlLIuER6mJH1m5cqVatasmRISEnTLLbeobt26Gjt2rK5cuVIeJcPNStJn4uLi9NNPP2n16tUyDENnz57VRx99pPvuu688SsZ16Hr5DOzj7gJuFOfOnVNOTo7Cw8OdloeHhyspKanAbZKSkgpsn52drXPnzikyMrLM6oX7laTPmL355pu6fPmy+vbtWxYlwsOUpM8cOXJE48eP19atW+Xjw6/8m01J+syPP/6obdu2yc/PT5988onOnTun4cOH6/z589yndBMoSZ+Ji4vTokWL1K9fP129elXZ2dl64IEH9M4775RHybgOXS+fgRlRKmUWi8Vp3jCMfMuKal/Qcty4XO0zeZYsWaL4+HgtW7ZMYWFhZVUePFBx+0xOTo4ee+wxTZ06VXXr1i2v8uCBXPk9k5ubK4vFokWLFqlFixbq3r27Zs6cqfnz5zOqdBNxpc8cPHhQo0aN0osvvqg9e/ZozZo1OnbsmIYNG1YepeI6dT18BubPi6WkSpUq8vb2zvfXluTk5HyJOU9ERESB7X18fFS5cuUyqxWeoSR9Js+yZcv05JNP6sMPP1THjh3Lskx4EFf7THp6unbv3q29e/dq5MiRkn79EGwYhnx8fLRu3Trde++95VI73KMkv2ciIyN1yy23yGazOZY1aNBAhmHop59+Up06dcq0ZrhXSfrMjBkz1Lp1a40bN06S1KhRIwUGBqpNmzZ66aWXPGZ0AJ7jevkMzIhSKalQoYLuvPNOrV+/3mn5+vXrFRcXV+A2rVq1ytd+3bp1atasmXx9fcusVniGkvQZ6deRpMGDB2vx4sVc/32TcbXPhISE6Ntvv9W+ffsc07Bhw1SvXj3t27dPLVu2LK/S4SYl+T3TunVrnT59WpcuXXIs++9//ysvLy9Vq1atTOuF+5Wkz9jtdnl5OX+k9Pb2lvS/UQLgt66bz8BueojEDWnp0qWGr6+vMXfuXOPgwYPGs88+awQGBhrHjx83DMMwxo8fbwwYMMDR/scffzQCAgKM0aNHGwcPHjTmzp1r+Pr6Gh999JG7XgLKmat9ZvHixYaPj4/x3nvvGWfOnHFMFy9edNdLQDlztc+Y8dS7m4+rfSY9Pd2oVq2a8dBDDxkHDhwwNm/ebNSpU8cYOnSou14CypmrfSYxMdHw8fExZs+ebfzwww/Gtm3bjGbNmhktWrRw10tAOUtPTzf27t1r7N2715BkzJw509i7d69x4sQJwzCu38/ABKVS9t577xkxMTFGhQoVjDvuuMPYvHmzY92gQYOMtm3bOrXftGmT0bRpU6NChQpG9erVjTlz5pRzxXA3V/pM27ZtDUn5pkGDBpV/4XAbV3/P/BZB6ebkap85dOiQ0bFjR8Pf39+oVq2aMWbMGMNut5dz1XAnV/vM22+/bcTGxhr+/v5GZGSk8fjjjxs//fRTOVcNd/nyyy+v+fnkev0MbDEMxkQBAAAA4Le4RwkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQBwXYmPj1eTJk1+934sFotWrFhR6Prjx4/LYrFo3759kqRNmzbJYrHo4sWLkqT58+erYsWKv7sOAIBnIigBAMrM4MGDZbFYZLFY5Ovrq5o1a2rs2LG6fPmyu0srUnR0tM6cOaPbbrutwPX9+vXTf//7X8d8aQU4AIBn8HF3AQCAG1vXrl2VmJiorKwsbd26VUOHDtXly5c1Z84cp3ZZWVny9fV1U5X5eXt7KyIiotD1/v7+8vf3L8eKAADliRElAECZslqtioiIUHR0tB577DE9/vjjWrFihWMEZt68eapZs6asVqsMw9DJkyf14IMPKigoSCEhIerbt6/Onj2bb7/vv/++oqOjFRAQoIcffthxSZwk7dq1S506dVKVKlVks9nUtm1b/ec//8m3jzNnzqhbt27y9/dXjRo19OGHHzrWmS+9M/vtpXfz58/X1KlT9c033zhG0ObPn68hQ4bo/vvvd9ouOztbERERmjdvnutvJgCg3BCUAADlyt/fX1lZWZKko0eP6p///Kc+/vhjRyDp2bOnzp8/r82bN2v9+vX64Ycf1K9fP6d95G332Wefac2aNdq3b59GjBjhWJ+enq5BgwZp69at2rlzp+rUqaPu3bsrPT3daT+TJ09Wnz599M0336h///569NFHdejQIZdfU79+/fTcc8+pYcOGOnPmjM6cOaN+/fpp6NChWrNmjc6cOeNou3r1al26dEl9+/Z1+TgAgPLDpXcAgHLz9ddfa/HixerQoYMkKTMzUwsXLlTVqlUlSevXr9f+/ft17NgxRUdHS5IWLlyohg0bateuXWrevLkk6erVq1qwYIGqVasmSXrnnXd033336c0331RERITuvfdep+O+//77qlSpkjZv3uw0wvPwww9r6NChkqTp06dr/fr1eueddzR79myXXpe/v7+CgoLk4+PjdLleXFyc6tWrp4ULF+r555+XJCUmJurhhx9WUFCQS8cAAJQvRpQAAGXqX//6l4KCguTn56dWrVrpnnvu0TvvvCNJiomJcYQkSTp06JCio6MdIUmSYmNjVbFiRaeRnltvvdURkiSpVatWys3N1eHDhyVJycnJGjZsmOrWrSubzSabzaZLly7p5MmTTrW1atUq33xJRpSuZejQoUpMTHTUtWrVKg0ZMqRUjwEAKH2MKAEAylT79u01Z84c+fr6KioqyumBDYGBgU5tDcOQxWLJt4/ClufJW5f338GDB+uXX37RrFmzFBMTI6vVqlatWikzM7PIeq91nJIYOHCgxo8frx07dmjHjh2qXr262rRpU6rHAACUPkaUAABlKjAwULVr11ZMTEyRT7WLjY3VyZMnderUKceygwcPKjU1VQ0aNHAsO3nypE6fPu2Y37Fjh7y8vFS3bl1J0tatWzVq1Ch1795dDRs2lNVq1blz5/Idb+fOnfnm69evX6LXWaFCBeXk5ORbXrlyZfXs2VOJiYlKTEzUE088UaL9AwDKFyNKAACP0bFjRzVq1EiPP/64Zs2apezsbA0fPlxt27ZVs2bNHO38/Pw0aNAgvfHGG0pLS9OoUaPUt29fx/1BtWvX1sKFC9WsWTOlpaVp3LhxBT7K+8MPP1SzZs109913a9GiRfr66681d+7cEtVevXp1HTt2TPv27VO1atUUHBwsq9Uq6dfL7+6//37l5ORo0KBBJdo/AKB8MaIEAPAYFotFK1asUKVKlXTPPfeoY8eOqlmzppYtW+bUrnbt2urdu7e6d++uzp0767bbbnN6AMO8efN04cIFNW3aVAMGDNCoUaMUFhaW73hTp07V0qVL1ahRIy1YsECLFi1SbGxsiWrv06ePunbtqvbt26tq1apasmSJY13Hjh0VGRmpLl26KCoqqkT7BwCUL4thGIa7iwAA4EZmt9sVFRWlefPmqXfv3u4uBwBQDFx6BwBAGcnNzVVSUpLefPNN2Ww2PfDAA+4uCQBQTAQlAADKyMmTJ1WjRg1Vq1ZN8+fPl48P/+wCwPWCS+8AAAAAwISHOQAAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMPn/JIAE2thRfacAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAHUCAYAAAAEKdj3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLVklEQVR4nO3deXxOZ/7/8fedRTbJTUK2IiK1721sUSWttWiVlm7KqA5TZdAyVBFa1aoaVcu0/UkYY2unqtqqpdRW0VHDaNEWldISIchKFjm/P/rI/XWfJCRpkvvG6/l4nMfMfc51zvnc576k9zvXOVcshmEYAgAAAADYuDi6AAAAAABwNgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUADpWTk6MGDRro9ddft63bvXu3YmJidOnSJccVJmnhwoVasmRJuRzbYrEoJibG9nrx4sW64447lJGRUaz9Bw8eLIvFYlt8fHxUu3ZtPfjgg4qLi1NWVlaBfTp16qROnTqVqM7Dhw8rJiZGCQkJJdrPfK6EhARZLBbNnj27RMe5kddee01r164tsH7btm2yWCzatm1bmZ6vpPr27SuLxaLnn3/eoXVUhPPnz8vDw0MWi0XffvttoW0GDx6s2rVr262rXbu2Bg8eXKxzZGVlacGCBerYsaMCAgLk7u6ugIAAderUSe+++67S0tL+4LsAgP9DUALgUAsXLtTFixc1cuRI27rdu3dr2rRpt3RQMhs0aJB8fHw0a9asYu/j5eWl+Ph4xcfH67PPPtP06dPl4+OjZ599Vnfffbd+/fVXu/YLFy7UwoULS1TX4cOHNW3atBIHpdKcqzSKCkp33XWX4uPjddddd5V7DUVJSkrSZ599Jklavny5rly54rBaKsKyZcuUnZ0t6ffgX9bOnTunqKgojR07VvXr19d7772nrVu3avHixWrWrJnGjx+v5557rszPC+D2RVAC4DC5ubl68803NWTIEPn4+JT6OJcvXy7DqhzDzc1Nw4YN09tvv63MzMxi7ePi4qK2bduqbdu2io6O1tNPP62VK1dq/fr1+umnn/TII4/YtW/UqJEaNWpUHuXb5NdeEee6Hj8/P7Vt21Z+fn4Oq+Gf//yncnJy1LNnT126dElr1qwps2MXt49UpNjYWAUGBqpVq1ZauXJlmf+7fOqpp/Tdd99p8+bNeu+999SvXz916NBBffr00bx58/Tzzz+rW7du1z3G1atXCx1tBYDCEJQAlKmYmBhZLBbt379fffv2lZ+fn6xWq5566imdO3fOru26dev022+/aeDAgXb7jxs3TpIUHh5uu7Us/xaq2rVrq1evXlqzZo1atmwpT09PTZs2TZKUmJioYcOGqUaNGqpUqZLCw8M1bdo05ebm2p132rRpatOmjfz9/eXn56e77rpLixcvlmEYtja1a9fWoUOHtH37dlsN194ylJqaqhdffFHh4eGqVKmS7rjjDo0ePbrArXOpqal69tlnFRAQoMqVK6t79+766aefCr12Tz75pFJTU7Vq1aqSXXSTrl276tlnn9U333yjHTt22NYXduvdokWL1Lx5c1WuXFm+vr5q0KCBXnrpJUnSkiVL9Oijj0qSoqOjbdchf5StU6dOatKkiXbs2KGoqCh5e3tryJAhRZ5LkvLy8jRjxgzVqlVLnp6eioyM1JYtW+zaFHZ7lvR/fSufxWJRRkaGli5daqst/5xF3Xq3bt06tWvXTt7e3vL19VWXLl0UHx9f6HkOHTqkxx9/XFarVUFBQRoyZIhSUlIKveaFiY2NVVBQkJYuXSovLy/FxsYW2u6bb75R7969FRAQIE9PT0VERGj06NEF6vnvf/+rRx55RFWrVlVERIQk6cqVK5o4caJdPxwxYkSB0ditW7eqU6dOCggIkJeXl2rVqqV+/frZBa7r9YUb+eabb/T9999r4MCBevbZZ5WSkqKPPvqo2NfqRvbu3atNmzbpz3/+s+69995C2wQEBOipp56yvc6/3XPWrFl69dVXFR4eLg8PD3311VeSitcXitsXJdlusXz33XdVr149eXh4qFGjRgX+PWdmZtp+dnh6esrf31+RkZFauXJlaS4NgHLk5ugCANyaHn74YfXv31/Dhw/XoUOHNHnyZB0+fFjffPON3N3dJUmff/65AgMD7UYehg4dqgsXLuidd97RmjVrFBISIkl2bf773//qyJEjevnllxUeHi4fHx8lJiaqdevWcnFx0ZQpUxQREaH4+Hi9+uqrSkhIUFxcnG3/hIQEDRs2TLVq1ZIk7dmzRyNHjtRvv/2mKVOmSJI+/vhjPfLII7JarbZbyDw8PCT9/kWnY8eO+vXXX/XSSy+pWbNmOnTokKZMmaLvvvtOX375pSwWiwzDUJ8+fbR7925NmTJFrVq10tdff60ePXoUes2Cg4PVoEEDff7557bAUVoPPvigFi5cqB07dhT5xXLVqlV67rnnNHLkSM2ePVsuLi46duyYDh8+LEnq2bOnXnvtNb300ktasGCB7Ta2/C/pknTmzBk99dRTGj9+vF577TW5uFz/92/z589XWFiY5s6dq7y8PM2aNUs9evTQ9u3b1a5duxK9x/j4eN13332Kjo7W5MmTJem6I0grVqzQk08+qa5du2rlypXKysrSrFmz1KlTJ23ZskX33HOPXft+/fppwIABeuaZZ/Tdd99p4sSJklRk4LnW7t27deTIEY0bN04BAQHq16+fli9frhMnTig8PNzWbuPGjerdu7caNmyoOXPmqFatWkpISNCmTZsKHLNv37567LHHNHz4cGVkZNj615YtWzRx4kR16NBBBw8e1NSpU223ZHp4eCghIUE9e/ZUhw4dFBsbqypVqui3337Thg0blJ2dLW9v7xv2hRvJv9VuyJAhqlmzpkaPHq3FixfbBZc/YvPmzZJ+79clNW/ePNWrV0+zZ8+Wn5+f6tatW+K+UFzr1q3TV199ZbsNduHChXr88cfl5uZmG+EdO3asli1bpldffVUtW7ZURkaGvv/+eyUnJ5fqnADKkQEAZWjq1KmGJGPMmDF265cvX25IMv71r3/Z1jVs2NDo3r17gWO8+eabhiTjxIkTBbaFhYUZrq6uxo8//mi3ftiwYUblypWNX375xW797NmzDUnGoUOHCq336tWrRk5OjjF9+nQjICDAyMvLs21r3Lix0bFjxwL7zJw503BxcTH27t1rt/7f//63IclYv369YRiG8cUXXxiSjLffftuu3YwZMwxJxtSpUwsc+8knnzSCgoIKrfVagwYNMnx8fIrcfuTIEUOS8Ze//MW2rmPHjnbv5/nnnzeqVKly3fN8+OGHhiTjq6++KrCtY8eOhiRjy5YthW679lwnTpwwJBmhoaHG5cuXbetTU1MNf39/o3PnznbvLSwsrMAx8/vWtXx8fIxBgwYVaPvVV1/Z1X316lUjNDTUaNq0qXH16lVbu7S0NCMwMNCIiooqcJ5Zs2bZHfO5554zPD097fpIUYYMGWJIMo4cOWJXz+TJk+3aRUREGBEREXbXpKj3PWXKFLv1GzZsKLTO1atXG5KM9957zzCM/+uXBw4cKPIcxekLRcnIyDD8/PyMtm3b2tYNGjTIsFgsxrFjx+zaFvbZhoWFFfoZXmv48OGGJOOHH36wW5+Xl2fk5OTYltzcXNu2/D4XERFhZGdn29aXpC+UpC9KMry8vIzExETbutzcXKNBgwbGnXfeaVvXpEkTo0+fPtd9vwCcA7feASgXTz75pN3r/v37y83NzXbbiySdPn1agYGBJT52s2bNVK9ePbt1n332maKjoxUaGqrc3Fzbkj96s337dlvbrVu3qnPnzrJarXJ1dZW7u7umTJmi5ORkJSUl3fD8n332mZo0aaIWLVrYnatbt252t3vlv1fztXjiiSeKPHZgYKCSkpIK3C5YUsY1txEWpXXr1rp06ZIef/xxffLJJzp//nyJz1O1alXdd999xW7ft29feXp62l77+vqqd+/e2rFjh65evVri8xfXjz/+qNOnT2vgwIF2o16VK1dWv379tGfPngLP/ZhHL5o1a6YrV67csI+kp6frgw8+UFRUlBo0aCBJ6tixoyIiIrRkyRLl5eVJkn766ScdP35czzzzjN01KUq/fv3sXm/dulWSCswY9+ijj8rHx8d2S2OLFi1UqVIl/fnPf9bSpUv1888/Fzj2H+kLH3zwgVJTU+1GQYcMGSLDMOxGcsvDJ598Ind3d9titVoLtHnwwQdto9hS6fpCcd1///0KCgqyvXZ1ddWAAQN07Ngx2+QqrVu31hdffKEJEyZo27Ztt8QzlsCtiqAEoFwEBwfbvXZzc1NAQIDd7SWXL18u1hdEs/zb8a519uxZffrpp3Zfmtzd3dW4cWNJsn3x+89//qOuXbtKkt5//319/fXX2rt3ryZNmmSr6UbOnj2rgwcPFjiXr6+vDMOwnSs5Odn2vq9lvjbX8vT0lGEYf3iGtF9++UWSFBoaWmSbgQMHKjY2Vr/88ov69eunwMBAtWnTxnabU3EU9llcT2HvPTg4WNnZ2UpPTy/RsUoiv98VVm9oaKjy8vJ08eJFu/Xmzy3/1ssb9ZHVq1crPT1d/fv316VLl3Tp0iWlpKSof//+OnXqlO365j+zV6NGjWK9B3Pt+f2revXqdustFouCg4Nt7zkiIkJffvmlAgMDNWLECEVERCgiIkJvv/22bZ8/0hcWL14sT09Pde/e3fZ+mzVrptq1a2vJkiVlEoDzb5PN79f5OnXqpL1792rv3r3q1atXofsWdt0KWy8V3ReKq6j+fe15582bp7/97W9au3atoqOj5e/vrz59+ujo0aOlOieA8kNQAlAuEhMT7V7n5uYqOTnZ7stntWrVdOHChRIf2/wQdf6xunbtavvSZF6eeeYZSb8/l+Pu7q7PPvtM/fv3V1RUlCIjI0t0/mrVqqlp06ZFniv/eZmAgADb+76W+dpc68KFC/Lw8FDlypVLVJPZunXrJOmGfzfpT3/6k3bv3q2UlBR9/vnnMgxDvXr1KvCFtCiFfRbXU9h7T0xMVKVKlWzv2dPTs9CZyUoz4pUvv9+dOXOmwLbTp0/LxcVFVatWLfXxr5X/vM7o0aNVtWpV2zJz5ky77fkBxzyNe1HM1zq/f5knSTEMQ4mJiapWrZptXYcOHfTpp58qJSVFe/bsUbt27TR69Gi7iQZK0xd++ukn7dq1S1euXFGtWrXs3m9CQoJ+++03bdy4sVjv73q6dOki6f/6db4qVaooMjJSkZGRBYJtvsKum1S8vlDSvlhU/772vD4+Ppo2bZp++OEHJSYmatGiRdqzZ4969+5d6DEBOA5BCUC5WL58ud3rDz74QLm5uXZf3Bs0aKDjx48X2Le4v7m/Vq9evfT9998rIiLC9sXp2iV/ZMViscjNzU2urq62fS9fvqxly5YVWkdhNfTq1UvHjx9XQEBAoefKnyUrOjq60GuxYsWKIt/Hzz///Ien1d68ebP+3//7f4qKiir2Q+k+Pj7q0aOHJk2apOzsbB06dEhS6T6L61mzZo3daFlaWpo+/fRTdejQwfaZ1K5dW0lJSTp79qytXXZ2dqFfuIv6jMzq16+vO+64QytWrLC7LTEjI0MfffSRbfazP+rIkSOKj49Xv3799NVXXxVY7r//fn3yySdKTk5WvXr1FBERodjY2FJNWX3//fdLkv71r3/Zrf/oo4+UkZFh234tV1dXtWnTRgsWLJD0+8QoZkX1hcLkh77333+/wHtdv3693N3dizX5xY1ERkaqa9euev/997Vz584/dKyS9IWS9EVJ2rJli13bq1evavXq1YqIiCh05DAoKEiDBw/W448/rh9//NEpp30HbmfMegegXKxZs0Zubm7q0qWLbda75s2bq3///rY2nTp10vTp05WZmWn3JbVp06aSpLfffluDBg2Su7u76tevL19f3yLPN336dG3evFlRUVEaNWqU6tevrytXrighIUHr16/XP/7xD9WoUUM9e/bUnDlz9MQTT+jPf/6zkpOTNXv2bFsguFbTpk21atUqrV69WnXq1JGnp6eaNm2q0aNH66OPPtK9996rMWPGqFmzZsrLy9PJkye1adMmvfDCC2rTpo26du2qe++9V+PHj1dGRoYiIyP19ddfFxrKpN+nzv7Pf/5jG/26kby8PO3Zs0eSlJWVpZMnT+qLL77QBx98oIYNG+qDDz647v7PPvusvLy81L59e4WEhCgxMVEzZ86U1WpVq1atJElNmjSRJL333nvy9fWVp6enwsPDi/zt/Y24urqqS5cuGjt2rPLy8vTGG28oNTXVNsW7JA0YMEBTpkzRY489pnHjxunKlSuaN29eobdwNW3aVNu2bdOnn36qkJAQ+fr6qn79+gXaubi4aNasWXryySfVq1cvDRs2TFlZWXrzzTd16dIlvf7666V6P2b5wWH8+PFq3bp1ge1paWnasmWL/vWvf+mvf/2rFixYoN69e6tt27YaM2aMatWqpZMnT2rjxo0FArZZly5d1K1bN/3tb39Tamqq2rdvb5v1rmXLlrZp9//xj39o69at6tmzp2rVqqUrV67Ywkvnzp0lFa8vmOXm5uqf//ynGjZsqKFDhxbapnfv3lq3bp3OnTtX4BbBkvrXv/6lbt26qXPnzho8eLC6deumwMBApaam6uDBg/ryyy+L9XezStIXStIXpd9Hm++77z5NnjzZNuvdDz/8YDdy16ZNG/Xq1UvNmjVT1apVdeTIES1btqzMwjqAMuSwaSQA3JLyZ4Pat2+f0bt3b6Ny5cqGr6+v8fjjjxtnz561a3vs2DHDYrEYH3zwQYHjTJw40QgNDTVcXFzsZi8LCwszevbsWei5z507Z4waNcoIDw833N3dDX9/f+Puu+82Jk2aZKSnp9vaxcbGGvXr1zc8PDyMOnXqGDNnzjQWL15cYKa9hIQEo2vXroavr68hyW72q/T0dOPll1826tevb1SqVMmwWq1G06ZNjTFjxtjNenXp0iVjyJAhRpUqVQxvb2+jS5cuxg8//FDorHdbtmyxXbsbGTRokCHJtnh5eRm1atUyevfubcTGxhpZWVkF9jHPRLd06VIjOjraCAoKMipVqmSEhoYa/fv3Nw4ePGi339y5c43w8HDD1dXVkGTExcXZjte4ceNC6ytq1rs33njDmDZtmlGjRg2jUqVKRsuWLY2NGzcW2H/9+vVGixYtDC8vL6NOnTrG/PnzC51p7MCBA0b79u0Nb29vQ5LtnOZZ7/KtXbvWaNOmjeHp6Wn4+PgY999/v/H111/btck/z7lz5+zWx8XFFTkbo2EYRnZ2thEYGGi0aNGi0O2G8fssaDVq1DCaNm1qWxcfH2/06NHDsFqthoeHhxEREWE3a2RR9RiGYVy+fNn429/+ZoSFhRnu7u5GSEiI8Ze//MW4ePGi3fEffvhhIywszPDw8DACAgKMjh07GuvWrbO1KW5fuNbatWsNScbcuXOLbJM/M99bb71lGEbpZ73Ld+XKFeOdd94x7rnnHqNKlSqGm5ub4e/vb3To0MF44403jOTkZFvb/D735ptvFln/jfqCYRS/L0oyRowYYSxcuNCIiIgw3N3djQYNGhjLly+3azdhwgQjMjLSqFq1qu1n0JgxY4zz588X6xoAqDgWwyjG1EgAUEwxMTGaNm2azp07Z/eMRFF69+6t3NxcffHFFxVQnXMbOHCgfv75Z3399deOLgVACVksFo0YMULz5893dCkAygi33gFwqJkzZ6ply5bau3dvkbf43A6OHz+u1atX26Z8BgAAjsVkDgAcqkmTJoqLi7vuTHC3g5MnT2r+/PnFnnwBAACUL269AwAAAAATRpQAAAAAwISgBAAAAAAmBCUAAAAAMHHorHczZ87UmjVr9MMPP8jLy0tRUVF644037P5Y4ODBg7V06VK7/dq0aWP7I4s3kpeXp9OnT8vX11cWi6VM6wcAAABw8zAMQ2lpaQoNDZWLy/XHjBwalLZv364RI0aoVatWys3N1aRJk9S1a1cdPnxYPj4+tnbdu3dXXFyc7XWlSpWKfY7Tp0+rZs2aZVo3AAAAgJvXqVOnVKNGjeu2cWhQ2rBhg93ruLg4BQYGat++fbr33ntt6z08PBQcHFyqc/j6+kr6/WL4+fmVvlgAAAAAN7XU1FTVrFnTlhGux6n+4GxKSookyd/f3279tm3bFBgYqCpVqqhjx46aMWOGAgMDCz1GVlaWsrKybK/T0tIkSX5+fgQlAAAAAMV6JMdp/o6SYRh66KGHdPHiRe3cudO2fvXq1apcubLCwsJ04sQJTZ48Wbm5udq3b588PDwKHCcmJkbTpk0rsD4lJYWgBAAAANzGUlNTZbVai5UNnCYojRgxQp9//rl27dp13fsFz5w5o7CwMK1atUp9+/YtsN08opQ/vEZQAgAAAG5vJQlKTnHr3ciRI7Vu3Trt2LHjhg9VhYSEKCwsTEePHi10u4eHR6EjTQAAAABQXA4NSoZhaOTIkfr444+1bds2hYeH33Cf5ORknTp1SiEhIWVaR25urq5evVpmxwTKg6urq9zc3JjqHgAAoJw5NCiNGDFCK1as0CeffCJfX18lJiZKkqxWq7y8vJSenq6YmBj169dPISEhSkhI0EsvvaRq1arp4YcfLpMasrOzdebMGWVmZpbJ8YDy5u3trZCQkBJNkw8AAICScegzSkX9VjwuLk6DBw/W5cuX1adPH+3fv1+XLl1SSEiIoqOj9corrxT7byNd7z7EvLw8HT16VK6urqpevboqVarEb+rhtAzDUHZ2ts6dO6erV6+qbt26N/xDaQAAAPg/N80zSjfKaF5eXtq4cWO5nT87O1t5eXmqWbOmvL29y+08QFnx8vKSu7u7fvnlF2VnZ8vT09PRJQEAANyS+HW0xG/lcVOhvwIAAJQ/vnEBAAAAgAlBCQAAAABMnOLvKDmjlJSUCpsJz9vbW1artULOhfKzZMkSjR49WpcuXXJ0KQAAAPiDCEqFSElJ0YxZf1dyWsUEpQBfb00aP8ZpwtLevXs1YcIE7du3TxaLRa1atdKsWbPUokULSVJCQkKhf/Pqiy++UPfu3Su4Wud15swZvfDCC9q3b5+OHj2qUaNGae7cuQXaffTRR5o8ebKOHz+uiIgIzZgxo8ymvwcAAEDpEJQKkZmZqeS0TPk3vkeVrf7leq70lAtKPrRLmZmZThGU0tLS1K1bNz300ENauHChcnNzNXXqVHXr1k2//vqr3N3dbW2//PJLNW7c2Pba3798r9XNJisrS9WrV9ekSZP097//vdA28fHxGjBggF555RU9/PDD+vjjj9W/f3/t2rVLbdq0qeCKAQAAkI9nlK6jstVffgGB5bqUNogZhqFZs2apTp068vLyUvPmzfXvf/9bhmGoc+fO6t69u2369UuXLqlWrVqaNGnSDY/7448/6uLFi5o+fbrq16+vxo0ba+rUqUpKStLJkyft2gYEBCg4ONi2FPcPoMbExKhFixZatmyZateuLavVqscee0xpaWm2NllZWRo1apQCAwPl6empe+65R3v37i329Tl06JB69uwpPz8/+fr6qkOHDjp+/Lik3/9+1vTp01WjRg15eHioRYsW2rBhg23fhIQEWSwWrVmzRtHR0fL29lbz5s0VHx9vd44lS5aoVq1a8vb21sMPP6zk5GS77bVr19bbb7+tp59+usgQPHfuXHXp0kUTJ05UgwYNNHHiRN1///2FjjwBAACg4jCidJN6+eWXtWbNGi1atEh169bVjh079NRTT6l69epaunSpmjZtqnnz5umvf/2rhg8frqCgIMXExNzwuPXr11e1atW0ePFivfTSS7p69aoWL16sxo0bKywszK7tgw8+qCtXrqhu3boaM2aMHnnkkWLXf/z4ca1du1afffaZLl68qP79++v111/XjBkzJEnjx4/XRx99pKVLlyosLEyzZs1St27ddOzYsRuOXP3222+699571alTJ23dulV+fn76+uuvlZubK0l6++239dZbb+ndd99Vy5YtFRsbqwcffFCHDh1S3bp1bceZNGmSZs+erbp162rSpEl6/PHHdezYMbm5uembb77RkCFD9Nprr6lv377asGGDpk6dWuz3ny8+Pl5jxoyxW9etWzeCEgAA+MMq8pn7G7kZn8knKN2EMjIyNGfOHG3dulXt2rWTJNWpU0e7du3Su+++qxUrVujdd9/VwIEDdfbsWX366afav3+/3W1zRfH19dW2bdv00EMP6ZVXXpEk1atXTxs3bpSb2+/dpXLlypozZ47at28vFxcXrVu3TgMGDNDSpUv11FNPFes95OXlacmSJfL19ZUkDRw4UFu2bNGMGTOUkZGhRYsWacmSJerRo4ck6f3339fmzZu1ePFijRs37rrHXrBggaxWq1atWmV7z/Xq1bNtnz17tv72t7/psccekyS98cYb+uqrrzR37lwtWLDA1u7FF19Uz549JUnTpk1T48aNdezYMTVo0EBvv/22unXrpgkTJtiOv3v3bruRqeJITExUUFCQ3bqgoCAlJiaW6DgAAADXquhn7m/E2Z7JLw6C0k3o8OHDunLlirp06WK3Pjs7Wy1btpQkPfroo/r44481c+ZMLVq0yC4oXM/ly5c1ZMgQtW/fXitXrtTVq1c1e/ZsPfDAA9q7d6+8vLxUrVo1u1GQyMhIXbx4UbNmzSp2UKpdu7YtJElSSEiIkpKSJP0+2pSTk6P27dvbtru7u6t169Y6cuTIDY994MABdejQodBgmJqaqtOnT9sdW5Lat2+v//3vf3brmjVrZlefJCUlJalBgwY6cuRIgQkX2rVrV+KgJEkWi8XutWEYBdYBAACUREU+c38jzvZMfnERlG5CeXl5kqTPP/9cd9xxh902Dw8PSb//49i3b59cXV119OjRYh97xYoVSkhIUHx8vFxcXGzrqlatqk8++cQ2CmPWtm1b/b//9/+KfR5ziLFYLLb3lf9sVWkDhJeX1w3bFOfY19aYv81c4x8VHBxcYPQoKSmpwCgTAABAaeQ/c+9oFxxdQCkwmcNNqFGjRvLw8NDJkyd155132i01a9aUJL3wwgtycXHRF198oXnz5mnr1q3FOnZmZqZcXFzsQkP+6/yQUJj9+/fbRl3+qDvvvFOVKlXSrl27bOtycnL07bffqmHDhjfcv1mzZtq5c6dycnIKbPPz81NoaKjdsSVp9+7dxTp2vkaNGmnPnj1268yvi6Ndu3bavHmz3bpNmzYpKiqqxMcCAABA2WFE6TrSU8o/+5bmHL6+vnrxxRc1ZswY5eXl6Z577lFqaqp2796typUrq1q1aoqNjVV8fLzuuusuTZgwQYMGDdLBgwdVtWrV6x67S5cuGjdunEaMGKGRI0cqLy9Pr7/+utzc3BQdHS1JWrp0qdzd3dWyZUu5uLjo008/1bx58/TGG2+U6hqY+fj46C9/+YvGjRsnf39/1apVS7NmzVJmZqaeeeaZG+7//PPP65133tFjjz2miRMnymq1as+ePWrdurXq16+vcePGaerUqYqIiFCLFi0UFxenAwcOaPny5cWucdSoUYqKitKsWbPUp08fbdq0qdDb7g4cOCBJSk9P17lz53TgwAFVqlRJjRo1kiT99a9/1b333qs33nhDDz30kD755BN9+eWXBYIcAAAAKhZBqRDe3t4K8PVW8qFdFTJMGODrLW9v7xLt88orrygwMFAzZ87Uzz//rCpVquiuu+7SxIkTNWDAAMXExOiuu+6SJE2dOlWbNm3S8OHDtXr16uset0GDBvr00081bdo0tWvXTi4uLmrZsqU2bNhgN2L06quv6pdffpGrq6vq1aun2NjYYj+fVByvv/668vLyNHDgQKWlpSkyMlIbN268YdCTfp+2fOvWrRo3bpw6duwoV1dXtWjRwvZc0qhRo5SamqoXXnhBSUlJatSokdatW2c3492N5N9qOHXqVMXExKhz5856+eWXbRNg5Mt/ZkyS9u3bpxUrVigsLEwJCQmSpKioKK1atUovv/yyJk+erIiICK1evZq/oQQAAOBgFqOsHrZwUqmpqbJarUpJSZGfn5/dtitXrujEiRMKDw+Xp6en3baKnE7xZpwuEY5zvX4LAAAgSWfOnNHLM/+uWlEPOvwZpdTkJJ3cvU6vThxTZo9qlLqW62QDM0aUimC1WgkvAAAAwG2KyRxuM8OHD1flypULXYYPH14m52jcuHGR5yjJc0BFqYj3AAAAgNsbI0q3menTp+vFF18sdNuNhh+La/369YXOOCepTKa9roj3AAAAgNsbQek2ExgYqMDA8r1PNSwsrFyPXxHvAQAAALc3br1T2f3xUKAi0F8BAADK320dlNzd3SWpwma3A8pCfn/N778AAAAoe7f1rXeurq6qUqWKkpKSJP0+TbfFYnFwVUDhDMNQZmamkpKSVKVKFbm6ujq6JAAAgFvWbR2UJCk4OFiSbGEJcHZVqlSx9VsAAACUj9s+KFksFoWEhCgwMLDImdoAZ+Hu7s5IEgAAQAW47YNSPldXV76AAgAAAJB0m0/mAAAAAACFISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABM3Rxdwu0lJSVFmZqajy5AkeXt7y2q1OroMAAAAwOkQlCpQSkqKZsz6u5LTnCMoBfh6a9L4MYQlAAAAwISgVIEyMzOVnJYp/8b3qLLV36G1pKdcUPKhXcrMzCQoAQAAACYEJQeobPWXX0Cgo8vQBUcXAAAAADgpJnMAAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwcGpRmzpypVq1aydfXV4GBgerTp49+/PFHuzaGYSgmJkahoaHy8vJSp06ddOjQIQdVDAAAAOB24NCgtH37do0YMUJ79uzR5s2blZubq65duyojI8PWZtasWZozZ47mz5+vvXv3Kjg4WF26dFFaWpoDKwcAAABwK3Nz5Mk3bNhg9zouLk6BgYHat2+f7r33XhmGoblz52rSpEnq27evJGnp0qUKCgrSihUrNGzYMEeUDQAAAOAW51TPKKWkpEiS/P39JUknTpxQYmKiunbtamvj4eGhjh07avfu3YUeIysrS6mpqXYLAAAAAJSE0wQlwzA0duxY3XPPPWrSpIkkKTExUZIUFBRk1zYoKMi2zWzmzJmyWq22pWbNmuVbOAAAAIBbjtMEpeeff14HDx7UypUrC2yzWCx2rw3DKLAu38SJE5WSkmJbTp06VS71AgAAALh1OfQZpXwjR47UunXrtGPHDtWoUcO2Pjg4WNLvI0shISG29UlJSQVGmfJ5eHjIw8OjfAsGAAAAcEtz6IiSYRh6/vnntWbNGm3dulXh4eF228PDwxUcHKzNmzfb1mVnZ2v79u2Kioqq6HIBAAAA3CYcOqI0YsQIrVixQp988ol8fX1tzx1ZrVZ5eXnJYrFo9OjReu2111S3bl3VrVtXr732mry9vfXEE084snQAAAAAtzCHBqVFixZJkjp16mS3Pi4uToMHD5YkjR8/XpcvX9Zzzz2nixcvqk2bNtq0aZN8fX0ruFoAAAAAtwuHBiXDMG7YxmKxKCYmRjExMeVfEAAAAADIiWa9AwAAAABnQVACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAAThwalHTt2qHfv3goNDZXFYtHatWvttg8ePFgWi8Vuadu2rWOKBQAAAHDbcGhQysjIUPPmzTV//vwi23Tv3l1nzpyxLevXr6/ACgEAAADcjtwcefIePXqoR48e123j4eGh4ODgCqoIAAAAAG6CZ5S2bdumwMBA1atXT88++6ySkpKu2z4rK0upqal2CwAAAACUhFMHpR49emj58uXaunWr3nrrLe3du1f33XefsrKyitxn5syZslqttqVmzZoVWDEAAACAW4FDb727kQEDBtj+f5MmTRQZGamwsDB9/vnn6tu3b6H7TJw4UWPHjrW9Tk1NJSwBAAAAKBGnDkpmISEhCgsL09GjR4ts4+HhIQ8PjwqsCgAAAMCtxqlvvTNLTk7WqVOnFBIS4uhSAAAAANzCHDqilJ6ermPHjtlenzhxQgcOHJC/v7/8/f0VExOjfv36KSQkRAkJCXrppZdUrVo1Pfzwww6sGgAAAMCtzqFB6dtvv1V0dLTtdf6zRYMGDdKiRYv03Xff6Z///KcuXbqkkJAQRUdHa/Xq1fL19XVUyQAAAABuAw4NSp06dZJhGEVu37hxYwVWAwAAAAC/u6meUQIAAACAikBQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYlCoo1alTR8nJyQXWX7p0SXXq1PnDRQEAAACAI5UqKCUkJOjq1asF1mdlZem33377w0UBAAAAgCO5laTxunXrbP9/48aNslqtttdXr17Vli1bVLt27TIrDgAAAAAcoURBqU+fPpIki8WiQYMG2W1zd3dX7dq19dZbb5VZcQAAAADgCCUKSnl5eZKk8PBw7d27V9WqVSuXogAAAADAkUoUlPKdOHGirOsAAAAAAKdRqqAkSVu2bNGWLVuUlJRkG2nKFxsb+4cLAwAAAABHKVVQmjZtmqZPn67IyEiFhITIYrGUdV0AAAAA4DClCkr/+Mc/tGTJEg0cOLCs6wEAAAAAhyvV31HKzs5WVFRUWdcCAAAAAE6hVEFp6NChWrFiRVnXAgAAAABOoVS33l25ckXvvfeevvzySzVr1kzu7u522+fMmVMmxQEAAACAI5QqKB08eFAtWrSQJH3//fd225jYAQAAAMDNrlRB6auvvirrOgAAAADAaZTqGSUAAAAAuJWVakQpOjr6urfYbd26tdQFAQAAAICjlSoo5T+flC8nJ0cHDhzQ999/r0GDBpVFXQAAAADgMKUKSn//+98LXR8TE6P09PQ/VBAAAAAAOFqZPqP01FNPKTY2tiwPCQAAAAAVrkyDUnx8vDw9PcvykAAAAABQ4Up1613fvn3tXhuGoTNnzujbb7/V5MmTy6QwAAAAAHCUUgUlq9Vq99rFxUX169fX9OnT1bVr1zIpDAAAAAAcpVRBKS4urqzrAAAAAACnUaqglG/fvn06cuSILBaLGjVqpJYtW5ZVXQAAAADgMKUKSklJSXrssce0bds2ValSRYZhKCUlRdHR0Vq1apWqV69e1nUCAAAAQIUp1ax3I0eOVGpqqg4dOqQLFy7o4sWL+v7775WamqpRo0aVdY0AAAAAUKFKNaK0YcMGffnll2rYsKFtXaNGjbRgwQImcwAAAABw0yvViFJeXp7c3d0LrHd3d1deXt4fLgoAAAAAHKlUQem+++7TX//6V50+fdq27rffftOYMWN0//33l1lxAAAAAOAIpQpK8+fPV1pammrXrq2IiAjdeeedCg8PV1pamt55552yrhEAAAAAKlSpnlGqWbOm/vvf/2rz5s364YcfZBiGGjVqpM6dO5d1fQAAAABQ4Uo0orR161Y1atRIqampkqQuXbpo5MiRGjVqlFq1aqXGjRtr586d5VIoAAAAAFSUEgWluXPn6tlnn5Wfn1+BbVarVcOGDdOcOXPKrDgAAAAAcIQSBaX//e9/6t69e5Hbu3btqn379v3hogAAAADAkUoUlM6ePVvotOD53NzcdO7cuT9cFAAAAAA4UomC0h133KHvvvuuyO0HDx5USEjIHy4KAAAAABypREHpgQce0JQpU3TlypUC2y5fvqypU6eqV69eZVYcAAAAADhCiaYHf/nll7VmzRrVq1dPzz//vOrXry+LxaIjR45owYIFunr1qiZNmlRetQIAAABAhShRUAoKCtLu3bv1l7/8RRMnTpRhGJIki8Wibt26aeHChQoKCiqXQgEAAACgopT4D86GhYVp/fr1unjxoo4dOybDMFS3bl1VrVq1POoDAAAAgApX4qCUr2rVqmrVqlVZ1gIAAAAATqFEkzkAAAAAwO2AoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATBwalHbs2KHevXsrNDRUFotFa9eutdtuGIZiYmIUGhoqLy8vderUSYcOHXJMsQAAAABuGw4NShkZGWrevLnmz59f6PZZs2Zpzpw5mj9/vvbu3avg4GB16dJFaWlpFVwpAAAAgNuJmyNP3qNHD/Xo0aPQbYZhaO7cuZo0aZL69u0rSVq6dKmCgoK0YsUKDRs2rCJLBQAAAHAbcdpnlE6cOKHExER17drVts7Dw0MdO3bU7t27i9wvKytLqampdgsAAAAAlITTBqXExERJUlBQkN36oKAg27bCzJw5U1ar1bbUrFmzXOsEAAAAcOtx2qCUz2Kx2L02DKPAumtNnDhRKSkptuXUqVPlXSIAAACAW4xDn1G6nuDgYEm/jyyFhITY1iclJRUYZbqWh4eHPDw8yr0+AAAAALcupx1RCg8PV3BwsDZv3mxbl52dre3btysqKsqBlQEAAAC41Tl0RCk9PV3Hjh2zvT5x4oQOHDggf39/1apVS6NHj9Zrr72munXrqm7dunrttdfk7e2tJ554woFVAwAAALjVOTQoffvtt4qOjra9Hjt2rCRp0KBBWrJkicaPH6/Lly/rueee08WLF9WmTRtt2rRJvr6+jioZAAAAwG3AoUGpU6dOMgyjyO0Wi0UxMTGKiYmpuKIAAAAA3Pac9hklAAAAAHAUghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADBxc3QBAAAAwK0gJSVFmZmZji5DknT27Fnl5GQ7uoybGkEJAAAA+INSUlI0Y9bflZzmHEEpMyNdR346phrtshxdyk2LoAQAAAD8QZmZmUpOy5R/43tU2erv6HKUePKYsg79oNycXEeXctMiKAEAAABlpLLVX34BgY4uQ2kXzzu6hJsekzkAAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADBx6qAUExMji8VitwQHBzu6LAAAAAC3ODdHF3AjjRs31pdffml77erq6sBqAAAAANwOnD4oubm5MYoEAAAAoEI59a13knT06FGFhoYqPDxcjz32mH7++efrts/KylJqaqrdAgAAAAAl4dRBqU2bNvrnP/+pjRs36v3331diYqKioqKUnJxc5D4zZ86U1Wq1LTVr1qzAigEAAADcCpw6KPXo0UP9+vVT06ZN1blzZ33++eeSpKVLlxa5z8SJE5WSkmJbTp06VVHlAgAAALhFOP0zStfy8fFR06ZNdfTo0SLbeHh4yMPDowKrAgAAAHCrceoRJbOsrCwdOXJEISEhji4FAAAAwC3MqYPSiy++qO3bt+vEiRP65ptv9Mgjjyg1NVWDBg1ydGkAAAAAbmFOfevdr7/+qscff1znz59X9erV1bZtW+3Zs0dhYWGOLg0AAADALcypg9KqVascXQIAAACA25BT33oHAAAAAI5AUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMnPoPzgKOkpKSoszMTEeXIUny9vaW1Wp1dBkAADglZ/lv9tmzZ5WTk+3oMlCGCEqASUpKimbM+ruS0xz/Q1eSAny9NWn8GMISAAAmzvTf7MyMdB356ZhqtMtydCkoIwQlwCQzM1PJaZnyb3yPKlv9HVpLesoFJR/apczMTIISAAAmzvTf7MSTx5R16Afl5uQ6tA6UHYISUITKVn/5BQQ6ugxdcHQBAAA4OWf4b3baxfMOPT/KHpM5AAAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYOLm6AKAfCkpKcrMzHR0GTp79qxycrIdXYZTcpbPKJ+3t7esVqujy8BNxJn6MP23cHxGAJwFQQlOISUlRTNm/V3JaY7/j2NmRrqO/HRMNdplOboUp+JMn1G+AF9vTRo/hi8yKBZn68P034L4jAA4E4ISnEJmZqaS0zLl3/geVbb6O7SWxJPHlHXoB+Xm5Dq0DmfjTJ+RJKWnXFDyoV3KzMzkSwyKxZn6MP23cHxGAJwJQQlOpbLVX34BgQ6tIe3ieYee39k5w2eU74KjC8BNyVn6MP23aHxGAJwBkzkAAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABg4uboAuA42VlZOnv2rKPLkCSdPXtWOTnZji7DKTnL58RndPNISUlRZmamo8uQJHl7e8tqtTq6DOCWwL9toGIRlG5TVzLTdfC7g5q1YLG8vLwcXY4yM9J15KdjqtEuy9GlOBVn+pz4jG4OKSkpmjHr70pOc44vUwG+3po0fgxfqIA/iH/bQMUjKN2mcrKuKDvPoqqN2iswpIajy1HiyWPKOvSDcnNyHV2KU3Gmz4nP6OaQmZmp5LRM+Te+R5Wt/g6tJT3lgpIP7VJmZiZfpoA/iH/bQMUjKN3mfPyqyi8g0NFlKO3ieUeX4NSc4XPiM7q5VLb6O7zPSNIFRxcA3GL4tw1UHCZzAAAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwMTN0QUAAG5d2VlZOnv2rKPLkCSdPXtWOTnZji7DxpmuTU5Ojtzd3R1dBp/RdXBtCuds1wW3FoISAKBcXMlM18HvDmrWgsXy8vJydDnKzEjXkZ+OqUa7LEeX4lTXJjsrSz/9cFj1GzWWu3slh9bCZ1Q0rk3hnOm64NZDUAIAlIucrCvKzrOoaqP2Cgyp4ehylHjymLIO/aDcnFxHl+JU1ybx5DGl/u87+dZr6xS18BkVjmtTOGe6Lrj1EJQAAOXKx6+q/AICHV2G0i6ed3QJBTjDtcm/Ls5UizNxhusicW2K4ozXBbcOJnMAAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAk5siKC1cuFDh4eHy9PTU3XffrZ07dzq6JAAAAAC3MKcPSqtXr9bo0aM1adIk7d+/Xx06dFCPHj108uRJR5cGAAAA4Bbl9EFpzpw5euaZZzR06FA1bNhQc+fOVc2aNbVo0SJHlwYAAADgFuXm6AKuJzs7W/v27dOECRPs1nft2lW7d+8udJ+srCxlZWXZXqekpEiSUlNTy6/QYkpLS1N2dpaSE3/VlcwMh9Zy8dwZXc3N1cWk3+RqcWgpTlcPtTh/LZKUkXpRGelpOn78uNLS0hxdjgzDkMXi+AuTlJSkzMx0fs44eT3UQi03cz3U4vy1OFs9GakXlZ2dpbS0NPn4+Di0lvxMYBjGjRsbTuy3334zJBlff/213foZM2YY9erVK3SfqVOnGpJYWFhYWFhYWFhYWFgKXU6dOnXDLOLUI0r5zL+lNa7zm9uJEydq7Nixttd5eXm6cOGCAgICHP7b3tTUVNWsWVOnTp2Sn5+fQ2vBzYE+g5Kiz6Ck6DMoKfoMSsqZ+oxhGEpLS1NoaOgN2zp1UKpWrZpcXV2VmJhotz4pKUlBQUGF7uPh4SEPDw+7dVWqVCmvEkvFz8/P4Z0ENxf6DEqKPoOSos+gpOgzKCln6TNWq7VY7Zx6ModKlSrp7rvv1ubNm+3Wb968WVFRUQ6qCgAAAMCtzqlHlCRp7NixGjhwoCIjI9WuXTu99957OnnypIYPH+7o0gAAAADcopw+KA0YMEDJycmaPn26zpw5oyZNmmj9+vUKCwtzdGkl5uHhoalTpxa4NRAoCn0GJUWfQUnRZ1BS9BmU1M3aZyyGUZy58QAAAADg9uHUzygBAAAAgCMQlAAAAADAhKAEAAAAACYEJQAAAAAwISiVsYULFyo8PFyenp66++67tXPnzuu23759u+6++255enqqTp06+sc//lFBlcJZlKTPrFmzRl26dFH16tXl5+endu3aaePGjRVYLZxBSX/O5Pv666/l5uamFi1alG+BcDol7TNZWVmaNGmSwsLC5OHhoYiICMXGxlZQtXAGJe0zy5cvV/PmzeXt7a2QkBD96U9/UnJycgVVC0fasWOHevfurdDQUFksFq1du/aG+9ws338JSmVo9erVGj16tCZNmqT9+/erQ4cO6tGjh06ePFlo+xMnTuiBBx5Qhw4dtH//fr300ksaNWqUPvroowquHI5S0j6zY8cOdenSRevXr9e+ffsUHR2t3r17a//+/RVcORylpH0mX0pKip5++mndf//9FVQpnEVp+kz//v21ZcsWLV68WD/++KNWrlypBg0aVGDVcKSS9pldu3bp6aef1jPPPKNDhw7pww8/1N69ezV06NAKrhyOkJGRoebNm2v+/PnFan9Tff81UGZat25tDB8+3G5dgwYNjAkTJhTafvz48UaDBg3s1g0bNsxo27ZtudUI51LSPlOYRo0aGdOmTSvr0uCkSttnBgwYYLz88svG1KlTjebNm5djhXA2Je0zX3zxhWG1Wo3k5OSKKA9OqKR95s033zTq1Kljt27evHlGjRo1yq1GOCdJxscff3zdNjfT919GlMpIdna29u3bp65du9qt79q1q3bv3l3oPvHx8QXad+vWTd9++61ycnLKrVY4h9L0GbO8vDylpaXJ39+/PEqEkyltn4mLi9Px48c1derU8i4RTqY0fWbdunWKjIzUrFmzdMcdd6hevXp68cUXdfny5YooGQ5Wmj4TFRWlX3/9VevXr5dhGDp79qz+/e9/q2fPnhVRMm4yN9P3XzdHF3CrOH/+vK5evaqgoCC79UFBQUpMTCx0n8TExELb5+bm6vz58woJCSm3euF4pekzZm+99ZYyMjLUv3//8igRTqY0febo0aOaMGGCdu7cKTc3fuTfbkrTZ37++Wft2rVLnp6e+vjjj3X+/Hk999xzunDhAs8p3QZK02eioqK0fPlyDRgwQFeuXFFubq4efPBBvfPOOxVRMm4yN9P3X0aUypjFYrF7bRhGgXU3al/Yety6Stpn8q1cuVIxMTFavXq1AgMDy6s8OKHi9pmrV6/qiSee0LRp01SvXr2KKg9OqCQ/Z/Ly8mSxWLR8+XK1bt1aDzzwgObMmaMlS5YwqnQbKUmfOXz4sEaNGqUpU6Zo37592rBhg06cOKHhw4dXRKm4Cd0s33/59WIZqVatmlxdXQv8tiUpKalAas4XHBxcaHs3NzcFBASUW61wDqXpM/lWr16tZ555Rh9++KE6d+5cnmXCiZS0z6Slpenbb7/V/v379fzzz0v6/UuwYRhyc3PTpk2bdN9991VI7XCM0vycCQkJ0R133CGr1Wpb17BhQxmGoV9//VV169Yt15rhWKXpMzNnzlT79u01btw4SVKzZs3k4+OjDh066NVXX3WqEQI43s30/ZcRpTJSqVIl3X333dq8ebPd+s2bNysqKqrQfdq1a1eg/aZNmxQZGSl3d/dyqxXOoTR9Rvp9JGnw4MFasWIF93/fZkraZ/z8/PTdd9/pwIEDtmX48OGqX7++Dhw4oDZt2lRU6XCQ0vycad++vU6fPq309HTbup9++kkuLi6qUaNGudYLxytNn8nMzJSLi/1XSldXV0n/N1IA5Lupvv86aBKJW9KqVasMd3d3Y/Hixcbhw4eN0aNHGz4+PkZCQoJhGIYxYcIEY+DAgbb2P//8s+Ht7W2MGTPGOHz4sLF48WLD3d3d+Pe//+2ot4AKVtI+s2LFCsPNzc1YsGCBcebMGdty6dIlR70FVLCS9hkzZr27/ZS0z6SlpRk1atQwHnnkEePQoUPG9u3bjbp16xpDhw511FtABStpn4mLizPc3NyMhQsXGsePHzd27dplREZGGq1bt3bUW0AFSktLM/bv32/s37/fkGTMmTPH2L9/v/HLL78YhnFzf/8lKJWxBQsWGGFhYUalSpWMu+66y9i+fbtt26BBg4yOHTvatd+2bZvRsmVLo1KlSkbt2rWNRYsWVXDFcLSS9JmOHTsakgosgwYNqvjC4TAl/TlzLYLS7amkfebIkSNG586dDS8vL6NGjRrG2LFjjczMzAquGo5U0j4zb948o1GjRoaXl5cREhJiPPnkk8avv/5awVXDEb766qvrfje5mb//WgyDMVEAAAAAuBbPKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQBuKjExMWrRosUfPo7FYtHatWuL3J6QkCCLxaIDBw5IkrZt2yaLxaJLly5JkpYsWaIqVar84ToAAM6JoAQAKDeDBw+WxWKRxWKRu7u76tSpoxdffFEZGRmOLu2GatasqTNnzqhJkyaFbh8wYIB++ukn2+uyCnAAAOfg5ugCAAC3tu7duysuLk45OTnauXOnhg4dqoyMDC1atMiuXU5Ojtzd3R1UZUGurq4KDg4ucruXl5e8vLwqsCIAQEViRAkAUK48PDwUHBysmjVr6oknntCTTz6ptWvX2kZgYmNjVadOHXl4eMgwDJ08eVIPPfSQKleuLD8/P/Xv319nz54tcNx3331XNWvWlLe3tx599FHbLXGStHfvXnXp0kXVqlWT1WpVx44d9d///rfAMc6cOaMePXrIy8tL4eHh+vDDD23bzLfemV17692SJUs0bdo0/e9//7ONoC1ZskRDhgxRr1697PbLzc1VcHCwYmNjS34xAQAVhqAEAKhQXl5eysnJkSQdO3ZMH3zwgT766CNbIOnTp48uXLig7du3a/PmzTp+/LgGDBhgd4z8/T799FNt2LBBBw4c0IgRI2zb09LSNGjQIO3cuVN79uxR3bp19cADDygtLc3uOJMnT1a/fv30v//9T0899ZQef/xxHTlypMTvacCAAXrhhRfUuHFjnTlzRmfOnNGAAQM0dOhQbdiwQWfOnLG1Xb9+vdLT09W/f/8SnwcAUHG49Q4AUGH+85//aMWKFbr//vslSdnZ2Vq2bJmqV68uSdq8ebMOHjyoEydOqGbNmpKkZcuWqXHjxtq7d69atWolSbpy5YqWLl2qGjVqSJLeeecd9ezZU2+99ZaCg4N133332Z333XffVdWqVbV9+3a7EZ5HH31UQ4cOlSS98sor2rx5s9555x0tXLiwRO/Ly8tLlStXlpubm93telFRUapfv76WLVum8ePHS5Li4uL06KOPqnLlyiU6BwCgYjGiBAAoV5999pkqV64sT09PtWvXTvfee6/eeecdSVJYWJgtJEnSkSNHVLNmTVtIkqRGjRqpSpUqdiM9tWrVsoUkSWrXrp3y8vL0448/SpKSkpI0fPhw1atXT1arVVarVenp6Tp58qRdbe3atSvwujQjStczdOhQxcXF2er6/PPPNWTIkDI9BwCg7DGiBAAoV9HR0Vq0aJHc3d0VGhpqN2GDj4+PXVvDMGSxWAoco6j1+fK35f/v4MGDde7cOc2dO1dhYWHy8PBQu3btlJ2dfcN6r3ee0nj66ac1YcIExcfHKz4+XrVr11aHDh3K9BwAgLLHiBIAoFz5+PjozjvvVFhY2A1ntWvUqJFOnjypU6dO2dYdPnxYKSkpatiwoW3dyZMndfr0advr+Ph4ubi4qF69epKknTt3atSoUXrggQfUuHFjeXh46Pz58wXOt2fPngKvGzRoUKr3WalSJV29erXA+oCAAPXp00dxcXGKi4vTn/70p1IdHwBQsRhRAgA4jc6dO6tZs2Z68sknNXfuXOXm5uq5555Tx44dFRkZaWvn6empQYMGafbs2UpNTdWoUaPUv39/2/NBd955p5YtW6bIyEilpqZq3LhxhU7l/eGHHyoyMlL33HOPli9frv/85z9avHhxqWqvXbu2Tpw4oQMHDqhGjRry9fWVh4eHpN9vv+vVq5euXr2qQYMGler4AICKxYgSAMBpWCwWrV27VlWrVtW9996rzp07q06dOlq9erVduzvvvFN9+/bVAw88oK5du6pJkyZ2EzDExsbq4sWLatmypQYOHKhRo0YpMDCwwPmmTZumVatWqVmzZlq6dKmWL1+uRo0alar2fv36qXv37oqOjlb16tW1cuVK27bOnTsrJCRE3bp1U2hoaKmODwCoWBbDMAxHFwEAwK0sMzNToaGhio2NVd++fR1dDgCgGLj1DgCAcpKXl6fExES99dZbslqtevDBBx1dEgCgmAhKAACUk5MnTyo8PFw1atTQkiVL5ObGf3YB4GbBrXcAAAAAYMJkDgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATP4/QwmfdyonlvsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images sorted by p(treated) in ascending order:\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B08-T01.tiff: 0.0003\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B11-T01.tiff: 0.0005\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B07-T01.tiff: 0.0007\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_F06-T01.tiff: 0.0011\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C8.tif: 0.0011\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C7.tif: 0.0020\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C02-T01.tiff: 0.0032\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_F03-T01.tiff: 0.0052\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C07-T01.tiff: 0.0062\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C11-T01.tiff: 0.0070\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_F2.tif: 0.0077\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C11.tif: 0.0077\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_B10-T01.tiff: 0.0080\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_F04-T01.tiff: 0.0108\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B10-T01.tiff: 0.0108\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C9.tif: 0.0109\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C03-T01.tiff: 0.0126\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C05-T01.tiff: 0.0141\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C08-T01.tiff: 0.0215\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C06-T01.tiff: 0.0248\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C04-T01.tiff: 0.0300\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_c10.tif: 0.0308\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_F05-T01.tiff: 0.0519\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C09-T01.tiff: 0.0854\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_B11-T01.tiff: 0.1111\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C2.tif: 0.1264\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C08-T01.tiff: 0.1362\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_B08-T01.tiff: 0.1605\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C10-T01.tiff: 0.1618\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_B07-T01.tiff: 0.1881\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C10-T01.tiff: 0.1924\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_D03-T01.tiff: 0.2152\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C03-T01.tiff: 0.2311\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C04-T01.tiff: 0.2556\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_B09-T01.tiff: 0.2595\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B09-T01.tiff: 0.2754\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_G05-T01.tiff: 0.4117\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C03-T01.tiff: 0.4365\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C07-T01.tiff: 0.4701\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C11-T01.tiff: 0.5041\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C05-T01.tiff: 0.5302\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C6.tif: 0.5375\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C04-T01.tiff: 0.5716\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C05-T01.tiff: 0.6237\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C02-T01.tiff: 0.6842\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_D04-T01.tiff: 0.6885\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_G06-T01.tiff: 0.6922\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_C06-T01.tiff: 0.7293\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_G02-T01.tiff: 0.7658\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C09-T01.tiff: 0.7878\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C09-T01.tiff: 0.8010\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C06-T01.tiff: 0.8580\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_G03-T01.tiff: 0.8722\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C07-T01.tiff: 0.8724\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C03-T01.tiff: 0.8867\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C04-T01.tiff: 0.9136\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C07-T01.tiff: 0.9167\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_E05-T01.tiff: 0.9170\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_b2.tif: 0.9286\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_D03-T01.tiff: 0.9302\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_E2.tif: 0.9383\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_G03-T01.tiff: 0.9600\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_D02-T01.tiff: 0.9654\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_G06-T01.tiff: 0.9768\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_D06-T01.tiff: 0.9849\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_E06-T01.tiff: 0.9871\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_D05-T01.tiff: 0.9878\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C08-T01.tiff: 0.9879\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_1b_C04-T01.tiff: 0.9881\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_D03-T01.tiff: 0.9884\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C08-T01.tiff: 0.9906\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_G04-T01.tiff: 0.9912\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C11-T01.tiff: 0.9914\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_F03-T01.tiff: 0.9925\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C05-T01.tiff: 0.9946\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C10-T01.tiff: 0.9972\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_41_C09-T01.tiff: 0.9978\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C02-T01.tiff: 0.9990\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B04-T01.tiff: 0.9997\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_42_C11-T01.tiff: 0.9998\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B03-T01.tiff: 0.9998\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_E04-T01.tiff: 0.9998\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_62_C06-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_B05-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inf_ex_85\\ex_85_no_cond10\\ds_61_E03-T01.tiff: 1.0000\n"
     ]
    }
   ],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending ordergroups_data[group_name]\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images copied and renamed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Assuming 'sorted_by_treated' is already defined as:\n",
    "# sorted_by_treated = [(img_path, p_tr), ...] sorted in ascending order by p_tr\n",
    "\n",
    "new_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\sort_ex_85\"\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "for i, (img_path, p_tr) in enumerate(sorted_by_treated, start=1):\n",
    "    # Extract the parent directory name and the original filename\n",
    "    parent_dir_name = os.path.basename(os.path.dirname(img_path))  # e.g. \"cond7\"\n",
    "    base_filename = os.path.basename(img_path)  # e.g. \"ds_41.tif\"\n",
    "    \n",
    "    # Combine them to get \"cond7_ds_41.tif\"\n",
    "    original_filename = f\"{parent_dir_name}_{base_filename}\"\n",
    "\n",
    "    # Format the probability score\n",
    "    formatted_score = f\"{p_tr:.4f}\"\n",
    "    \n",
    "    # Construct the new filename with index and score\n",
    "    new_filename = f\"{i}_{formatted_score}_{original_filename}\"\n",
    "    new_path = os.path.join(new_dir, new_filename)\n",
    "    \n",
    "    shutil.copy2(img_path, new_path)\n",
    "\n",
    "print(\"Images copied and renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 18/18 [00:14<00:00,  1.20it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images sorted by p(treated) in ascending order:\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B07-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C08-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G04-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C03-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F09-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B11-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C02-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D05-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E06-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C10-T01.tiff: 0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G11-T01.tiff: 0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G10-T01.tiff: 0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E08-T01.tiff: 0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G03-T01.tiff: 0.0002\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B08-T01.tiff: 0.0003\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B11-T01.tiff: 0.0005\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B07-T01.tiff: 0.0007\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F06-T01.tiff: 0.0011\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C8.tif: 0.0011\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C7.tif: 0.0020\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C02-T01.tiff: 0.0032\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F02-T01.tiff: 0.0050\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F03-T01.tiff: 0.0052\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C07-T01.tiff: 0.0062\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C11-T01.tiff: 0.0070\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C11.tif: 0.0077\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B10-T01.tiff: 0.0080\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F04-T01.tiff: 0.0108\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B10-T01.tiff: 0.0108\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C9.tif: 0.0109\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C03-T01.tiff: 0.0126\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C05-T01.tiff: 0.0141\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C08-T01.tiff: 0.0215\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C06-T01.tiff: 0.0248\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C04-T01.tiff: 0.0300\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_c10.tif: 0.0308\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F05-T01.tiff: 0.0519\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C09-T01.tiff: 0.0854\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B11-T01.tiff: 0.1111\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C08-T01.tiff: 0.1362\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C10-T01.tiff: 0.1618\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B07-T01.tiff: 0.1881\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C10-T01.tiff: 0.1923\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C04-T01.tiff: 0.2556\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B09-T01.tiff: 0.2595\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B09-T01.tiff: 0.2754\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C03-T01.tiff: 0.4365\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C11-T01.tiff: 0.5039\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C05-T01.tiff: 0.5302\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C02-T01.tiff: 0.7032\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C06-T01.tiff: 0.7293\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C09-T01.tiff: 0.7878\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C07-T01.tiff: 0.8724\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g9_C10-T01.tiff: 0.9972\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B05-T01.tiff: 0.9981\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E5.tif: 0.9981\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B06-T01.tiff: 0.9983\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E3.tif: 0.9985\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E2.tif: 0.9986\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B04-T01.tiff: 0.9992\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E04-T01.tiff: 0.9993\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B04-T01.tiff: 0.9996\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E4.tif: 0.9997\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B06-T01.tiff: 0.9998\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B11-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B03-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F08-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E10-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D09-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C07-T01.tiff: 0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E02-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E05-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B03-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E03-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E04-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E06-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B04-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E06-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E05-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B02-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B03-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D08-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E11-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G07-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G09-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G10-T01.tiff: 1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D10-T01.tiff: 1.0000\n"
     ]
    }
   ],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending ordergroups_data[group_name]\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images sorted by p(treated) in ascending order:\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B07-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_F02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_D05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_B11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C08-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G04-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C03-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_F09-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_B11-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_C02-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_D05-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_E06-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_C10-T01.tiff: p(treated)=0.0000, p(control)=1.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBTDS1B_Day_7_G11-T01.tiff: p(treated)=0.0001, p(control)=0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G10-T01.tiff: p(treated)=0.0001, p(control)=0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_E08-T01.tiff: p(treated)=0.0001, p(control)=0.9999\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\cond7\\RBT_01.04_Day7_G03-T01.tiff: p(treated)=0.0002, p(control)=0.9998\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B08-T01.tiff: p(treated)=0.0003, p(control)=0.9997\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B11-T01.tiff: p(treated)=0.0005, p(control)=0.9995\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B07-T01.tiff: p(treated)=0.0007, p(control)=0.9993\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F06-T01.tiff: p(treated)=0.0011, p(control)=0.9989\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C8.tif: p(treated)=0.0011, p(control)=0.9989\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C7.tif: p(treated)=0.0020, p(control)=0.9980\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C02-T01.tiff: p(treated)=0.0032, p(control)=0.9968\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F02-T01.tiff: p(treated)=0.0050, p(control)=0.9950\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F03-T01.tiff: p(treated)=0.0052, p(control)=0.9948\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C07-T01.tiff: p(treated)=0.0062, p(control)=0.9938\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C11-T01.tiff: p(treated)=0.0070, p(control)=0.9930\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C11.tif: p(treated)=0.0077, p(control)=0.9923\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B10-T01.tiff: p(treated)=0.0080, p(control)=0.9920\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F04-T01.tiff: p(treated)=0.0108, p(control)=0.9892\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B10-T01.tiff: p(treated)=0.0108, p(control)=0.9892\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_C9.tif: p(treated)=0.0109, p(control)=0.9891\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C03-T01.tiff: p(treated)=0.0126, p(control)=0.9874\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C05-T01.tiff: p(treated)=0.0141, p(control)=0.9859\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C08-T01.tiff: p(treated)=0.0215, p(control)=0.9785\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C06-T01.tiff: p(treated)=0.0248, p(control)=0.9752\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g3_C04-T01.tiff: p(treated)=0.0300, p(control)=0.9700\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds1B_g9_c10.tif: p(treated)=0.0308, p(control)=0.9692\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g6_F05-T01.tiff: p(treated)=0.0519, p(control)=0.9481\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C09-T01.tiff: p(treated)=0.0854, p(control)=0.9146\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B11-T01.tiff: p(treated)=0.1111, p(control)=0.8889\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C08-T01.tiff: p(treated)=0.1362, p(control)=0.8638\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g9_C10-T01.tiff: p(treated)=0.1618, p(control)=0.8382\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B07-T01.tiff: p(treated)=0.1881, p(control)=0.8119\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C10-T01.tiff: p(treated)=0.1923, p(control)=0.8077\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C04-T01.tiff: p(treated)=0.2556, p(control)=0.7444\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g8_B09-T01.tiff: p(treated)=0.2595, p(control)=0.7405\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g8_B09-T01.tiff: p(treated)=0.2754, p(control)=0.7246\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C03-T01.tiff: p(treated)=0.4365, p(control)=0.5635\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C11-T01.tiff: p(treated)=0.5039, p(control)=0.4961\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C05-T01.tiff: p(treated)=0.5302, p(control)=0.4698\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C02-T01.tiff: p(treated)=0.7032, p(control)=0.2968\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds61_g3_C06-T01.tiff: p(treated)=0.7293, p(control)=0.2707\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C09-T01.tiff: p(treated)=0.7878, p(control)=0.2122\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds62_g9_C07-T01.tiff: p(treated)=0.8724, p(control)=0.1276\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\ex_40\\ds41_g9_C10-T01.tiff: p(treated)=0.9972, p(control)=0.0028\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B05-T01.tiff: p(treated)=0.9981, p(control)=0.0019\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E5.tif: p(treated)=0.9981, p(control)=0.0019\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B06-T01.tiff: p(treated)=0.9983, p(control)=0.0017\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E3.tif: p(treated)=0.9985, p(control)=0.0015\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E2.tif: p(treated)=0.9986, p(control)=0.0014\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B04-T01.tiff: p(treated)=0.9992, p(control)=0.0008\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E04-T01.tiff: p(treated)=0.9993, p(control)=0.0007\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B04-T01.tiff: p(treated)=0.9996, p(control)=0.0004\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds1b_g5_E4.tif: p(treated)=0.9997, p(control)=0.0003\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B06-T01.tiff: p(treated)=0.9998, p(control)=0.0002\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B11-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g2_B03-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F08-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E10-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D09-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C07-T01.tiff: p(treated)=0.9999, p(control)=0.0001\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E02-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E05-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g2_B03-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E03-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E04-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E06-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_E07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B04-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds41_g5_E06-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds42_g5_E05-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B02-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_C11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_F10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_G10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_B10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_C08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_B08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_F09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_C09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\ds62_g2_B03-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_B10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_D11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_E11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_F11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.04_Day10_G11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_B09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.09_Day10_D08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_C10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_D08-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_E11-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G07-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G09-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.10_Day10_G10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference_for_accuracy\\single_dose\\sd_RBT_01.14_Day10_D10-T01.tiff: p(treated)=1.0000, p(control)=0.0000\n",
      "\n",
      "Group-Wise Ranking Accuracy:\n",
      "Correct Transitions: 2\n",
      "Total Possible Transitions: 2\n",
      "Ranking Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1], entry[0]) for entry in all_images_data]\n",
    "\n",
    "# Print sorted images\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr, p_ctrl in sorted_by_treated:\n",
    "    print(f\"{img_path}: p(treated)={p_tr:.4f}, p(control)={p_ctrl:.4f}\")\n",
    "\n",
    "# Initialize group-wise data\n",
    "grouped_data = {group: [] for group in groups}\n",
    "for group in groups:\n",
    "    grouped_data[group].extend(groups_data[group])\n",
    "\n",
    "# Step 1: Sort distances and keep track of group membership\n",
    "sorted_distances = []\n",
    "for group, data in grouped_data.items():\n",
    "    for _, p_treated, _ in data:\n",
    "        sorted_distances.append((p_treated, group))\n",
    "\n",
    "sorted_distances.sort(key=lambda x: x[0])  # Sort by p(treated)\n",
    "\n",
    "# Step 2: Check for correct transitions between groups\n",
    "correct_transitions = 0\n",
    "total_transitions = len(groups) - 1  # Total possible adjacent group transitions\n",
    "\n",
    "for i in range(total_transitions):\n",
    "    group_i = groups[i]\n",
    "    group_j = groups[i + 1]\n",
    "\n",
    "    # Get all distances for groups i and j\n",
    "    distances_i = [dist for dist, grp in sorted_distances if grp == group_i]\n",
    "    distances_j = [dist for dist, grp in sorted_distances if grp == group_j]\n",
    "\n",
    "    # Check the condition: all d in G_i < all d in G_j\n",
    "    if all(d_i < d_j for d_i in distances_i for d_j in distances_j):\n",
    "        correct_transitions += 1\n",
    "\n",
    "# Step 3: Calculate ranking accuracy\n",
    "ranking_accuracy = correct_transitions / total_transitions if total_transitions > 0 else 1.0\n",
    "\n",
    "# Step 4: Print the group-wise ranking accuracy\n",
    "print(\"\\nGroup-Wise Ranking Accuracy:\")\n",
    "print(f\"Correct Transitions: {correct_transitions}\")\n",
    "print(f\"Total Possible Transitions: {total_transitions}\")\n",
    "print(f\"Ranking Accuracy: {ranking_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Assuming 'sorted_by_treated' is already defined as:\n",
    "# sorted_by_treated = [(img_path, p_tr), ...] sorted in ascending order by p_tr\n",
    "\n",
    "new_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\sorted_explodall\"\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "for i, (img_path, p_tr) in enumerate(sorted_by_treated, start=1):\n",
    "    # Extract the parent directory name and the original filename\n",
    "    parent_dir_name = os.path.basename(os.path.dirname(img_path))  # e.g. \"cond7\"\n",
    "    base_filename = os.path.basename(img_path)  # e.g. \"ds_41.tif\"\n",
    "    \n",
    "    # Combine them to get \"cond7_ds_41.tif\"\n",
    "    original_filename = f\"{parent_dir_name}_{base_filename}\"\n",
    "\n",
    "    # Format the probability score\n",
    "    formatted_score = f\"{p_tr:.4f}\"\n",
    "    \n",
    "    # Construct the new filename with index and score\n",
    "    new_filename = f\"{i}_{formatted_score}_{original_filename}\"\n",
    "    new_path = os.path.join(new_dir, new_filename)\n",
    "    \n",
    "    shutil.copy2(img_path, new_path)\n",
    "\n",
    "print(\"Images copied and renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Assuming 'sorted_by_treated' is already defined as:\n",
    "# sorted_by_treated = [(img_path, p_tr), ...] sorted in ascending order by p_tr\n",
    "\n",
    "new_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\sorted_cond10\"\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "for i, (img_path, p_tr) in enumerate(sorted_by_treated, start=1):\n",
    "    # Extract the parent directory name and the original filename\n",
    "    parent_dir_name = os.path.basename(os.path.dirname(img_path))  # e.g. \"cond7\"\n",
    "    base_filename = os.path.basename(img_path)  # e.g. \"ds_41.tif\"\n",
    "    \n",
    "    # Combine them to get \"cond7_ds_41.tif\"\n",
    "    original_filename = f\"{parent_dir_name}_{base_filename}\"\n",
    "\n",
    "    # Format the probability score\n",
    "    formatted_score = f\"{p_tr:.4f}\"\n",
    "    \n",
    "    # Construct the new filename with index and score\n",
    "    new_filename = f\"{i}_{formatted_score}_{original_filename}\"\n",
    "    new_path = os.path.join(new_dir, new_filename)\n",
    "    \n",
    "    shutil.copy2(img_path, new_path)\n",
    "\n",
    "print(\"Images copied and renamed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\2_class\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#500 epochs, one outlier of exploder in ds closer to sd: which is ds41_g9_C10-T01   inbetween 4 ds closer\n",
    "# 100 epochs, one outlier of exploder also one outlier from ds_clos_tosd ds41_g5_E04-T01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "#from helper import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression model definition\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Recreate the model structure\n",
    "feature_dim = 512 # Set this to the same dimension used during training\n",
    "num_classes = 2   # Since you trained for 2 classes\n",
    "logreg_model = LogisticRegression(feature_dim, num_classes).to(device)\n",
    "\n",
    "# Load the saved weights\n",
    "logreg_model.load_state_dict(torch.load(\"best_loss_model.pth\", map_location=device))\n",
    "logreg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "from torchvision import transforms\n",
    "\n",
    "# Assume simclr_model, logreg_model, device are already defined and loaded.\n",
    "# simclr_model: the feature extractor model\n",
    "# logreg_model: the logistic regression model for control vs treated classification.\n",
    "\n",
    "def get_all_groups(root_dir):\n",
    "    # All directories in root_dir are considered different groups\n",
    "    groups = []\n",
    "    for d in os.listdir(root_dir):\n",
    "        dir_path = os.path.join(root_dir, d)\n",
    "        if os.path.isdir(dir_path):\n",
    "            groups.append(d)\n",
    "    return groups\n",
    "\n",
    "def load_inference_data(root_dir):\n",
    "    groups = get_all_groups(root_dir)\n",
    "    \n",
    "    image_files = []\n",
    "    group_names = []\n",
    "    \n",
    "    # We won't use numeric labels for correctness here since our model is only binary.\n",
    "    # However, we still need a label tensor. We'll just assign a dummy label (e.g. 0) \n",
    "    # because we only need to run inference. The label doesn't affect inference.\n",
    "    dummy_labels = []\n",
    "\n",
    "    for group in groups:\n",
    "        class_dir = os.path.join(root_dir, group)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir)\n",
    "                 if file.lower().endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        group_names.extend([group] * len(files))\n",
    "        dummy_labels.extend([0]*len(files))  # dummy label\n",
    "\n",
    "    return image_files, dummy_labels, group_names, groups\n",
    "\n",
    "# Dataset that returns image, a dummy label, path, and keeps track of group name externally\n",
    "class LabeledImageDatasetWithPath:\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Handle shape: ensure CxHxW\n",
    "        if image.ndim == 3 and image.shape[2] == 3:\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} doesn't have 3 channels.\")\n",
    "\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label, img_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_features(model, dataloader, device):\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    for batch_imgs, batch_labels, batch_paths in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        all_features.append(batch_feats.cpu())\n",
    "        all_labels.append(batch_labels)\n",
    "        all_paths.extend(batch_paths)\n",
    "    all_features = torch.cat(all_features, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    return all_features, all_labels, all_paths\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_probs(model, features, device):\n",
    "    model.eval()\n",
    "    features = features.to(device)\n",
    "    logits = model(features)  # shape: [N, 2]\n",
    "    probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    return probs.cpu().numpy()\n",
    "\n",
    "# --------------------\n",
    "# MAIN INFERENCE FLOW\n",
    "# --------------------\n",
    "\n",
    "root_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\softmax_dataset\\inference\"\n",
    "\n",
    "# 1. Load data from all groups\n",
    "image_files, dummy_labels, group_names, groups = load_inference_data(root_dir)\n",
    "\n",
    "# 2. Create dataset and dataloader for inference\n",
    "inference_dataset = LabeledImageDatasetWithPath(image_files, dummy_labels)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "# 3. Extract features\n",
    "inference_feats, inference_true_labels, inference_paths = extract_features(simclr_model, inference_loader, device)\n",
    "\n",
    "# 4. Get probabilities from logistic regression model\n",
    "probs = predict_with_probs(logreg_model, inference_feats, device)  # shape [N, 2]\n",
    "# probs[:, 0] = probability of control\n",
    "# probs[:, 1] = probability of treated\n",
    "\n",
    "# Let's store results in a dictionary keyed by group name\n",
    "# groups_data[group_name] = list of [p_control, p_treated, img_path]\n",
    "groups_data = {g: [] for g in groups}\n",
    "\n",
    "# Since group_names aligns with inference_paths and probs, we can pair them up\n",
    "for p_control, p_treated, grp, path in zip(probs[:,0], probs[:,1], group_names, inference_paths):\n",
    "    groups_data[grp].append([p_control, p_treated, path])\n",
    "\n",
    "# Now we have groups_data with each group having a list of probabilities and paths.\n",
    "\n",
    "# 5. Plot distributions\n",
    "\n",
    "# Individual distributions per group:\n",
    "# We'll create subplots dynamically based on how many groups we have.\n",
    "num_groups = len(groups)\n",
    "fig, axs = plt.subplots(num_groups, 2, figsize=(10, 4*num_groups))\n",
    "# axs[i,0] -> p(control) distribution for group i\n",
    "# axs[i,1] -> p(treated) distribution for group i\n",
    "\n",
    "if num_groups == 1:\n",
    "    # If there's only one group, axs is not a 2D array\n",
    "    axs = np.array([axs])  # make it 2D for consistency\n",
    "\n",
    "for i, grp in enumerate(groups):\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "\n",
    "    axs[i, 0].hist(p_control_list, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axs[i, 0].set_title(f\"{grp} Group p(control)\")\n",
    "    axs[i, 0].set_xlabel(\"Probability\")\n",
    "    axs[i, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axs[i, 1].hist(p_treated_list, bins=20, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axs[i, 1].set_title(f\"{grp} Group p(treated)\")\n",
    "    axs[i, 1].set_xlabel(\"Probability\")\n",
    "    axs[i, 1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot combined distributions\n",
    "# 6.1: Compare all groups p(control) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_control_list = [item[0] for item in groups_data[grp]]\n",
    "    plt.hist(p_control_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(control) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6.2: Compare all groups p(treated) together\n",
    "plt.figure(figsize=(10, 5))\n",
    "for grp in groups:\n",
    "    p_treated_list = [item[1] for item in groups_data[grp]]\n",
    "    plt.hist(p_treated_list, bins=20, alpha=0.5, edgecolor='black', label=grp)\n",
    "plt.title(\"p(treated) Distribution Across All Groups\")\n",
    "plt.xlabel(\"Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# At this point:\n",
    "# - We've dynamically handled any number of groups in the directory.\n",
    "# - We plotted individual probability distributions for each group.\n",
    "# - We also plotted combined distributions comparing all groups together for both p(control) and p(treated).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in descending order (highest p(treated) first)\n",
    "all_images_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Now all_images_data is sorted by p(treated)\n",
    "# Extract (img_path, p_treated)\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "# Print or handle as needed\n",
    "print(\"Images sorted by p(treated) in descending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_data = []\n",
    "for grp in groups:\n",
    "    # each entry is [p_control, p_treated, img_path]\n",
    "    all_images_data.extend(groups_data[grp])\n",
    "\n",
    "# Sort by p(treated) in ascending order\n",
    "all_images_data.sort(key=lambda x: x[1])  # No reverse=True\n",
    "\n",
    "# Now all_images_data is sorted by p(treated) in ascending order\n",
    "sorted_by_treated = [(entry[2], entry[1]) for entry in all_images_data]\n",
    "\n",
    "print(\"Images sorted by p(treated) in ascending order:\")\n",
    "for img_path, p_tr in sorted_by_treated:\n",
    "    print(f\"{img_path}: {p_tr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_Image(image_path):\n",
    "    # Load the image\n",
    "    image = tiff.imread(image_path)\n",
    "    \n",
    "    # Ensure the image has 3 layers (channels)\n",
    "    if image.shape[0] != 3:\n",
    "        raise ValueError(f\"Image at {image_path} does not have exactly 3 layers.\")\n",
    "    \n",
    "    # Normalize the 16-bit image to [0, 1]\n",
    "    image = image.astype(np.float32) / 65535.0\n",
    "    \n",
    "    # Convert to a torch tensor\n",
    "    image = torch.tensor(image, dtype=torch.float32)\n",
    "    \n",
    "    # Resize to (96, 96)\n",
    "    image = TF.resize(image, (96, 96))\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_image = r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B07-T01.tiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = Preprocess_Image(path_of_image)\n",
    "print(first_image.shape)\n",
    "prep_first_image = first_image.unsqueeze(0)\n",
    "print(prep_first_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image_np = first_image.numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(first_image_np, cmap='gray')  # Use `cmap='gray'` if it's grayscale\n",
    "plt.title('First Image')\n",
    "plt.axis('off')  # Optional: turn off the axis for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathimage = r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B08-T01.tiff'\n",
    "second_image = Preprocess_Image(pathimage)\n",
    "print(second_image.shape)\n",
    "prep_second_image = second_image.unsqueeze(0)\n",
    "print(prep_second_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_image_np = second_image.numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(second_image_np, cmap='gray')  # Use `cmap='gray'` if it's grayscale\n",
    "plt.title('second Image')\n",
    "plt.axis('off')  # Optional: turn off the axis for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(model, image_tensor):\n",
    "    # prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    # Move image tensor to device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    features = network(image_tensor)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"simclr_model: {simclr_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for both images\n",
    "first_image_feats = extract_features(simclr_model, prep_first_image)\n",
    "second_image_feats = extract_features(simclr_model, prep_second_image)\n",
    "print(first_image_feats.shape)\n",
    "print(second_image_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE FROM NEWDATA CROP VAL&INFER\n",
    "# Compute cosine similarity between the feature vectors\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats, second_image_feats, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE old data crop\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats, second_image_feats, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\untreated\\D08-T01.tiff'\n",
    "untreated_image = Preprocess_Image(im_path)\n",
    "print(untreated_image.shape)\n",
    "prep_untreated_image = untreated_image.unsqueeze(0)\n",
    "print(prep_untreated_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untreated_image_np = untreated_image.numpy().transpose(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(untreated_image_np, cmap='gray')  # Use `cmap='gray'` if it's grayscale\n",
    "plt.title('untreated Image')\n",
    "plt.axis('off')  # Optional: turn off the axis for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "untreated_image_feats = extract_features(simclr_model, prep_untreated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE NEW DATA CROP\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats, untreated_image_feats, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine old data crop\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats, untreated_image_feats, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference after projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def features_after_projection(model, image_tensor):\n",
    "    # prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    #network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "    \n",
    "    # Move image tensor to device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    features = network(image_tensor)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for both images\n",
    "first_image_feats_after = features_after_projection(simclr_model, prep_first_image)\n",
    "second_image_feats_after = features_after_projection(simclr_model, prep_second_image)\n",
    "print(first_image_feats_after.shape)\n",
    "print(second_image_feats_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine newdata crop \n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats_after, second_image_feats_after, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine old data crop\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats_after, second_image_feats_after, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")\n",
    "\n",
    "#Cosine similarity between features: 0.8507535457611084\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected after projection head cosine similarity is higher this leads to the question that maybe for ranking images with time series, this after projection maybe better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untreated_image_feats_after = features_after_projection(simclr_model, prep_untreated_image)\n",
    "print(untreated_image_feats_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the feature vectors\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats_after, untreated_image_feats_after, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity between the feature vectors\n",
    "cosine_similarity = nn.functional.cosine_similarity(first_image_feats_after, untreated_image_feats_after, dim=1)\n",
    "print(f\"Cosine similarity between features: {cosine_similarity.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as expected after projection head cosine similarity is lower for different class images this leads to the question that maybe for ranking images with time series, this after projection maybe better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orig images (without simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_image)\n",
    "first_image.view(-1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(second_image)\n",
    "second_image.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_flat = first_image.view(-1)\n",
    "second_flat = second_image.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_flat.unsqueeze(0).shape == untreated_flat.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "similarity = F.cosine_similarity(first_flat.unsqueeze(0), second_flat.unsqueeze(0))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "untreated_flat = untreated_image.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = F.cosine_similarity(first_flat.unsqueeze(0), untreated_flat.unsqueeze(0))\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_flat == untreated_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "orig without resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load and normalize both images\n",
    "im1 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B07-T01.tiff')\n",
    "im2 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B08-T01.tiff')\n",
    "\n",
    "# Ensure each image has 3 layers (channels)\n",
    "if im1.shape[0] != 3 or im2.shape[0] != 3:\n",
    "    raise ValueError(\"Each image must have exactly 3 layers.\")\n",
    "\n",
    "# Normalize the 16-bit images to [0, 1]\n",
    "im1 = im1.astype(np.float32) / 65535.0\n",
    "im2 = im2.astype(np.float32) / 65535.0\n",
    "print(im2.shape)\n",
    "\n",
    "# Flatten the 3D images to 1D vectors\n",
    "im1_flattened = im1.flatten()\n",
    "im2_flattened = im2.flatten()\n",
    "print(im2_flattened.shape)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = np.dot(im1_flattened, im2_flattened) / (norm(im1_flattened) * norm(im2_flattened))\n",
    "\n",
    "print(f\"Cosine Similarity between the two images: {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B07-T01.tiff')\n",
    "im2 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\untreated\\D08-T01.tiff')\n",
    "\n",
    "# Ensure each image has 3 layers (channels)\n",
    "if im1.shape[0] != 3 or im2.shape[0] != 3:\n",
    "    raise ValueError(\"Each image must have exactly 3 layers.\")\n",
    "\n",
    "# Normalize the 16-bit images to [0, 1]\n",
    "im1 = im1.astype(np.float32) / 65535.0\n",
    "im2 = im2.astype(np.float32) / 65535.0\n",
    "print(im2.shape)\n",
    "\n",
    "# Flatten the 3D images to 1D vectors\n",
    "im1_flattened = im1.flatten()\n",
    "im2_flattened = im2.flatten()\n",
    "print(im2_flattened.shape)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = np.dot(im1_flattened, im2_flattened) / (norm(im1_flattened) * norm(im2_flattened))\n",
    "\n",
    "print(f\"Cosine Similarity between the two images: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WAS IST DAS?\n",
    "Mach kein Sinn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B07-T01.tiff')\n",
    "img2 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\single_dose\\B08-T01.tiff')\n",
    "img3 = tiff.imread(r'C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\Data_supervised\\untreated\\D08-T01.tiff')\n",
    "img1_flattened = img1.flatten()\n",
    "img2_flattened = img2.flatten()\n",
    "img3_flattened = img3.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = np.dot(img1_flattened, img2_flattened) / (norm(img1_flattened) * norm(img2_flattened))\n",
    "print(f\"Cosine Similarity between the two images: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = np.dot(img1_flattened, img3_flattened) / (norm(img1_flattened) * norm(img3_flattened))\n",
    "print(f\"Cosine Similarity between the two images: {cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, if we didn't use simclr and just try to find the cosine similarity between orig images: it doesn't deviate too  much not good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
