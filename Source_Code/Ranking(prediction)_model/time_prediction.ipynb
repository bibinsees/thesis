{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.tiff')]\n",
    "        #we don't need to resize into 96*96 because we are doing that in below contrastive transform (self.resize_transform = transforms.resize((96,96)))\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "        \n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        \n",
    "        # Convert to a torch tensor\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Data augmentation similar to the tutorial\n",
    "contrast_transforms = transforms.Compose([\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(size=256),\n",
    "    ])\n",
    "\n",
    "# Create the dataset\n",
    "image_dir = r\"../../Day10_drugscreened&singledose_untreated\"\n",
    "\n",
    "\n",
    "dataset = ImageDataset(image_dir=image_dir, transform=contrast_transforms)\n",
    "batch_size = 16\n",
    "\n",
    "# Function to split dataset with explicit percentage\n",
    "def split_dataset(dataset, val_percentage):\n",
    "    val_size = int(len(dataset) * val_percentage)\n",
    "    train_size = len(dataset) - val_size\n",
    "    return random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Split the dataset with 20% for validation\n",
    "val_percentage = 0.2\n",
    "train_dataset, val_dataset = split_dataset(dataset, val_percentage)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=0) #num_workers=os.cpu count() using cluster gpu\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(train_loader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  image: {image.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencod_fituning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencod_fituning, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding='same'),  # Input: (3, 96, 96) Output: (64, 96, 96)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, padding=1),        # Output: (64, 48, 48)\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding='same'), # Output: (32, 48, 48)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, padding=1),        # Output: (32, 24, 24)\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding='same'), # Output: (16, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2, padding=0)         # Output: (16, 12, 12)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding='same'), # Output: (32, 12, 12)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),      # Output: (32, 24, 24)\n",
    "    \n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding='same'), # Output: (16, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),      # Output: (16, 48, 48)\n",
    "    \n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding='same'),  # Output: (3, 48, 48)\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),      # Output: (3, 96, 96)\n",
    "            nn.Sigmoid()                                      # Ensures output values are in [0, 1] range\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"Input:\", x.shape)\n",
    "        \n",
    "        # Encoder\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            #print(f\"After {layer}: {x.shape}\")\n",
    "        \n",
    "        # Decoder\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "            #print(f\"After {layer}: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modi = Autoencod_fituning()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(modi.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(mo, train_loader, val_loader, optimizer, criterion, num_epochs=1):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()  # Start time for the epoch\n",
    "        \n",
    "        # Training phase\n",
    "        modi.train()\n",
    "        train_loss = 0\n",
    "        for images in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = modi(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        modi.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images in val_loader:\n",
    "                outputs = modi(images)\n",
    "                loss = criterion(outputs, images)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        end_time = time.time()  # End time for the epoch\n",
    "        epoch_time = end_time - start_time  # Calculate epoch duration\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Time: {epoch_time:.2f} seconds')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "#train_losses, val_losses = train_and_validate(modi, train_loader, val_loader, optimizer, criterion, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train_and_validate(modi, train_loader, val_loader, optimizer, criterion, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
