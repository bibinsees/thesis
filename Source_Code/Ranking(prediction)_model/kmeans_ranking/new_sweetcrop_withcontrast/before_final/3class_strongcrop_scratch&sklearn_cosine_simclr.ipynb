{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay,max_epochs):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        \n",
    "        # Modify the fully connected layer\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, 20)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=max_epochs, eta_min=lr / 50)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)\n",
    "\n",
    "    def info_nce_loss(self, imgs1, imgs2, device):\n",
    "\n",
    "        imgs = torch.cat((imgs1, imgs2), dim=0)  # Concatenate along the batch dimension\n",
    "        imgs = imgs.to(device)  # Move images to the device\n",
    "\n",
    "        # Encode all images\n",
    "        feats = self.forward(imgs)\n",
    "    \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = nn.functional.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
    "    \n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "    \n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
    "    \n",
    "        # Normalize similarity scores by temperature\n",
    "        cos_sim = cos_sim / self.temperature\n",
    "\n",
    "        # InfoNCE loss\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Accuracy calculations\n",
    "        # Create a combination of positive and negative similarities for ranking\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:, None],  # Positive example in first position\n",
    "                          cos_sim.masked_fill(pos_mask, -9e15)], dim=-1)\n",
    "    \n",
    "        # Sort and get the ranking position of the positive example\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "    \n",
    "        # Compute accuracy metrics\n",
    "        top1_acc = (sim_argsort == 0).float().mean()  # Top-1 accuracy\n",
    "        top5_acc = (sim_argsort < 5).float().mean()   # Top-5 accuracy\n",
    "        mean_pos = 1 + sim_argsort.float().mean()     # Mean position of the positive example\n",
    "\n",
    "        return nll, top1_acc, top5_acc, mean_pos\n",
    "\n",
    "    def train_epoch(self, train_loader, device):\n",
    "        self.train()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            imgs1, imgs2, _ = batch\n",
    "            imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "        \n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate loss and accuracy metrics\n",
    "            loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #self.lr_scheduler.step()\n",
    "\n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            total_top1_acc += top1_acc.item()\n",
    "            total_top5_acc += top5_acc.item()\n",
    "            total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(train_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(train_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(train_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos\n",
    "\n",
    "    def validate_epoch(self, val_loader, device):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                imgs1, imgs2, _ = batch\n",
    "                imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "\n",
    "                # Calculate loss and accuracy metrics\n",
    "                loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "                # Accumulate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_top1_acc += top1_acc.item()\n",
    "                total_top5_acc += top5_acc.item()\n",
    "                total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(val_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(val_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(val_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos\n",
    "    \n",
    "\n",
    "    def inference_epoch(self, inference_loader, device):\n",
    "        self.eval()\n",
    "        total_loss = 0.0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(inference_loader, desc=\"Inferencing\", leave=False):\n",
    "                imgs1, imgs2, _ = batch\n",
    "                imgs1, imgs2 = imgs1.to(device), imgs2.to(device)  # Move data to device\n",
    "\n",
    "                # Calculate loss and accuracy metrics\n",
    "                loss, top1_acc, top5_acc, mean_pos = self.info_nce_loss(imgs1, imgs2, device)\n",
    "\n",
    "                # Accumulate metrics\n",
    "                total_loss += loss.item()\n",
    "                total_top1_acc += top1_acc.item()\n",
    "                total_top5_acc += top5_acc.item()\n",
    "                total_mean_pos += mean_pos.item()\n",
    "\n",
    "        avg_loss = total_loss / len(inference_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(inference_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(inference_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(inference_loader)\n",
    "\n",
    "        return avg_loss, avg_top1_acc, avg_top5_acc, avg_mean_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k54739\\AppData\\Local\\Temp\\ipykernel_247348\\1068533374.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  simclr_model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimCLR(\n",
       "  (convnet): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Linear(in_features=512, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file paths\n",
    "model_path = r'C:\\Users\\k54739\\Bibi_new_thesis\\saved_model\\sweetcrop_simclr_model_epoch_245.pth'\n",
    "simclr_model = SimCLR(hidden_dim=128, lr=5e-4, temperature=0.07, weight_decay=1e-4,max_epochs=245)\n",
    "simclr_model.load_state_dict(torch.load(model_path))\n",
    "simclr_model.to(device)\n",
    "simclr_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: {'cond7_all': 472}\n"
     ]
    }
   ],
   "source": [
    "class LabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_files, labels, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        #self.transform = transform\n",
    "        self.resize_transform = transforms.Resize((96, 96))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "\n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        image = self.resize_transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image, label\n",
    "\n",
    "        #return image, label, img_path\n",
    "\n",
    "def load_data(root_dir):\n",
    "    \n",
    "    #classes = ['cond7_curated']\n",
    "    #classes = ['sd_only']\n",
    "    #classes = ['ex']\n",
    "    #classes = ['ds_closeto_sd']\n",
    "    classes = ['cond7_all']\n",
    "\n",
    "\n",
    "    image_files = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        files = [os.path.join(class_dir, file) for file in os.listdir(class_dir) if file.endswith(('.tiff', '.tif'))]\n",
    "        image_files.extend(files)\n",
    "        labels.extend([idx] * len(files))\n",
    "    \n",
    "    # Check if the labels correctly reflect the classes\n",
    "    print(\"Label distribution:\", {classes[i]: labels.count(i) for i in range(len(classes))})\n",
    "\n",
    "    return image_files, labels\n",
    "\n",
    "\n",
    "# Directories for labeled data\n",
    "image_dir = r\"C:\\Users\\k54739\\Bibi_new_thesis\\thesis\\classification\"\n",
    "\n",
    "# Load data\n",
    "image_files, labels = load_data(image_dir)\n",
    "\n",
    "# Create the labeled datasets\n",
    "labeled_dataset = LabeledImageDataset(image_files, labels)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "loader_labeled = DataLoader(labeled_dataset, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 96, 96]) torch.Size([16]) torch.float32\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#for anchor,label, path in loader_labeled:\n",
    "for anchor,label in loader_labeled:\n",
    "    print(anchor.shape, label.shape, anchor.dtype)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataloader):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(dataloader):\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        print(f\"Batch features shape: {batch_feats.shape}\")\n",
    "        print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "        \n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    print(f\"Features shape after concatenation: {feats.shape}\")\n",
    "    print(f\"Labels shape after concatenation: {labels.shape}\")\n",
    "\n",
    "    return torch.utils.data.TensorDataset(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:27,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:22,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:20,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:03<00:19,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:18,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:04<00:17,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:05<00:16,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:15,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:06<00:14,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:07<00:14,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:08<00:13,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:08<00:12,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:09<00:11,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:10<00:11,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:10<00:10,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:11<00:09,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:12<00:09,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:12<00:08,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:13<00:07,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:14<00:07,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:15<00:06,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:15<00:06,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:16<00:05,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:17<00:04,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:18<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:19<00:03,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:20<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:20<00:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:21<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:22<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch features shape: torch.Size([8, 512])\n",
      "Batch labels shape: torch.Size([8])\n",
      "Features shape after concatenation: torch.Size([472, 512])\n",
      "Labels shape after concatenation: torch.Size([472])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features for train and test datasets\n",
    "feats_simclr = prepare_data_features(simclr_model, loader_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features (for K-Means): (472, 512)\n",
      "Shape of labels: (472,)\n"
     ]
    }
   ],
   "source": [
    "# Convert features and labels to NumPy arrays\n",
    "feats_np = feats_simclr.tensors[0].numpy()  # Features in shape (60, 512)\n",
    "feats_np_norm = normalize(feats_np, axis=1)\n",
    "labels_np = feats_simclr.tensors[1].numpy()  # Corresponding labels\n",
    "\n",
    "# Check the shapes\n",
    "print(\"Shape of features (for K-Means):\", feats_np.shape)\n",
    "print(\"Shape of labels:\", labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([472, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condall = feats_simclr.tensors[0]\n",
    "condall.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the current folder\n",
    "save_path = \"condall.pt\"  # File name only\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(condall, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = feats_simclr.tensors[0]\n",
    "sd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the current folder\n",
    "save_path = \"sd.pt\"  # File name only\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(sd, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond7 = feats_simclr.tensors[0]\n",
    "cond7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the current folder\n",
    "save_path = \"cond7.pt\"  # File name only\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(cond7, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = feats_simclr.tensors[0]\n",
    "ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the current folder\n",
    "save_path = \"ex.pt\"  # File name only\n",
    "\n",
    "# Save the tensor\n",
    "torch.save(ex, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means initialization function\n",
    "def kMeans_init_centroids(X, K):\n",
    "    randidx = np.random.permutation(X.shape[0])\n",
    "    centroids = X[randidx[:K]]\n",
    "    return centroids\n",
    "\n",
    "# Function to find the closest centroids using Cosine Similarity\n",
    "def find_closest_centroids(X, centroids):\n",
    "    # Normalize both the data points and centroids to ensure we compute cosine similarity\n",
    "    #X_norm = normalize(X, axis=1)\n",
    "    #centroids_norm = normalize(centroids, axis=1)\n",
    "    \n",
    "    # Assign data points to closest centroids based on cosine similarity\n",
    "    idx = np.zeros(X.shape[0], dtype=int)\n",
    "    for i in range(X.shape[0]):\n",
    "        # Compute cosine similarity\n",
    "        similarities = np.dot(centroids, X[i])  # Dot product gives cosine similarity\n",
    "        idx[i] = np.argmax(similarities)  # We want the most similar (highest value)\n",
    "    return idx\n",
    "\n",
    "# Function to compute new centroids\n",
    "def compute_centroids(X, idx, K):\n",
    "    centroids = np.zeros((K, X.shape[1]))  # Use shape[1] for features\n",
    "    for k in range(K):\n",
    "        points = X[idx == k]\n",
    "        if len(points) > 0:\n",
    "            centroids[k] = np.mean(points, axis=0)   # noralised vectors mean maynot be normalised. hence we normalise before calculating mean.https://chatgpt.com/share/671b97a7-ec2c-8010-af33-af106df0a25c\n",
    "            centroids_norm = normalize(centroids, axis=1)\n",
    "    return centroids_norm\n",
    "\n",
    "# Function to run K-Means algorithm with cost tracking (using Cosine Similarity)\n",
    "def run_kMeans(X, initial_centroids, max_iters=10):\n",
    "    K = initial_centroids.shape[0]\n",
    "    centroids = initial_centroids\n",
    "    idx = np.zeros(X.shape[0])\n",
    "    \n",
    "    best_centroids = centroids\n",
    "    lowest_cost = float('inf')\n",
    "    final_iteration = 0  # To keep track of the iteration where the best centroids were found\n",
    "    best_idx = idx  # Track the best index assignment\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        print(f\"K-Means iteration {i}/{max_iters - 1}\")\n",
    "        \n",
    "        # Assign each data point to the closest centroid using cosine similarity\n",
    "        idx = find_closest_centroids(X, centroids)\n",
    "        \n",
    "        # Compute new centroids\n",
    "        centroids = compute_centroids(X, idx, K)\n",
    "\n",
    "        # Calculate cost function for the current centroids using cosine distance\n",
    "\n",
    "        # 1. Compute cosine similarity\n",
    "        sim = np.dot(X, centroids.T)\n",
    "        #print(sim.shape)\n",
    "        #print(sim)\n",
    "        # 2. Calculate cosine distance\n",
    "        cosine_dist = 1 - sim\n",
    "        #print(cosine_dist.shape)\n",
    "        #print(cosine_dist)\n",
    "        # 3. Find maximum cosine distance for each data point\n",
    "        #max_cosine_dist = cosine_dist.max(axis=1)\n",
    "        #print(max_cosine_dist.shape)\n",
    "        #print(max_cosine_dist)\n",
    "        cost = np.sum(cosine_dist)\n",
    "\n",
    "        # 4. Sum of all maximum distances\n",
    "        #cost = np.sum(max_cosine_dist)\n",
    "\n",
    "        #cost = np.sum(1 - np.dot(X_norm, centroids_norm.T).max(axis=1))  # Cosine distance = 1 - cosine similarity  \n",
    "        print(f\"Cost function value: {cost}\")  # Print the cost function value\n",
    "\n",
    "        # Check if this is the best cost so far\n",
    "        if cost < lowest_cost:\n",
    "            lowest_cost = cost\n",
    "            best_centroids = centroids\n",
    "            best_idx = idx.copy()\n",
    "            final_iteration = i  # Update the iteration where best centroids were found\n",
    "\n",
    "    print(f\"Final centroids selected from iteration: {final_iteration}\")  # Indicate which iteration was chosen\n",
    "    return best_centroids, best_idx, centroids,idx  # Return the best centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means iteration 0/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 1/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 2/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 3/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 4/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 5/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 6/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 7/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 8/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 9/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 10/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 11/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 12/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 13/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 14/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 15/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 16/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 17/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 18/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 19/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 20/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 21/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 22/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 23/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 24/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 25/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 26/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 27/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 28/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 29/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 30/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 31/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 32/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 33/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 34/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 35/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 36/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 37/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 38/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 39/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 40/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 41/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 42/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 43/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 44/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 45/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 46/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 47/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 48/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 49/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 50/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 51/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 52/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 53/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 54/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 55/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 56/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 57/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 58/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 59/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 60/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 61/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 62/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 63/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 64/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 65/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 66/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 67/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 68/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 69/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 70/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 71/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 72/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 73/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 74/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 75/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 76/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 77/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 78/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 79/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 80/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 81/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 82/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 83/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 84/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 85/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 86/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 87/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 88/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 89/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 90/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 91/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 92/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 93/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 94/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 95/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 96/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 97/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 98/99\n",
      "Cost function value: 146.43389674911572\n",
      "K-Means iteration 99/99\n",
      "Cost function value: 146.43389674911572\n",
      "Final centroids selected from iteration: 0\n",
      "Final centroids: [[0.01450272 0.04216911 0.04801466 0.03051626 0.06299964 0.05432907\n",
      "  0.02501262 0.04773025 0.02374443 0.06048645 0.02847794 0.02336011\n",
      "  0.03293034 0.0223331  0.03706068 0.03262169 0.01400577 0.03499406\n",
      "  0.07004246 0.03044893 0.06560752 0.04406485 0.03254295 0.02141878\n",
      "  0.07057307 0.06888093 0.03906109 0.05558268 0.03752691 0.02679527\n",
      "  0.08372629 0.06087431 0.03601621 0.0478649  0.02890398 0.09222496\n",
      "  0.04932474 0.04963018 0.04840381 0.03268248 0.02480238 0.05123709\n",
      "  0.03551817 0.06688501 0.02598961 0.06623164 0.03391014 0.05368966\n",
      "  0.01979587 0.03957195 0.06880772 0.04207939 0.0337273  0.02463616\n",
      "  0.0372488  0.07921958 0.04933177 0.01501084 0.06247552 0.05953664\n",
      "  0.06348649 0.03330502 0.05746788 0.03981077 0.0190536  0.04957752\n",
      "  0.01583575 0.0206231  0.02280042 0.01314167 0.03710267 0.03423824\n",
      "  0.0385422  0.01113119 0.0405151  0.01589313 0.04759635 0.05530845\n",
      "  0.04976032 0.02620421 0.05796221 0.04094954 0.02630642 0.04200361\n",
      "  0.0553648  0.03078027 0.06755944 0.01527025 0.06113459 0.02387541\n",
      "  0.03014429 0.08104702 0.01930852 0.08547545 0.05198284 0.00901226\n",
      "  0.05196221 0.07284213 0.03835698 0.04145647 0.04753523 0.04505875\n",
      "  0.04359701 0.04862332 0.02862039 0.04978901 0.04320927 0.03985428\n",
      "  0.03179833 0.01805173 0.01711181 0.03524109 0.05338555 0.04108991\n",
      "  0.02959181 0.03228109 0.02100163 0.04399094 0.07696878 0.03364773\n",
      "  0.04490346 0.01231867 0.03496673 0.03247422 0.04229724 0.04683523\n",
      "  0.03368196 0.04222606 0.07114755 0.01217133 0.02770412 0.01094161\n",
      "  0.01919864 0.02898297 0.0522867  0.0611814  0.04454571 0.02143899\n",
      "  0.03353084 0.04656586 0.04524918 0.06595383 0.02546269 0.01842724\n",
      "  0.02017076 0.08485744 0.03949968 0.04180253 0.01841958 0.07760974\n",
      "  0.08107856 0.01473051 0.01762493 0.02044957 0.04743149 0.03395837\n",
      "  0.0128614  0.01586719 0.06802835 0.06075711 0.02042398 0.04673631\n",
      "  0.03093171 0.07488579 0.03286345 0.06326626 0.04544121 0.01613829\n",
      "  0.04201565 0.04993109 0.0302601  0.05158447 0.04724594 0.06431721\n",
      "  0.02657473 0.00319688 0.04396498 0.02672293 0.06813431 0.02043214\n",
      "  0.01915715 0.07231448 0.1044818  0.02613067 0.0360597  0.08346505\n",
      "  0.07616461 0.04777857 0.02513661 0.01756607 0.04340698 0.00965421\n",
      "  0.05247714 0.05303596 0.01274494 0.03759242 0.05964327 0.06362597\n",
      "  0.03847975 0.02324438 0.05344823 0.01826142 0.04898359 0.04292552\n",
      "  0.04119855 0.00638766 0.02135137 0.03664946 0.00848422 0.08476058\n",
      "  0.0203527  0.0293768  0.03891575 0.02797991 0.05907733 0.04439346\n",
      "  0.01546787 0.03020394 0.03107432 0.05612715 0.08879933 0.0448463\n",
      "  0.02237074 0.04512004 0.03996032 0.01691785 0.03259429 0.03672755\n",
      "  0.04211904 0.03803917 0.05031147 0.04125077 0.03483311 0.02119885\n",
      "  0.03424107 0.02272911 0.05926056 0.03937729 0.00419944 0.07899658\n",
      "  0.01700471 0.02982418 0.01838832 0.02121793 0.01063963 0.08196723\n",
      "  0.03215154 0.02733894 0.04440659 0.06053175 0.01346693 0.03462142\n",
      "  0.06064015 0.0068599  0.02422551 0.06909355 0.02855767 0.05848866\n",
      "  0.05069366 0.05374806 0.02899645 0.00963115 0.03154472 0.0790095\n",
      "  0.05156182 0.0345449  0.03951133 0.02060356 0.04848217 0.0283124\n",
      "  0.03198072 0.04595666 0.02792504 0.04939228 0.06289321 0.04210402\n",
      "  0.04574839 0.07472425 0.02014364 0.04357844 0.07664738 0.04863662\n",
      "  0.06888862 0.05490626 0.00839083 0.01625851 0.05205275 0.03603136\n",
      "  0.0720046  0.03550731 0.0346108  0.05349795 0.0233276  0.0212912\n",
      "  0.04489585 0.02064879 0.05560382 0.05778244 0.04602506 0.06517846\n",
      "  0.04686864 0.02805507 0.06436456 0.03009041 0.06258162 0.04297239\n",
      "  0.0205983  0.03031931 0.03656546 0.01999117 0.0550662  0.03503636\n",
      "  0.05505363 0.0411732  0.02590351 0.05415984 0.08208764 0.04226378\n",
      "  0.06072868 0.01165817 0.02079189 0.03807287 0.01706703 0.04436982\n",
      "  0.03717572 0.02363223 0.0132401  0.01765154 0.04699572 0.02897978\n",
      "  0.01099353 0.04925701 0.04607033 0.04739856 0.04410665 0.0155401\n",
      "  0.04899521 0.05928556 0.05409968 0.06785701 0.01605874 0.04388216\n",
      "  0.02762122 0.04432395 0.06927492 0.03343443 0.01072761 0.03138203\n",
      "  0.03366295 0.06820251 0.06600636 0.02378255 0.02793241 0.0200422\n",
      "  0.015862   0.01339111 0.03687962 0.03052874 0.06060124 0.05056893\n",
      "  0.04071128 0.06645235 0.03744354 0.06987237 0.0412212  0.035086\n",
      "  0.03099915 0.03456519 0.05043404 0.02449834 0.02793097 0.06479726\n",
      "  0.02379006 0.02545846 0.02923586 0.04570405 0.01732271 0.03635971\n",
      "  0.03192363 0.01790184 0.06701499 0.0285885  0.0327426  0.02082788\n",
      "  0.01172706 0.03905613 0.02333834 0.00873827 0.03616371 0.08113041\n",
      "  0.02012556 0.02243172 0.01275979 0.06290957 0.07633189 0.04227698\n",
      "  0.07339998 0.01542633 0.02811147 0.04212437 0.04301642 0.03848257\n",
      "  0.01213596 0.01763792 0.0324397  0.03460291 0.01659751 0.04206262\n",
      "  0.01432368 0.02264384 0.02817384 0.01220162 0.04579472 0.02458114\n",
      "  0.04532342 0.07235528 0.02348315 0.04596441 0.00886086 0.01198178\n",
      "  0.07223197 0.05850832 0.02617367 0.03645302 0.05079371 0.01008111\n",
      "  0.03452348 0.01287522 0.05407325 0.03428652 0.06424959 0.04963501\n",
      "  0.02121686 0.03973783 0.0376569  0.09302526 0.06297914 0.0412262\n",
      "  0.06449458 0.01678133 0.01762007 0.03935919 0.01520617 0.01689035\n",
      "  0.04793152 0.03021114 0.01008586 0.07146138 0.03484165 0.06805382\n",
      "  0.06306357 0.01745759 0.0532841  0.04580891 0.01881212 0.06827338\n",
      "  0.04747204 0.05071867 0.03482831 0.03446031 0.08348366 0.03991846\n",
      "  0.02561638 0.05479485 0.06747293 0.03962855 0.02351187 0.0517433\n",
      "  0.01817847 0.08503433 0.06121619 0.0361551  0.01204939 0.03320136\n",
      "  0.02055795 0.04794286 0.03616659 0.03383952 0.06552582 0.04429219\n",
      "  0.02352097 0.04371833 0.014888   0.06391752 0.04028121 0.00979307\n",
      "  0.01288656 0.03160039 0.09491637 0.02910268 0.04271421 0.05464631\n",
      "  0.03687805 0.05929913 0.01660152 0.01879816 0.04161427 0.01137702\n",
      "  0.03812307 0.03078814 0.03053727 0.03549684 0.06732929 0.00963489\n",
      "  0.02022948 0.05359329 0.0401672  0.05043245 0.04337345 0.07052924\n",
      "  0.02786788 0.05644537]]\n"
     ]
    }
   ],
   "source": [
    "# Main function to run the K-Means algorithm\n",
    "\n",
    "K = 1                     # Set number of clusters\n",
    "initial_centroids = kMeans_init_centroids(feats_np_norm, K)  # Step 3: Initialize centroids\n",
    "max_iters = 100                # Step 4: Number of iterations\n",
    "best_centroids, best_idx, centroids, idx = run_kMeans(feats_np_norm, initial_centroids, max_iters)  # Step 5: Run K-Means\n",
    "print(\"Final centroids:\", centroids)  # Output the final centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('condall.npy', centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ex.npy', centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('cond7.npy', centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sd.npy', centroids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
