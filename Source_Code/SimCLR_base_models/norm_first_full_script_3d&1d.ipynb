{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset class for 3 channel/image\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir,size):\n",
    "        self.image_dir = image_dir\n",
    "        #self.transform = transform\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.tiff')]\n",
    "        #we don't need to resize into 96*96 because we are doing that in below contrastive transform (self.resize_transform = transforms.resize((96,96)))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomResizedCrop(size=size),\n",
    "            transforms.RandomApply([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "        \n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        \n",
    "        # Convert to a torch tensor\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image1 = self.transform(image)\n",
    "            image2 = self.transform(image)\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, size, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.tiff')]\n",
    "        self.resize_transform = transforms.Resize((size,size))\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = tiff.imread(img_path)\n",
    "\n",
    "        # Ensure the image has 3 layers (channels)\n",
    "        if image.shape[0] != 3:\n",
    "            raise ValueError(f\"Image {img_path} does not have exactly 3 layers.\")\n",
    "        \n",
    "        # Normalize the 16-bit image to [0, 1]\n",
    "        image = image.astype(np.float32) / 65535.0\n",
    "        \n",
    "        # Calculate sharpness for each layer\n",
    "        sharpness_scores = []\n",
    "        for i in range(3):\n",
    "            layer = image[i]\n",
    "            gy, gx = np.gradient(layer)\n",
    "            gnorm = np.sqrt(gx**2 + gy**2)\n",
    "            sharpness = np.average(gnorm)\n",
    "            sharpness_scores.append(sharpness)\n",
    "        \n",
    "\n",
    "        # Find the index of the sharpest layer\n",
    "        sharpest_layer_index = np.argmax(sharpness_scores)\n",
    "        \n",
    "        # Determine the anchor (sharpest layer) and the other two layers (augmentations)\n",
    "        anchor = image[sharpest_layer_index]\n",
    "        other_indices = [i for i in range(3) if i != sharpest_layer_index]\n",
    "        augmentation1 = image[other_indices[0]]\n",
    "        augmentation2 = image[other_indices[1]]\n",
    "\n",
    "        # Convert to a torch tensor and add channel dimension\n",
    "        anchor = torch.tensor(anchor, dtype=torch.float32).unsqueeze(0)\n",
    "        img1 = torch.tensor(augmentation1, dtype=torch.float32).unsqueeze(0)\n",
    "        img2 = torch.tensor(augmentation2, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Apply resize transform\n",
    "        #anchor = self.resize_transform(anchor)\n",
    "        img1 = self.resize_transform(img1)\n",
    "        img2 = self.resize_transform(img2)\n",
    "        \n",
    "        return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(train_losses, val_losses, train_top1_accs, val_top1_accs, train_top5_accs, val_top5_accs, train_mean_pos, val_mean_pos):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Top-1 accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_top1_accs, 'bo-', label='Training Top-1 Accuracy')\n",
    "    plt.plot(epochs, val_top1_accs, 'ro-', label='Validation Top-1 Accuracy')\n",
    "    plt.title('Training and Validation Top-1 Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Top-1 Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Top-5 accuracy\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, train_top5_accs, 'bo-', label='Training Top-5 Accuracy')\n",
    "    plt.plot(epochs, val_top5_accs, 'ro-', label='Validation Top-5 Accuracy')\n",
    "    plt.title('Training and Validation Top-5 Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Top-5 Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Mean Position of Positive Example\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(epochs, train_mean_pos, 'bo-', label='Training Mean Position')\n",
    "    plt.plot(epochs, val_mean_pos, 'ro-', label='Validation Mean Position')\n",
    "    plt.title('Training and Validation Mean Position of Positive Example')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Position')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Function to split dataset with explicit percentage\n",
    "def split_dataset(dataset, val_percentage):\n",
    "    val_size = int(len(dataset) * val_percentage)\n",
    "    train_size = len(dataset) - val_size\n",
    "    return random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Resnet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained ResNet-18 model\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel images\n",
    "        self.convnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Modify the fully connected layer\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the ResNet layers\n",
    "        x = self.convnet.conv1(x)\n",
    "        x = self.convnet.bn1(x)\n",
    "        x = self.convnet.relu(x)\n",
    "        x = self.convnet.maxpool(x)\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "\n",
    "        x = self.convnet.layer1(x)\n",
    "        print(f\"Shape after layer1: {x.shape}\")\n",
    "\n",
    "        x = self.convnet.layer2(x)\n",
    "        print(f\"Shape after layer2: {x.shape}\")\n",
    "\n",
    "        x = self.convnet.layer3(x)\n",
    "        print(f\"Shape after layer3: {x.shape}\")\n",
    "\n",
    "        x = self.convnet.layer4(x)\n",
    "        print(f\"Shape after layer4: {x.shape}\")\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.convnet.avgpool(x)\n",
    "        print(f\"Shape after avgpool: {x.shape}\")\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f\"Shape after flatten: {x.shape}\")\n",
    "\n",
    "        # Pass through the projection head\n",
    "        x = self.convnet.fc(x)\n",
    "        print(f\"Shape after projection head: {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for 3 channel/image\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.convnet.conv1(x)\n",
    "        x = self.convnet.bn1(x)\n",
    "        x = self.convnet.relu(x)\n",
    "        x = self.convnet.maxpool(x)\n",
    "\n",
    "        x = self.convnet.layer1(x)\n",
    "        x = self.convnet.layer2(x)\n",
    "        x = self.convnet.layer3(x)\n",
    "        x = self.convnet.layer4(x)\n",
    "\n",
    "        # Print the shape after each layer\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "        print(f\"Shape after layer1: {x.shape}\")\n",
    "        print(f\"Shape after layer2: {x.shape}\")\n",
    "        print(f\"Shape after layer3: {x.shape}\")\n",
    "        print(f\"Shape after layer4: {x.shape}\")\n",
    "\n",
    "        # Global average pooling\n",
    "        x = self.convnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        print(f\"Shape after avgpool: {x.shape}\")\n",
    "\n",
    "        # Pass through the projection head\n",
    "        x = self.convnet.fc(x)\n",
    "        print(f\"Shape after projection head: {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Resnet, self).__init__()\n",
    "        self.convnet = torchvision.models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            nn.Linear(self.convnet.fc.in_features, 4 * hidden_dim),  # Linear layer with 4*hidden_dim output\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4 * hidden_dim, hidden_dim)  # Output layer with hidden_dim output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convnet(x)\n",
    "    \n",
    "\n",
    "#print(Resnet(96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimCLR_loss(feats, temperature):\n",
    "    # Calculate cosine similarity\n",
    "    cos_sim = nn.functional.cosine_similarity(feats[:, None, :], feats[None, :, :], dim=-1)\n",
    "    \n",
    "    # Mask out cosine similarity to itself\n",
    "    self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "    cos_sim.masked_fill_(self_mask, -9e15)\n",
    "    \n",
    "    # Find positive example -> batch_size//2 away from the original example\n",
    "    pos_mask = self_mask.roll(shifts=cos_sim.shape[0] // 2, dims=0)\n",
    "    \n",
    "    # InfoNCE loss\n",
    "    cos_sim = cos_sim / temperature\n",
    "    nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "    nll = nll.mean()\n",
    "\n",
    "    # Accuracy calculations\n",
    "    comb_sim = torch.cat([cos_sim[pos_mask][:, None],  # First position positive example\n",
    "                          cos_sim.masked_fill(pos_mask, -9e15)],\n",
    "                         dim=-1)\n",
    "    sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "\n",
    "    # Calculate accuracy metrics\n",
    "    acc_top1 = (sim_argsort == 0).float().mean()\n",
    "    acc_top5 = (sim_argsort < 5).float().mean()\n",
    "    mean_pos = 1 + sim_argsort.float().mean()\n",
    "\n",
    "    return nll, acc_top1.item(), acc_top5.item(), mean_pos.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs, lr=5e-4, temperature=0.07, weight_decay=1e-4, device='cuda', validate=False):\n",
    "    if not validate:\n",
    "        model = model.train().to(device)\n",
    "    else:\n",
    "        model = model.eval().to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=lr / 50)\n",
    "\n",
    "    train_losses = []\n",
    "    train_top1_accs = []\n",
    "    train_top5_accs = []\n",
    "    mean_positions = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_top1_acc = 0.0\n",
    "        total_top5_acc = 0.0\n",
    "        total_mean_pos = 0.0\n",
    "\n",
    "        for imgs1, imgs2 in train_loader:\n",
    "            imgs1, imgs2 = imgs1.to(device), imgs2.to(device)\n",
    "            imgs = torch.cat((imgs1, imgs2), dim=0)\n",
    "            feats = model(imgs)\n",
    "\n",
    "            # Compute the loss and accuracy\n",
    "            loss, acc_top1, acc_top5, mean_pos = SimCLR_loss(feats, temperature)\n",
    "\n",
    "            if not validate:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_top1_acc += acc_top1\n",
    "            total_top5_acc += acc_top5\n",
    "            total_mean_pos += mean_pos\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_top1_acc = total_top1_acc / len(train_loader)\n",
    "        avg_top5_acc = total_top5_acc / len(train_loader)\n",
    "        avg_mean_pos = total_mean_pos / len(train_loader)\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        train_top1_accs.append(avg_top1_acc)\n",
    "        train_top5_accs.append(avg_top5_acc)\n",
    "        mean_positions.append(avg_mean_pos)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Top-1 Acc: {avg_top1_acc:.2f}% | \"\n",
    "              f\"Top-5 Acc: {avg_top5_acc:.2f}% | \"\n",
    "              f\"Mean Position: {avg_mean_pos:.2f}\")\n",
    "\n",
    "    return model, (train_losses, train_top1_accs, train_top5_accs, mean_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Image1: torch.Size([16, 1, 96, 96])\n",
      "  Image2: torch.Size([16, 1, 96, 96])\n",
      "training\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n",
      "Shape after conv1: torch.Size([10, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([10, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([10, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([10, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([10, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([10, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([10, 512])\n",
      "Shape after projection head: torch.Size([10, 96])\n",
      "Epoch 1/1 | Loss: 0.0115 | Top-1 Acc: 1.00% | Top-5 Acc: 1.00% | Mean Position: 1.00\n",
      "validating\n",
      "Shape after conv1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer1: torch.Size([32, 64, 24, 24])\n",
      "Shape after layer2: torch.Size([32, 128, 12, 12])\n",
      "Shape after layer3: torch.Size([32, 256, 6, 6])\n",
      "Shape after layer4: torch.Size([32, 512, 3, 3])\n",
      "Shape after avgpool: torch.Size([32, 512, 1, 1])\n",
      "Shape after flatten: torch.Size([32, 512])\n",
      "Shape after projection head: torch.Size([32, 96])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "size = 96\n",
    "\n",
    "image_dir = r\"../../tiff_experiment_unsupervised_data/combined\"\n",
    "dataset = ImageDataset(image_dir=image_dir,size=size)\n",
    "\n",
    "\n",
    "# Split the dataset with 20% for validation\n",
    "val_percentage = 0.2\n",
    "train_dataset, val_dataset = split_dataset(dataset, val_percentage)\n",
    "\n",
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          drop_last=False, \n",
    "                          pin_memory=True, \n",
    "                          num_workers=0) #num_workers=os.cpu count() using cluster gpu\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        drop_last=False, \n",
    "                        pin_memory=True, \n",
    "                        num_workers=0)\n",
    "\n",
    "for i, (image1, image2) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Image1: {image1.shape}\")\n",
    "    print(f\"  Image2: {image2.shape}\")\n",
    "    plt.imshow(image1[0,0])\n",
    "    break\n",
    "\n",
    "model = Resnet(size)\n",
    "#print(model)\n",
    "print('training')\n",
    "model, train_results = train(train_loader, model, epochs=1, device='cuda')\n",
    "print('validating')\n",
    "model, val_results = train(val_loader, model, epochs=1, device='cuda', validate=True)\n",
    "\n",
    "train_losses, train_top1_accs, train_top5_accs, train_mean_pos = train_results\n",
    "val_losses, val_top1_accs, val_top5_accs, val_mean_pos = val_results\n",
    "\n",
    "plot_curves(train_losses, val_losses, train_top1_accs, val_top1_accs, train_top5_accs, val_top5_accs, train_mean_pos, val_mean_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
