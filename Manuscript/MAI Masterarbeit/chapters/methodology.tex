\chapter{Methodology}\label{ch: Methodology}
The goal is to leverage representation learning of bright-field microscopy images to develop a ranking/ordering scale (1 to n) for these images. 




\section{Data preprocessing} \label{sec:data preprocessing}

Detailed study/research/experiments on data augmentation and image preprocessing techniques sepcifically for our 16 bit gray scale image are still need to be done.
Currently, as the focus is on creating the complete pipeline, the standard data augmentation combination (which showed high performance for SimCLR downstream tasks) from the SimCLR \cite{chen2020simple} paper is being used, as shown below.

\begin{enumerate}
  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform the following augmentations:
  \begin{enumerate}
      \item Apply a horizontal flip.
      \item Randomly crop the image and resize it to $96 \times 96$.
      \item Randomly change the brightness, contrast, saturation, and hue of the cropped patch.
  \end{enumerate}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\section{Data augmentation} \label{sec:data augmentation}

\section{Train SimCLR as SSL model}
For step 1, as explained in chapter \ref{ch: Methodology}, SimCLR was used as the first model for self-supervised learning (SSL). Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{subsec:data preprocessing} in the batch, resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Explore different data augmentation combinations by researching the best augmentations for medical grayscale images, and applying intuitive approaches beyond the standard SimCLR \cite{chen2020simple} data augmentation combinations as explained in section \ref{subsec:data preprocessing}.
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream task.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for the our downstream task.
  \item Supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for medical grayscale images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations). This resembles to triplet loss (not sure, need to be studied)  
\end{enumerate}
For variations 5 and 7 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{subsec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{subsec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}
\section{Ranking strategy 1: Using CAE}

\subsection{Day7 to day7 reconstruction}
I start with classical anomaly detection approach that we train a model to 
reconstruct/predict day 10 image from day 10 image exclusively on the untreated images. Since the day 10 prediction model is trained solely on untreated images,
 we expect the the inference loss/metric (i.e., the difference between the predicted and actual Day 10 image) will be very small for untreated images.
Conversely, the inference loss/metric will increase for treated images as their predictions deviate from those of untreated images.
 This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
 with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order.


\subsection{Day7 to day10 predcition}
\subsubsection{day10 predcition}
\begin{enumerate}
    \item \textbf{Step 1:} Create a latent space representation of all images, including untreated, clinically recommended, 
    and drug screening images, using SimCLR. 
    The idea is that SimCLR effectively learns efficient features of similar images that are not captured by 
    human-interpretable metrics. We expect the SimCLR feature vectors of similar images will be closer in the latent space. 
    In other words, feature vectors of similar images will be more linearly separable.
  
  \item \textbf{Step 2:} Train a prediction model exclusively on the representations of 
  untreated images from Day 7 to Day 10. ( Input: Day 7 feature vector and target: Day 10 feature vector )

  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include untreated, clinically recommended, and drug screening images.
  \item \textbf{Step 4:} Perform step 2 and step 3 on images instead of simclr feature vectors for comparitive study.
\end{enumerate}

Since the day 10 prediction model is trained solely on the representations of untreated images, the inference loss/metric 
(i.e., the difference between the predicted and actual Day 10 image representations) will be very small for untreated images.
 Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of untreated images.
This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. 
Determining a reasonable inference loss/metric will be one of the research problems to tackle.

\subsubsection{Delta predcition}
\begin{enumerate}
  \item \textbf{Step 1:} Calculate the difference/delta between the day 7 and day 10 simclr 
  feature vectors  exclusively on the untreated images.
  \item \textbf{Step 2:} Train a prediction model where input is day 7 feature vector 
  and target will be Delta.


  \item \textbf{Step 3:} Perform inference on the test set where input will be day 7 feature vector of all images which include untreated, clinically recommended, and drug screening images to predict the delta between the day 7 feature vector  and day 10 feature vector.
\end{enumerate}

Since the delta prediction model is trained solely on the representations of day 
7 untreated  images to predict the delta between the day 7 feature vector 
and day 10 feature vector of untreated images,
 the inference loss/metric (i.e., the difference between the predicted delta and actual
  delta) will be very small for untreated images. Conversely,
   the inference loss/metric will increase for treated images as their representations 
   deviate from those of untreated images. This inference loss/metric will be used as 
   the feature for the ranking/order scale, where the initial images will start with 
   untreated images that have very small inference loss/metric, and the scale will end 
   with images having high inference loss/metric in ascending order. 
   Determining a reasonable inference loss/metric will be one of the research problems 
   to tackle.



\section{Ranking strategy 2: K means centroid approach}

\begin{enumerate}
  \item \textbf{Step 1:} Feed control images into k means and find the centriod of control (untreated) cluster based on both cosine distance and the euclidean distance. ( we can choose the distacne metric if we have time)
  
  \item \textbf{Step 2:} calculate the euclidean/cosine distance from this centroid to every images.
  
  \item \textbf{Step 3:} Perform the same operation for simlcr features and on original images.
\end{enumerate}


\section{Intermediate evaluation of SSL model}

\subsubsection{classification}
\subsubsection{kmeans clustering}


**suppose I have a vector of image. if I normalise it unit length, willl I loose critical information?**

**https://chatgpt.com/share/671a6a63-ca7c-8010-92a0-23b6bd25ef05**

**if we do kmeans withcosine distance distance then it only compare cosine sim of whole image thats why we need to do normal kmeans with euclidean dist to show the magnitude similarity** 

**thats why we need to do instance segmantation and classifiacation to see if it able to learn more than magnitude similarity and cosine similarity like as texture, contrast, and brightness**

Evaluation of the SSL model depends on the  time series inference loss/accuracy metric, neverthless we can use other evaluation metrics, such as downstream task like classification. 

1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features. In other words, we use a single, linear layer that maps these representations to class predictions, where the two categories, 'untreated' and 'single dose,' serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data. We implemented a simple pipeline for a Logistic Regression setup, where the images are encoded into their feature vectors.

2. Baseline comparison: As a baseline for comparison  to our results above in the section \ref{sec:variations_implementations}, we will train a standard ResNet-18 with random initialization on the labeled training set, consisting of the 'untreated' and 'single dose' categories. The results will help us assess the advantages of contrastive learning on unlabeled data compared to purely supervised training. It is evident that ResNet-18 easily overfits the training data since its parameter count is over 1,000 times larger than the dataset size. To ensure a fair comparison with the contrastive learning models, we apply similar data augmentations as before, including crop-and-resize and color jittering.

