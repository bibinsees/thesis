\chapter{Methodology}\label{ch: Methodology}
The goal is to leverage representation learning of bright-field microscopy images to develop a ranking/ordering scale (1 to n) for these images. 
since we don't have ground truth labels to rank the images except control images. my strategy was to simplify the current ranking problem to only scale/order/rank images 
using only control, single dose and exploded images.
Thereason to pick these groups is that controls we know that there is no drug applied which means that there is no effect of drug at all. 
single dose images are the category which is clinically recommended at the moment (eventhough we don't know how much drug effected or howmmuch it killed the cancer) 
and exploded are visually exploded from the original cancer cell meansing we can visually see debris around the cancer cell potentially which may potentially harm the surrounding goof cells.
once if we can order those small subset of entire images, we can add the other image as inference to see where they plotted relative to control 
or single dose or exloded in this scale.




\section{Data preprocessing} \label{sec:data preprocessing}

Detailed study/research/experiments on data augmentation and image preprocessing techniques sepcifically for our 16 bit gray scale image are still need to be done.
Currently, as the focus is on creating the complete pipeline, the standard data augmentation combination (which showed high performance for SimCLR downstream tasks) from the SimCLR \cite{chen2020simple} paper is being used, as shown below.

\begin{enumerate}
  \item  We can't center crop it because for some of cancer cells are not centered in the image instead they are close to egdes. so we have to make 
  sure that when we crop it it should include the cacne cell fully. the solution is find the boundary of cancer cell and get the bounding box of cancer 
  cell then make crop to required size. original image width size is: 2456*2054 (H*W). crop the original image to have H = W. since the cancer cell debris
   spread across the width, we didn't change the width to include the debris,
   instead we reduced the hight to same size of width. hence we get H = W = 2054. if cancer cell is formed for instance, the cell it self spread acroos
    one diemsion that means its not valid cultvation by robot so we can digard it. ie maximum area of cancer cell should be included in this square 
    2054*2054 size image. advantage of croping like this are:
    \begin{enumerate}
      \item  remove unnecessary background which contains no information
      \item  there will be no shear during resize to 96*96 augmentation like in the figure.
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: croped to 2054*2054 then resized to 96*96: no shear change/elongation in one dimension happened, Second row: 
       original image 2456*2054 then resized to 96*96: shear change/elongation in one dimension happened}
        \label{fig:elong}
      \end{figure}
      
      \item rotation = 90 and 270, Horiflip+rotation 90, Horiflip+rotation270 this is only possible because of square image. if its other 
      angles except multiples of 90 then images will have black part for that we need additional careful interpolation or something like that.
       so my point is since the image is square we could take rotations of 90 mulitiples without any additional tasks.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/rotation.png} 
        \caption{A: croped to 2054*2054 then resized to 96*96: No black cuts. B: 
        original image 2456*2054 then resized to 96*96: black cuts happened}
        \label{fig:rotation}
      \end{figure}
       \item \textcolor{red}{should I do rotation after resie to 96 or not? check} 
    \end{enumerate}

  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform the following augmentations:
  \begin{enumerate}
      \item Apply a horizontal flip.
      \item Randomly crop the image and resize it to $96 \times 96$. i choosed this small H and W as image size to feed train our model because of two reasons: 1. 
      computational fast, so that we can complete the pipeline in time. 2. if we can get good performance in ranking with this small image sizes (ie less pixel details
       compared to 2054 * 2056 ) that means we can improve the performance with larger image sizes.
      \item Randomly change the brightness, contrast, saturation, and hue of the cropped patch.
  \end{enumerate}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\section{Data augmentation} \label{sec:data augmentation}
\textcolor{red}{add what kind of rotations and flips are possible without repetition image} 
I started with data augmentation just like simclr paper did that is strong data augmentation, just to get 
to play around , beacuse it was easy to do , since I can directly adapt code 
from tutorial just to have complete pipeline.

I found that those strong augmentations transformed to distribution out of 
the original distribution like we see in the figures below. specifically color jitterness including brightness, 
contrast, hue, saturation.

Interestingly, for 16-bit images with 3 channels, instead of a reduction, there was an increase in the number of unique pixel valuesâ€”by a maximum of 258,757 percentage. 
The issue with this increase is that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift to
 the two extremes,
 such as 0 or 1, or sometimes pushing values to both 0 and 1, which deviate significantly from the original image distribution, as shown in Figures
  \ref{fig:16bit_three_v1} and \ref{fig:16bit_three_v2}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
    \item Original Image - Minimum pixel value: 0.13064774870872498, Maximum pixel value: 0.6874189376831055
    \item Augmented Image - Minimum pixel value: 0.022128667682409286, Maximum pixel value: 0.11041323840618134
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v1}
  \end{figure}
  
  \textbf{Another example of a 16-bit three-channel image before and after data augmentation:}
  
  \begin{itemize}
    \item Original Image - Unique pixel counts per channel: 2137
    \item Augmented Image - Unique pixel counts per channel: 1686717
    \item Original Image - Minimum pixel value: 0.1306, Maximum pixel value: 0.6874
    \item Augmented Image -  Minimum pixel value: 0.1970, Maximum pixel value: 0.3748
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bithree2.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v2}
  \end{figure}
  
  \textbf{16-bit single-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 1058
    \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
    \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_onen.png} 
    \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
    \label{fig:16bit_single_channel}
  \end{figure}

  $49\%$ maximum reduction for 16-bit single-channel data augmentation with color jitter is also not ideal, as it diminishes the gradual spread of 
  darker regions 
as happened in original image, as  as observed in figure \ref{fig:16bit_single_channel}. One potential solution is to experiment with specific parameters
 within the color 
jitter transform instead of using random values, ensuring that the reduction in the number of unique pixel values does not exceed, for example, $30\%$. 
Another 
option would be to write a custom Python function, depending on the available time. Other augmentations from PyTorch work fine in this experiment.

then I removed contrast, saturation and hue. because if I tried to make 
those augs as invariant that means I'm making control and treated as 
similar because if I increase or decrease contrast or 
saturation or hue it increase/decrease the darkness of cancer cell to 
have similarity.
which doesn't make sense because we want exact opposite of it.

but if I use the same croping percentage as the original simclr does, 
that means I crop 0.1  to 1 percentage which doesn't make sense because that 
menas we make smilarity to small background to cancer which is unnecessary. 
so i chaneg the percentage to 0.4-1.

Questionable below one:

lasty it may be possible to learn efficient feature extraction by not 
applying cropping, ie learn by seeing full picture (big picture). 
I understand this is not good argument because then dog and cat have same 
color and maybe similar shape but still learns to differentiate using simclr.
But maybe for time predictoin ranking model it helps. because model have to 
predict the change from day 7 to day 10. and most of the ime the size/shape
 changes. especially to the category which are exploded, they produce debris. 
 so maybe data augmentation seeing the time change. 





\section{Train SimCLR as SSL model}
For step 1, as explained in chapter \ref{ch: Methodology}, SimCLR was used as the first model for self-supervised learning (SSL). 
Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Explore different data augmentation combinations by researching the best augmentations for medical grayscale images, and applying intuitive approaches beyond the standard SimCLR \cite{chen2020simple} data augmentation combinations as explained in section \ref{sec:data preprocessing}.
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream task.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for the our downstream task.
  \item Supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for medical grayscale images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations). This resembles to triplet loss (not sure, need to be studied)  
\end{enumerate}
For variations 5 and 7 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}
\section{Ranking strategy 1: Using CAE}

\subsection{Day7 to day7 reconstruction}
I start with classical anomaly detection approach that we train a model to 
reconstruct/predict day 10 image from day 10 image exclusively on the untreated images. Since the day 10 prediction model is trained solely on untreated images,
 we expect the the inference loss/metric (i.e., the difference between the predicted and actual Day 10 image) will be very small for untreated images.
Conversely, the inference loss/metric will increase for treated images as their predictions deviate from those of untreated images.
 This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
 with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order.


\subsection{Day7 to day10 predcition}
\subsubsection{day10 predcition}
\begin{enumerate}
    \item \textbf{Step 1:} Create a latent space representation of all images, including untreated, clinically recommended, 
    and drug screening images, using SimCLR. 
    The idea is that SimCLR effectively learns efficient features of similar images that are not captured by 
    human-interpretable metrics. We expect the SimCLR feature vectors of similar images will be closer in the latent space. 
    In other words, feature vectors of similar images will be more linearly separable.
  
  \item \textbf{Step 2:} Train a prediction model exclusively on the representations of 
  untreated images from Day 7 to Day 10. ( Input: Day 7 feature vector and target: Day 10 feature vector )

  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include untreated, clinically recommended, and drug screening images.
  \item \textbf{Step 4:} Perform step 2 and step 3 on images instead of simclr feature vectors for comparitive study.
\end{enumerate}

Since the day 10 prediction model is trained solely on the representations of untreated images, the inference loss/metric 
(i.e., the difference between the predicted and actual Day 10 image representations) will be very small for untreated images.
 Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of untreated images.
This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. 
Determining a reasonable inference loss/metric will be one of the research problems to tackle.

\subsubsection{Delta predcition}
\begin{enumerate}
  \item \textbf{Step 1:} Calculate the difference/delta between the day 7 and day 10 simclr 
  feature vectors  exclusively on the untreated images.
  \item \textbf{Step 2:} Train a prediction model where input is day 7 feature vector 
  and target will be Delta.


  \item \textbf{Step 3:} Perform inference on the test set where input will be day 7 feature vector of all images which include untreated, 
  clinically recommended, and drug screening images to predict the delta between the day 7 feature vector  and day 10 feature vector.
\end{enumerate}

Since the delta prediction model is trained solely on the representations of day 
7 untreated  images to predict the delta between the day 7 feature vector 
and day 10 feature vector of untreated images,
 the inference loss/metric (i.e., the difference between the predicted delta and actual
  delta) will be very small for untreated images. Conversely,
   the inference loss/metric will increase for treated images as their representations 
   deviate from those of untreated images. This inference loss/metric will be used as 
   the feature for the ranking/order scale, where the initial images will start with 
   untreated images that have very small inference loss/metric, and the scale will end 
   with images having high inference loss/metric in ascending order. 
   Determining a reasonable inference loss/metric will be one of the research problems 
   to tackle.



\section{Ranking strategy 2: K means centroid approach}

\begin{enumerate}
  \item \textbf{Step 1:} Feed control images into k means and find the centriod of control (untreated) cluster based on both cosine distance and the euclidean distance. 
  ( we can choose the distacne metric if we have time)
  
  \item \textbf{Step 2:} calculate the euclidean/cosine distance from this centroid to every images.
  
  \item \textbf{Step 3:} Perform the same operation for simlcr features and on original images.
\end{enumerate}

\section{Ranking strategy 3: Softmax approach}
1. Train classification model to classify untreated and treated. 
2. Then do inference on them and take softmax probability as metric for ranking.

\section{Intermediate evaluation of SSL model}

**if we do kmeans withcosine distance distance then it only compare cosine sim of whole image thats why we need to do normal kmeans with euclidean dist to show 
the magnitude similarity** 

Evaluation of the SSL model depends on the  time series inference loss/accuracy metric, neverthless we can use other evaluation metrics, such as downstream task 
like classification. 

**suppose I have a vector of image. if I normalise it unit length, willl I loose critical information?**
**https://chatgpt.com/share/671a6a63-ca7c-8010-92a0-23b6bd25ef05**

\subsubsection{classification}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single, linear layer that maps these representations to class predictions, where the two categories, 'untreated' and 'single dose,' 
 serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. 
 Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
  We implemented a simple pipeline for a Logistic Regression setup, where the images are encoded into their feature vectors.

2. Baseline comparison: As a baseline for comparison  to our results above in the section \ref{sec:variations_implementations}, we will train a standard ResNet-18 with
 random initialization on the labeled training set, consisting of the 'untreated' and 'single dose' categories. The results will help us assess the advantages of contrastive 
 learning on unlabeled data compared to purely supervised training. It is evident that ResNet-18 easily overfits the training data since its parameter count is over 1,000 times 
 larger than the dataset size. To ensure a fair comparison with the contrastive learning models, we apply similar data augmentations as before, including crop-and-resize 
 and color jittering.
\subsubsection{kmeans clustering}
Idea is whether the learned representation from simlcr outperforms the original images in clustering the images (unsupervised manner) For that we use simple kmeans clusetring.
We will cluster them based on both euclidean distance as well as cosine distance.

Derivation of K-Means Clustering using Euclidean Distance and Mean

Objective Function
The k-means algorithm aims to minimize the total squared Euclidean distance between data points and their assigned cluster centroids. The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): The \( i \)-th data point,
    \item \( \boldsymbol{\mu}_k \): The centroid of the \( k \)-th cluster,
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\subsection*{Cluster Assignment Step}
For a fixed set of centroids \( \{ \boldsymbol{\mu}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest centroid. This minimizes:

\[
r_{ik} =
\begin{cases} 
1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
0, & \text{otherwise.}
\end{cases}
\]

\subsection*{Centroid Update Step}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \boldsymbol{\mu}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

Focus on a single cluster \( k \). The term involving \( \boldsymbol{\mu}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
= \sum_{i=1}^{n} r_{ik} \left( \mathbf{x}_i^\top \mathbf{x}_i - 2 \mathbf{x}_i^\top \boldsymbol{\mu}_k + \boldsymbol{\mu}_k^\top \boldsymbol{\mu}_k \right)
\]

Take the derivative with respect to \( \boldsymbol{\mu}_k \) and set it to zero:

\[
\frac{\partial}{\partial \boldsymbol{\mu}_k} \sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2 =
-2 \sum_{i=1}^{n} r_{ik} \mathbf{x}_i + 2 \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k = 0
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k
\]

Factor out \( \boldsymbol{\mu}_k \):

\[
\boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the points in cluster \( k \).

\subsection*{Algorithm Summary}
The k-means algorithm alternates between the following two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the nearest cluster:
    \[
    r_{ik} =
    \begin{cases} 
    1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
    0, & \text{otherwise.}
    \end{cases}
    \]
    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the mean of its assigned points:
    \[
    \boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
    \]
\end{enumerate}

This iterative process continues until the assignments \( r_{ik} \) and centroids \( \boldsymbol{\mu}_k \) no longer change or the change is below a threshold.

\subsection*{Cosine distance}

\section*{Normalization}

To calculate the cosine distance, we first \textbf{normalize} all data points and centroids. Suppose the normalized data points and centroids are \( \mathbf{x}_i \) and \( \mathbf{c}_k \), respectively, then:

\[
\|\mathbf{x}_i\| = 1 \quad \text{and} \quad \|\mathbf{c}_k\| = 1.
\]

This ensures all vectors are on the unit sphere. The cosine similarity between two vectors \( \mathbf{x}_i \) and \( \mathbf{c}_k \) is defined as:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|}
\]

Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we substitute these values into the equation:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{1 \times 1}
\]

This simplifies to:

\[
\text{cosine similarity} = \mathbf{x}_i^\top \mathbf{c}_k.
\]

Thus, the \textbf{cosine distance} becomes:
\[
\text{cosine distance} = 1 - \mathbf{x}_i^\top \mathbf{c}_k.
\]

\section*{Relating Cosine Distance to Euclidean Distance}

For normalized vectors, we derive the relationship between \textbf{Euclidean distance} and \textbf{cosine distance}. The squared Euclidean distance between a data point \( \mathbf{x}_i \) and a centroid \( \mathbf{c}_k \) is:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \sum_{j} (x_{ij} - c_{kj})^2.
\]
Expanding this:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we get:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Simplify:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k).
\]
Thus, for normalized vectors, the Euclidean distance is proportional to the cosine distance:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2 \cdot \text{cosine distance}.
\]
Rearranging to express the cosine distance:
\[
\text{cosine distance} = \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]




\section*{Objective Function}

The k-means algorithm with cosine distance aims to minimize the cosine distance between data points \( \mathbf{x}_i \) and their assigned cluster centroids \( \mathbf{c}_k \). The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|} \right),
\]
where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): \( i \)-th data point,
    \item \( \mathbf{c}_k \): Centroid of cluster \( k \) (normalized to unit length),
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\section*{Objective Function in Terms of Euclidean Distance}

Using the above result, the k-means objective function with cosine distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]
can be rewritten in terms of Euclidean distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]
Here, the factor \( \frac{1}{2} \) accounts for the scaling difference.

\section*{Cluster Assignment Step}

For a fixed set of centroids \( \{ \mathbf{c}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest cluster based on the \textbf{cosine similarity} (or equivalently, minimize cosine distance):
\[
r_{ik} =
\begin{cases}
1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
0, & \text{otherwise.}
\end{cases}
\]
\subsection*{Centroid Update Step for Cosine Distance}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \mathbf{c}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
\]

Focus on a single cluster \( k \). The term involving \( \mathbf{c}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
= \sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Since the vectors are normalized, \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we have:

\[
\sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Now, take the derivative of the above with respect to \( \mathbf{c}_k \) and set it to zero:

\[
\frac{\partial}{\partial \mathbf{c}_k} \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Factor out \( \mathbf{c}_k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the normalized points in cluster \( k \).

\section*{Centroid Update Step}

For a fixed cluster assignment \( \{ r_{ik} \} \), update the centroids \( \{ \mathbf{c}_k \} \) as the \textbf{normalized mean} of all data points assigned to cluster \( k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
\]

\section*{Algorithm Summary}

The k-means algorithm with cosine distance alternates between two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the cluster with the highest cosine similarity:
    \[
    r_{ik} =
    \begin{cases}
    1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
    0, & \text{otherwise.}
    \end{cases}
    \]

    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the normalized mean of its assigned points:
    \[
    \mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
    \]
\end{enumerate}

\section*{Conclusion}

By normalizing the data points and centroids, the k-means clustering objective can be expressed in terms of both cosine distance and Euclidean distance. The equivalence:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k)
\]
enables seamless interpretation and implementation in the algorithm.








\subsubsection{Direct day 7 to day 10 distance evaluation}
calculate distance between day 7 untreated and its corresponding day 10 treated one.
Idea is:
1. it should give same distance between day 7 and its corresponding day 10
 even its changed in position/flipped. ie checking whether simlcr learned to 
 be invariant to position error in microscope error because of manual 
 handling/transporting.
2. it sould give same distance even if its blured/sharpened
3. it should give sma edistance even if its changed in brightness
4. main test for k means centroid approach. it should give different distance to single dose and exploded.

