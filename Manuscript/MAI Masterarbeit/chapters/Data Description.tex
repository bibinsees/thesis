\chapter{Data Description}\label{ch: DataDescription}
\section{Data set}
\label{sec:Data set}
As explained in the chapter \ref{ch:intro}, the dataset consists of images taken by bright-field microscopy, which operate as follows: 

The sample is illuminated by transmitted white light (i.e., light is projected from below and observed from above).
 The contrast in the image is created due to the reduction in light intensity as it passes through denser regions of the sample, where more light is
  absorbed or scattered. This results in a typical bright-field microscopy image where the sample appears darker against a bright background, giving 
the technique its name. In our case, the 3D tumor model appears as a dark grayish structure on a bright background. 
For simplicity, bright-field microscopy images will be referred to as 'images'.

 Since the samples are cultivated using robotic arms, the process sometimes fails to replicate the natural shapes and patterns that typically occur when laboratory personnel manually cultivate the samples. To ensure the 
 dataset's quality and consistency, Dalia pre-processed the images through a machine learning model to filter out invalid ones. By 'invalid images' , we mean
  those that variates from the expected morphology of the tumor tissues as cultivated by lab personnel. These images are typically elongated either in height or width as shown in second image 
 of figure \ref{fig:valid}. I collected these filtered images via USB and Google Drive in their original TIFF format. 

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/finevalid.png} 
    \caption{invalid vs valid}
    \label{fig:valid}
  \end{figure}

  The original images are 2456x2054 pixels in size, in 16-bit grayscale, and consist of multiple layers. These layers are obtained by capturing images
   at different focal planes in brightfield microscopy. The number of layers can vary, as images can be taken at any number of focal planes. The sharpness 
   of each layer of an image depends on how well the focal plane of the microscope aligns with the depth of the sample. Only one layer of this 3D tumor tissue model will 
   be perfectly in focus, while others may appear blurry because they are slightly above or below the focal point as you can see in figure \ref{fig:blur}. Combining these focal planes later in 
   computational analysis can provide richer data, even if some layers are blurry individually. This is one of the reasons why I decided to use multiple
    layers. The images I received from the lab mostly have three layers, while a few have one or five layers.

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.4]{figures/blur.png} 
      \caption{invalid vs valid}
      \label{fig:blur}
    \end{figure}

Each image in the dataset consists of different layers captured at varying focal lengths. For example, if Image 1 has layers corresponding to focal
 lengths A, B, and C, Image 2 will also have layers corresponding to the same focal lengths A, B, and C, as all images within the same experiment are 
 captured using consistent acquisition settings. However, for images from different experiments, the focal lengths may differ. For instance, images from 
 Experiment 1 (single dose) may have slightly different acquisition settings compared to images from Experiment 2 (drug screening), leading to variations 
 in the focal lengths and corresponding layers. 
 Secondly, the layers in each image are slightly misaligned when stacked, as shown in the figure  \ref{fig:posi}. This misalignment may or may not affect the performance of ranking.

 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/posi.png} 
  \caption{Misalighned}
  \label{fig:posi}
\end{figure}

We have some time noise as in figure in the images that happens during the process of whole experiment. It could happen due to the mishandling of the
 environmental settings.
 \textcolor{red}{talk about noise that we ignored as our limitation of work}
  
 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/noise.png} 
  \caption{Noise}
  \label{fig:noise}
\end{figure}
 
\textcolor{red}{organised folders}


Figure \ref{fig:Transition} illustrates that, even with the application of the same drug at the same concentration, the morphology of 3D tumor tissues changes differently.

\textcolor{red}{ show drug screen with same drug concentration we have diferent effect between different experiment. this maybe due to environmental 
factors or anything}


\textcolor{red}{rewrite intro lab to include all type explained in tabel below}

The table below shows the division of different types of image datasets we have, as  explained in the section ~\ref{sec:lab-setup}.
\begin{table}[ht!]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{|l|c|c|c|c|c|c|}
  \hline
  \textbf{Class} & \textbf{Drug Screened} & \textbf{Single Dose} & \textbf{Cond 10} & \textbf{Untreated} & \textbf{Day 8 \& 9} & \textbf{Total} \\ \hline
  \textbf{No. of Images (\%)}  & 200 (15.94\%) & 103 (8.25\%) & 201 (16\%) & 510 (40.67\%) & 240 (19.14\%) & 1254 \\ \hline
  \end{tabular}%
  }
  \caption{Dataset Class Overview}
  \label{tab:dataset}
\end{table}



An 8-bit image encompasses 256 color tones (ranging from 0 to 255) per channel, whereas a 16-bit image accommodates 65,536 color tones 
(ranging from 0 to 65,535) per channel, in our case 65,536 shades of gray. Retaining the original 16-bit depth is crucial for two primary reasons:
\begin{enumerate}
  \item Converting it to an 8-bit image for faster and more efficient computation can lead to significant information loss in intensity details.
Since 8-bit images only allow 256 possible values, the finer variations in intensity that are present in 16-bit images become compressed as illusstrated in \ref{fig:8bitvs16bit}.
For example, two distinct values in 16-bit (such as 30,000 to 30,048 ) could map to the same 8-bit value (for instance, both might be mapped to 117).
This results in the loss of subtle intensity differences, which can be crucial in our image task, where minute variations in intensity can be indicative
 of important features such as gradual trasnsition of dark color frfom center to border or amount of debris surrounded to the tumor cell.


 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bitvs16bit.png} 
  \caption{8-bit vs 16 bit data lose comparison}
  \label{fig:8bitvs16bit}
\end{figure}

\item During data augmentation processes that involve substantial alterations in brightness, contrast, or color, an 8-bit image—already limited to 256 tones—could lose up to 50 percentage of these tones, leaving only 128 levels of color and tone. This reduction can lead to "banding," where areas with smooth transitions in tone exhibit visible stripes with jagged edges. In contrast, a 16-bit image, even with a 50 percentage reduction in tones, would retain over 32,000 levels. This higher tonal range allows for smoother transitions, better edge preservation, and enhanced accuracy in color and hue representation. As a result, the dynamic range—the difference between the lightest and darkest areas of the image—remains much more effectively preserved in 16-bit images than in 8-bit images.
\end{enumerate}
In our case, the maximum reduction in unique pixel values for the 8-bit images, regardless number of channels was found to be 99.27 percentage after 3000 epochs of random
color jitter applied  using \texttt{torch.transforms.RandomApply([transform.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0)], p=1)} 
as shown in figure \ref{fig:8bit_nThree} and \ref{fig:8bit_n one}, whereas for the 16-bit single-channel images (where one sharp layer was extracted from all three layers and considered as input for data augmentation), the reduction in unique pixel values was only 49 percentage after 3,000 epochs of random color jitter, as shown in Figure \ref{fig:16bit_n one}.
\textcolor{red}{Hence its interesting to experiment the entire methods using single channel as future work, also because single channel remove the 
problem of stacking inconsistant layers}



\textbf{8-bit three-channel image example before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 2
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nThree.png} 
  \caption{8-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 97.81\%}
  \label{fig:8bit_nThree}
\end{figure}

\textbf{8-bit single-channel (sharp layer) image before and after data augmentation:}

\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 3, Maximum pixel value: 3
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nOne.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 99.27\%}
  \label{fig:8bit_n one}
\end{figure}

\textbf{16-bit single-channel image before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 2111
  \item Number of unique pixel values in the augmented image: 1058
  \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bit_onen.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
  \label{fig:16bit_n one}
\end{figure}

\section{Challenges}
\begin{enumerate}
    \item \textbf{Limited Dataset for Day 7 to Day 10 ranking prediction Model:} The dataset for predicting outcomes between Day 7 and Day 10 is limited, which poses a challenge for training and evaluation.

    \item \textbf{Issues with Day 10 Images:}
    \begin{enumerate}
        \item \textbf{Image Variations:} Day 10 images can exhibit variations such as flipping, blurring, brightness changes, and position changes from the original position of tumot tissue 
        cell in the image. 
        The position change occurs because, when the well plate is brought outside and then placed back under the microscope for taking day 10 image, 
        the position often shifts. This relative position of the tumor cell in the image can shift from the center to other directions.
         While a 'center crop' explained in in Section~\ref{sec:data preprocessing} approach can address this issue to some extent, it fails when the tumor cell 
         is located at the edge of the image. To deal this issue for some extend, Applying horizontal and vertical flips, rotations by $90^\circ$ and $270^\circ$, as 
        well as combinations like horizontal flip + rotation $90^\circ$ and horizontal flip + rotation $270^\circ$, can help make the model invariant to position changes.
    \end{enumerate}

    \item \textbf{Variability in Drug Effects on Day 10:}
    \begin{enumerate}
        \item \textbf{Effect Differences:} The same drug can have different effects on Day 10. For instance, there is a huge variation between DS 61 G6 and 
        41 GP6, while less variation is observed between RBTDS 4.1 and 4.2 GP3. Hence we can't assess the drug efficacy based on the drug combination instead we need to make the ranking relative 
        assessment based on the day 10 images.
        \item \textbf{Debris Amount:} Images from different groups with significant debris were visually selected. Since the number of such images was small, no specific code was implemented for this selection process.
    \end{enumerate}
\end{enumerate}


 

1.3 same drug can have different effect on day 10 (example: huge variation in: ds 61 g6 and 41 gp6. less variation: 
RBTDS 4.1 and 4.2 gp 3) initially i thought I will be able to use 
as drug gp to evaluate, because of this difference, the difference can be due to different patient cancer cell?
I choosed images from different gps which have huge debris amount visually. it was few so I didn't code my self.
