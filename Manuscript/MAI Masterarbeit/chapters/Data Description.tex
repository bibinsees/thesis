\chapter{Data Description}\label{ch: DataDescription}
\section{Data set}
\label{sec:Data set}
As explained in the chapter \ref{ch:intro}, the dataset consists of images taken by bright-field microscopy, which operate as follows: 

The sample is illuminated by transmitted white light (i.e., light is projected from below and observed from above).
 The contrast in the image is created due to the reduction in light intensity as it passes through denser regions of the sample, where more light is
  absorbed or scattered. This results in a typical bright-field microscopy image where the sample appears darker against a bright background, giving 
the technique its name. In our case, the 3D tumor model appears as a dark grayish structure on a bright background. 
For simplicity, bright-field microscopy images will be referred to as 'images'.

 Since the samples are cultivated using robotic arms, the process sometimes fails to replicate the natural shapes and patterns that typically occur when laboratory personnel manually cultivate the samples. To ensure the 
 dataset's quality and consistency, Dalia pre-processed the images through a machine learning model to filter out invalid ones. By 'invalid images' , we mean
  those that variates from the expected morphology of the tumor tissues as cultivated by lab personnel. These images are typically elongated either in height or width as shown in second image 
 of figure \ref{fig:valid}. I collected these filtered images via USB and Google Drive in their original TIFF format. 

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/finevalid.png} 
    \caption{invalid vs valid}
    \label{fig:valid}
  \end{figure}

  The original images are 2456x2054 pixels in size, in 16-bit grayscale, and consist of multiple layers. These layers are obtained by capturing images
   at different focal planes in brightfield microscopy. The number of layers can vary, as images can be taken at any number of focal planes. The sharpness 
   of each layer of an image depends on how well the focal plane of the microscope aligns with the depth of the sample. Only one layer of this 3D tumor tissue model will 
   be perfectly in focus, while others may appear blurry because they are slightly above or below the focal point as you can see in figure \ref{fig:blur}. Combining these focal planes later in 
   computational analysis can provide richer data, even if some layers are blurry individually. This is one of the reasons why I decided to use multiple
    layers. The images I received from the lab mostly have three layers, while a few have one or five layers.

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.4]{figures/blur.png} 
      \caption{invalid vs valid}
      \label{fig:blur}
    \end{figure}

Each image in the dataset consists of different layers captured at varying focal lengths. For example, if Image 1 has layers corresponding to focal
 lengths A, B, and C, Image 2 will also have layers corresponding to the same focal lengths A, B, and C, as all images within the same experiment are 
 captured using consistent acquisition settings. However, for images from different experiments, the focal lengths may differ. For instance, images from 
 Experiment 1 (single dose) may have slightly different acquisition settings compared to images from Experiment 2 (drug screening), leading to variations 
 in the focal lengths and corresponding layers. 
 Secondly, the layers in each image are slightly misaligned when stacked, as shown in the figure  \ref{fig:posi}. This misalignment may or may not affect the performance of ranking.

 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/posi.png} 
  \caption{invalid vs valid}
  \label{fig:posi}
\end{figure}

We have some time noise as in figure in the images that happens during the process of whole experiment. It could happen due to the mishandling of the
 environmental settings.
 \textcolor{red}{talk about noise that we ignored as our limitation of work}
  
 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/noise.png} 
  \caption{invalid vs valid}
  \label{fig:noise}
\end{figure}
 

Figure \ref{fig:Transition} illustrates that, even with the application of the same drug at the same concentration, the morphology of 3D tumor tissues changes differently.

\textcolor{red}{correct the figure. show drug screen with same drug concentration we have diferent effect between different experiment. this maybe due to environmental 
factors or anything}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/transition.png} 
  \caption{C06, F11 and G04 are well names in the well plate.}
  \label{fig:Transition}
\end{figure}



The table below shows the division of three different types of image datasets, as  explained in the section ~\ref{sec:lab-setup}.
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
  \hline
  \textbf{Class} & \textbf{Drug Screened} & \textbf{Single Dose} & \textbf{Untreated} & \textbf{Total} \\ \hline
  \textbf{No. of Images (\%)}  & 12 (3\%) & 204 (60\%) & 150 (37\%) & 366 \\ \hline
  \end{tabular}
  \caption{Dataset Class Overview}
  \label{tab:dataset}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/originals.png} 
  \caption{Three different types of images: Drug Screened, Single Dose, and Untreated as mentioned in section ~\ref{sec:lab-setup}.}
  \label{fig:originals}
\end{figure}

An 8-bit image encompasses 256 color tones (ranging from 0 to 255) per channel, whereas a 16-bit image accommodates 65,536 color tones 
(ranging from 0 to 65,535) per channel, in our case 65,536 shades of gray. Retaining the original 16-bit depth is crucial for two primary reasons:
\begin{enumerate}
  \item Converting it to an 8-bit image for faster and more efficient computation can lead to significant information loss in intensity details.
Since 8-bit images only allow 256 possible values, the finer variations in intensity that are present in 16-bit images become compressed as illusstrated in \ref{fig:8bitvs16bit}.
For example, two distinct values in 16-bit (such as 30,000 to 30,048 ) could map to the same 8-bit value (for instance, both might be mapped to 117).
This results in the loss of subtle intensity differences, which can be crucial in our image task, where minute variations in intensity can be indicative
 of important features such as gradual trasnsition of dark color frfom center to border or amount of debris surrounded to the tumor cell.


 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bitvs16bit.png} 
  \caption{8-bit vs 16 bit data lose comparison}
  \label{fig:8bitvs16bit}
\end{figure}

\item During data augmentation processes that involve substantial alterations in brightness, contrast, or color, an 8-bit image—already limited to 256 tones—could lose up to 50 percentage of these tones, leaving only 128 levels of color and tone. This reduction can lead to "banding," where areas with smooth transitions in tone exhibit visible stripes with jagged edges. In contrast, a 16-bit image, even with a 50 percentage reduction in tones, would retain over 32,000 levels. This higher tonal range allows for smoother transitions, better edge preservation, and enhanced accuracy in color and hue representation. As a result, the dynamic range—the difference between the lightest and darkest areas of the image—remains much more effectively preserved in 16-bit images than in 8-bit images.
\end{enumerate}
In our case, the maximum reduction in unique pixel values for the 8-bit images, regardless number of channels was found to be 99.27 percentage after 3000 epochs of random
color jitter applied  using \texttt{torch.transforms.RandomApply([transform.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0)], p=1)} 
as shown in figure \ref{fig:8bit_nThree} and \ref{fig:8bit_n one}, whereas for the 16-bit single-channel images (where one sharp layer was extracted from all three layers and considered as input for data augmentation), the reduction in unique pixel values was only 49 percentage after 3,000 epochs of random color jitter, as shown in Figure \ref{fig:16bit_n one}.
\textcolor{red}{Hence its interesting to experiment the entire methods using single channel as future work, also because single channel remove the 
problem of stacking inconsistant layers}



\textbf{8-bit three-channel image example before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 2
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nThree.png} 
  \caption{8-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 97.81\%}
  \label{fig:8bit_nThree}
\end{figure}

\textbf{8-bit single-channel (sharp layer) image before and after data augmentation:}

\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 3, Maximum pixel value: 3
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nOne.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 99.27\%}
  \label{fig:8bit_n one}
\end{figure}

\textbf{16-bit single-channel image before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 2111
  \item Number of unique pixel values in the augmented image: 1058
  \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bit_onen.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
  \label{fig:16bit_n one}
\end{figure}



1. challenges:
1.1 limited data set for day 7 to day 10 prediction model
1.2 day 10  image can be flipped, blured, brightness change, position change (position cahnge is due to because when we take day 10 images we have to bring the
 well plate outside and sometimes
 when we put that into microscope the position is already changed). Relativly small position of tumor cell in the image from center to other directions change  can 
be resolved for some extend using center crop approach, but if the cell is in the edge then center crop won't help. i didn't do any data
 preprocessing
 specifically for that problem. but I expect the croping and resizing to 96*96 will be a solution. because 
 when we do that 
 data transformation, augmented pairs will be positioned differently.  \textcolor{red}{only state if you can show the proof} 
 horizontal, vertical, rotation = 90 and 270, Horiflip+rotation 90,
  Horiflip+rotation270 
 augs also helps to make invariate to position changes.
 

1.3 same drug can have different effect on day 10 (example: huge variation in: ds 61 g6 and 41 gp6. less variation: 
RBTDS 4.1 and 4.2 gp 3) initially i thought I will be able to use 
as drug gp to evaluate, because of this difference, the difference can be due to different patient cancer cell?
I choosed images from different gps which have huge debris amount visually. it was few so I didn't code my self.
