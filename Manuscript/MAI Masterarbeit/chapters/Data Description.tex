\chapter{Data Description}\label{ch: Data Description}
\section{Data set}
\label{sec:Data set}

As we explained in introduction chapter, dataset consist of bright-field microscopy images which 
basically works by: 
Sample illumination is transmitted (i.e., illuminated from below and observed from above) 
white light, and contrast
in the sample is caused by attenuation of the transmitted light in dense areas of the sample. The
 typical appearance 
of a bright-field microscopy image is a dark sample on a bright background, hence the name.
In our case, we have   3d tumor model as dark grayish color in the bright background.

1. challenges:
1.1 limited data set for day 7 to day 10 prediction model
1.2 day 10  image can be flipped, blured, brightness change, position change (position cahnge is due to because when we take day 10 images we have to bring the
 well plate outside and sometimes
 when we put that into microscope the position is already changed). Relativly small position of tumor cell in the image from center to other directions change  can 
be resolved for some extend using center crop approach, but if the cell is in the edge then center crop won't help. i didn't do any data
 preprocessing
 specifically for that problem. but I expect the croping and resizing to 96*96 will be a solution. because 
 when we do that 
 data transformation, augmented pairs will be positioned differently. 
 
 \textcolor{red}{only state if you can show the proof}
1.3 same drug can have different effect on day 10 (example: huge variation in: ds 61 g6 and 41 gp6. less variation: 
RBTDS 4.1 and 4.2 gp 3) initially i thought I will be able to use 
as drug gp to evaluate, because of this difference, the difference can be due to different patient cancer cell?
I choosed images from different gps which have huge debris amount visually. it was few so I didn't code my self.

The original images are approximately 2500×2500 pixels in size, in 16-bit grayscale,
 and consist of multiple channels. These channels come from taking images at different 
 focal planes in brightfield microscopy. The number of channels can vary, as you can take
  images at any number of focal planes. the sharpness of an image is dependent 
  on how well the focal plane of the microscope aligns with the depth of the sample. Only one focal plane
   (layer) will be perfectly in focus, while others might appear blurry because they are slightly above 
   or below the focal point. Combining these focal planes later in computational analysis can provide richer data,
    even if some channels are blurry individually. thats why I initially decided to use multiple channels.
  Images I got from the lab consisit of majority have 3 channels while minorly as very few have one channel and 
  5 cahnnels.

  \textcolor{red}{add the details of no of channels/image and how does it calcualtes the sharpness intuiton behind it: https://chatgpt.com/share/675f440d-3868-8010-ae82-ad6cdca13c5d last explanation}

  However, for the ease of use to integrate with our pretarined architecture we use (both resnet18 and unet),
  I determined to start with 3 channels per image. 
what matters is  most sharped because 
  most sharpest channels have more less texture/edge information information than less sharpness 
 less information (less texture/edge information) 
  how to calculate the sharpness and why does it make sense explained in data preprocess section.

  also one thing to point out is that the   \textcolor{red}{depth of focus or focal length amoung the channels can be different/same in different mages ? ask Dalia}
  \textcolor{red}{when I observed the channels/layers in each image, there is a slight gradual shift (visually) in the position amoung them. ie, if the first channel
   have the tumor model in the center, if look at the last channel we can see 0.1 mm shift in random direction.} 

  the images with one channel will duplicate the channels to make upto 3.
  
 
  investigate the performance of single channel too because maybe thats enough for us too to able to get our expected result.
  how to choose the focused layer will be explained the data preprocessing section.
  \textcolor{red}{correct the figure.take small patch and show the difference in edge effect. but neverthless we can't 
  give show the focused one in whole image? check for image like that maybe you will find in 5 layers or maybeyou will find in 3 layers}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/threes.png} 
  \caption{Illustration of three layers per image: A, B, and C. The three layers looks visually similar, with slight differences in focal planes. In this figure, A is the sharpest/focused layer.}
  \label{fig:Threes}
\end{figure}

Figure \ref{fig:Transition} illustrates that, even with the application of the same drug at the same concentration, the morphology of 3D tumor tissues changes differently.

\textcolor{red}{correct the figure. show drug screen with same drug concentration we have diferent effect between different experiment. this maybe due to environmental 
factors or anything}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/transition.png} 
  \caption{C06, F11 and G04 are well names in the well plate.}
  \label{fig:Transition}
\end{figure}



The table below shows the division of three different types of image datasets, as  explained in the section ~\ref{sec:lab-setup}.
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
  \hline
  \textbf{Class} & \textbf{Drug Screened} & \textbf{Single Dose} & \textbf{Untreated} & \textbf{Total} \\ \hline
  \textbf{No. of Images (\%)}  & 12 (3\%) & 204 (60\%) & 150 (37\%) & 366 \\ \hline
  \end{tabular}
  \caption{Dataset Class Overview}
  \label{tab:dataset}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/originals.png} 
  \caption{Three different types of images: Drug Screened, Single Dose, and Untreated as mentioned in section ~\ref{sec:lab-setup}.}
  \label{fig:originals}
\end{figure}

An 8-bit image encompasses 256 color tones (ranging from 0 to 255) per channel, whereas a 16-bit image accommodates 65,536 color tones 
(ranging from 0 to 65,535) per channel, in our case 65,536 shades of gray. Retaining the original 16-bit depth is crucial for two primary reasons:
\begin{enumerate}
  \item Converting it to an 8-bit image for faster and more efficient computation can lead to significant information loss in intensity details.
Since 8-bit images only allow 256 possible values, the finer variations in intensity that are present in 16-bit images become compressed as illusstrated in \ref{fig:8bitvs16bit}.
For example, two distinct values in 16-bit (such as 30,000 to 30,048 ) could map to the same 8-bit value (for instance, both might be mapped to 117).
This results in the loss of subtle intensity differences, which can be crucial in our image task, where minute variations in intensity can be indicative
 of important features such as gradual trasnsition of dark color frfom center to border or amount of debris surrounded to the tumor cell.


 \begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bitvs16bit.png} 
  \caption{8-bit vs 16 bit data lose comparison}
  \label{fig:8bitvs16bit}
\end{figure}

\item During data augmentation processes that involve substantial alterations in brightness, contrast, or color, an 8-bit image—already limited to 256 tones—could lose up to 50 percentage of these tones, leaving only 128 levels of color and tone. This reduction can lead to "banding," where areas with smooth transitions in tone exhibit visible stripes with jagged edges. In contrast, a 16-bit image, even with a 50 percentage reduction in tones, would retain over 32,000 levels. This higher tonal range allows for smoother transitions, better edge preservation, and enhanced accuracy in color and hue representation. As a result, the dynamic range—the difference between the lightest and darkest areas of the image—remains much more effectively preserved in 16-bit images than in 8-bit images.
\end{enumerate}
In our case, the maximum reduction in unique pixel values for the 8-bit images, regardless number of channels was found to be 99.27 percentage after 3000 epochs of random
color jitter applied  using \texttt{torch.transforms.RandomApply([transform.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0)], p=1)} 
as shown in figure \ref{fig:8bit_nThree} and \ref{fig:8bit_n one}, whereas for the 16-bit single-channel images (where one sharp layer was extracted from all three layers and considered as input for data augmentation), the reduction in unique pixel values was only 49 percentage after 3,000 epochs of random color jitter, as shown in Figure \ref{fig:16bit_n one}.



\textbf{8-bit three-channel image example before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 2
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nThree.png} 
  \caption{8-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 97.81\%}
  \label{fig:8bit_nThree}
\end{figure}

\textbf{8-bit single-channel (sharp layer) image before and after data augmentation:}

\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 3, Maximum pixel value: 3
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nOne.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 99.27\%}
  \label{fig:8bit_n one}
\end{figure}

\textbf{16-bit single-channel image before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 2111
  \item Number of unique pixel values in the augmented image: 1058
  \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bit_onen.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
  \label{fig:16bit_n one}
\end{figure}


