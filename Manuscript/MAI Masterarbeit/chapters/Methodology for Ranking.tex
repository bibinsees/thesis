\chapter{Methodology for Ranking}\label{ch:Methodology for Ranking}
 
Since we don't have ground truth labels to rank the images except control images (since control images doesn't applied by drug, drug efficacy is basically zero).
My strategy was to simplify the current ranking problem to only scale/order/rank images relative to only control, single dose and explode images. What I mean by ordering relative to these specified classes means for example, while ordering drug screen images it shows maybe first control look alike drug screen images then single dose look alike drug screen images then comes explode images. 
The reason to pick these specific classes for relative positioning is that controls: we know that there is no drug applied which means that there is no effect of drug at all. 
single dose images are the category which we apply specific drug concentration and its clinically recommended at the moment as the most effective generally (even though we don't know how much drug effected or how much it killed the tumor tissue model) 
and explode are visually see debris around the tumor tissue model which may potentially harm the surrounding good tissues.
once if we can order those control, single dose and explode image using ranking strategy, we can add the  inference images to see where they positioned in the scale of control or single dose or explode images. Finally if we can order control , single dose and exploded images in an scale using any of the ranking methodologies thats gonna explain below, we use the same methodology to order all of the drug screening images, we expect it will follow an order of images  where drug screening images which are similar to control comes first in the scale then single dose and then exploded images comes next in the scale.

\section*{Ranking customized accuracy calculation}
Accuracy formulation: Goal is to check whether we have clear separation of each class by the inference metric as explained in literature review \ref{{ch:Literature Review}} while using as it as a scale order. Accuracy below formulated to check whether the first and third classes are well-separated from the middle class. there by it also penalizes if first and third classes are overladed each other. If its clearly separated from the middle class means all 3 classes  are well separated and accuracy will be 100.

Detailed explanation of the accuracy calculation:

Once we obtain the inference metric for each individual feature vector in each class, the next steps involve calculating the mean inference metric for each class, identifying the middle class based on the mean values of inference metric, and proceeding with the remaining computations as described below.

\textbf{Task 1: Mean of Each Class}

The mean inference metric for each class \( C_i \) is calculated as:

\[
\mu_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} M(\mathbf{x}, \mathbf{c})
\]

where \( |C_i| \) represents the number of points in class \( C_i \), and \( M(\mathbf{x}, \mathbf{c}) \) is the inference metric between a point \( \mathbf{x} \) and the centroid \( \mathbf{c} \).

\textbf{Task 2: Middle Class Based on Mean}

To determine the middle class, sort the mean values \( \mu_1, \mu_2, \mu_3 \) in ascending order:

\[
\mu_{\text{sorted}} = \{\mu_{\text{min}}, \mu_{\text{mid}}, \mu_{\text{max}}\}
\]

The middle class is the class corresponding to \( \mu_{\text{mid}} \).

\textbf{Task 3: Minimum and Maximum of Middle Class}

For the middle class (denoted as \( C_{\text{mid}} \)), compute the minimum and maximum inference metric values:

\[
\text{middle\_class\_min} = \min_{\mathbf{x} \in C_{\text{mid}}} M(\mathbf{x}, \mathbf{c})
\]
\[
\text{middle\_class\_max} = \max_{\mathbf{x} \in C_{\text{mid}}} M(\mathbf{x}, \mathbf{c})
\]

\textbf{Task 4: Error and Accuracy Calculation}

1. \textbf{Error Calculation}: Count the number of points in the first class \( C_1 \) that exceed the middle class's minimum, and the number of points in the third class \( C_3 \) that are less than the middle class's maximum. The total error is given by:

\[
\text{error}_1 = \left| \{\mathbf{x} \in C_1 : M(\mathbf{x}, \mathbf{c}) > \text{middle\_class\_min} \} \right|
\]
\[
\text{error}_3 = \left| \{\mathbf{x} \in C_3 : M(\mathbf{x}, \mathbf{c}) < \text{middle\_class\_max} \} \right|
\]
\[
\text{total\_errors} = \text{error}_1 + \text{error}_3
\]

2. \textbf{Accuracy Calculation}: Subtract the total errors from the total number of points across all classes to compute the number of non-errors. Divide this by the total number of points to calculate the accuracy:

\[
\text{accuracy} = \frac{\text{total\_points} - \text{total\_errors}}{\text{total\_points}}
\]

where:
\[
\text{total\_points} = |C_1| + |C_2| + |C_3|
\]

\textbf{Summary of Steps}
1. Compute \( \mu_1, \mu_2, \mu_3 \).
2. Identify the middle class using \( \mu_{\text{mid}} \).
3. Compute \( \text{middle\_class\_min} \) and \( \text{middle\_class\_max} \).
4. Count the errors \( \text{error}_1 \) and \( \text{error}_3 \).
5. Calculate accuracy using the formula above.

\section{Day 7 to day 10 prediction using Convolutional Autoencoder} \label{subsec:day7-to-day10}
\begin{enumerate}
    \item \textbf{Step 1:} Create a latent space representation of all images, including control, single dose, 
    and drug screen images, using SimCLR. 
    The idea is that SimCLR effectively learns efficient features of similar images that are not captured by 
    human-interpretable metrics. We expect the SimCLR feature vectors of similar images will be closer in the latent space. 
  
  \item \textbf{Step 2:} Train a prediction model exclusively on the representations of 
  control images from Day 7 to Day 10 using convolutional autoencoder. ( Input: Day 7 SimCLR feature vector and target: Day 10 SimCLR feature vector )

  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include control, single dose, and drug screen images.
  Since the day 10 prediction model is trained solely on the representations of control images, the inference loss/metric 
  (i.e., the difference between the predicted and actual Day 10 image representations) will be very small for control images.
   Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of control images.
  This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
  with control images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. 
  
  \item \textbf{Step 4:} so in the above methods we train first solely on control images then we did inference on all images just like classical anomaly detection approach. 
  with that same idea/concept, but now we considered other classes as normal and will try to find the deviation from that transition. that is for instance we train solely on day 7 untreated to day 10 single dose images then we do the inference on all images so that inference loss will be how much it deviated from the single dose 
  images. repeating the same concept, we train solely on day 7 untreated to day 10 explode images then we do the inference on all images so that inference loss will be how much it deviated from the explode.
  \item Perform the above steps on original image features instead of SimCLR feature vectors for comparative study.
\end{enumerate}

\textbf{Lack of dataset problem for day 7 to day 10 images:}
Due to environmental factors sometimes we may get images of day 7  but we won't able to acquire  day 10 images or sometimes we may have day 10 images but not corresponding day 7 images. Hence for the images for prediction from day 7 to day 10, we only have 130 control images of pair transitions. 29 images of pair for single dose and 40 image pairs for explode. 

\subsubsection*{CAE Prediction model for original images}
Data preparation: I decided to train with day 7 control to day 10 control images.Normalized the images by dividing them by 65535. reduced the size to 96*96. Added data augmentations such as horizontal/vertical flips and rotations and random brightness/sharpness/blur as explained in the sweet augmentation section  except contrast and cropping. We can't change contrast  as I explained in the augmentation chapter,and also i didn't use cropping because of the idea that in order to learn whole change or transformation to day 10, it needs to see the whole day 7 image which is already explained in the data augmentation resize chapter \ref{sec:data augmentation}. I made sure that:
\begin{enumerate}
  \item Augmentation to be coupled, meaning the same data augmentation type parameter value is used for both day 7 and corresponding day 10 images.
  \item Flips or rotation will be unique to each image with random brightness/sharpness/blur change. This means that in the dataset, the original image can have different augmentations with the same parameter value of brightness/sharpness/blur since it's random, but it won't have the same geometrical transformation.
\end{enumerate}


The augmentation helps to increase the possible sample distribution that could happen in real life. By that we are trying to find a solution for data deficiency and also helps to prevent overfilling. also it helps to be invariant of the brightness/position/blur/sharp changes that can happen due to the microscope mishandling. So now final dataset consist of all class having 650 pair of transitions.


\textbf{Training parameters and archiecture:}

Convolutional Autoencoder Architecture:
We developed an autoencoder architecture with an encoder-decoder structure, utilizing convolutional layers, batch normalization, dropout, and activation functions
 to learn feature representations and predict day 10 image from day 7. The number of feature maps is progressively increased and then symmetrically decreased to balance
  feature extraction and construction of day 10 same size image as day 7.   
  \textbf{Encoder}
  The encoder consists of three convolutional layers with a kernel size of $3 \times 3$ and \texttt{same} padding to maintain spatial resolution.  
  \begin{itemize}
      \item Input channels begin with 3, progressively increasing to 16, 32, and 64 to extract deeper hierarchical features.  
      \item Each convolutional layer is followed by \texttt{BatchNorm2d} for stable learning, \texttt{ReLU} activation for non-linearity, and dropout with probabilities of 0.2, 0.3, and 0.4, respectively, to mitigate overfitting.  
      \item Downsampling is performed using \texttt{MaxPool2d} with a kernel size of $2 \times 2$, stride of 2, and padding to reduce spatial dimensions at each step.  
  \end{itemize}
  
  \textbf{Decoder}
  The decoder mirrors the encoder, reconstructing the input from the learned features.  
  \begin{itemize}
      \item It begins with 64 channels and symmetrically decreases the number of channels to 32, 16, and finally 3 to match the input dimensions.  
      \item Each convolutional layer retains a kernel size of $3 \times 3$ with \texttt{same} padding, followed by \texttt{BatchNorm2d}, \texttt{ReLU} activation, and dropout with probabilities of 0.3 and 0.2, respectively.  
      \item Upsampling is performed using \texttt{Upsample(scale\_factor=2, mode='nearest')} to restore spatial dimensions step by step.  
      \item The final layer includes a \texttt{Sigmoid} activation function to ensure output pixel values are in the range $[0, 1]$, making the model suitable for 
      day 10 image construction.  
  \end{itemize}
  

The overall structure employs progressive channel expansion in the encoder (\(3 \to 16 \to 32 \to 64\)) to capture detailed feature hierarchies and symmetric 
channel reduction in the decoder (\(64 \to 32 \to 16 \to 3\)) for day 10 image construction. 
training parameters: bacth size = 32. learning rate = 0.001. optimizer = adam. loss = mse. epochs = 500. 


\textbf{Result:}
Unfortunately, the model didn't learn the features of transition from day 7 to day 10 well. the loss didn't decrease in the 500 epochs as we see in the figure \ref{fig:uloss}.  Figure reflects that
model struggles to predict the day 10 images from day 7 image, the predicted image is more resemblance visually to day 7 than day 10. For the model its hard to learn, probably because in most cases as we see in figure \ref{fig:mispo} the day 10 image tumor tissue model is totally changed in 
shape or size or  pixel intensity. Also from the same figure shows, it could be the positional change due to the microscope handling position. ( We used different flip and rotation 
augmentation but it seems like it mayn't be effective as we think it is.) Since the same problem we may have with the day 7 to day 10 single dose or explode images, I decided to move to
SimCLR features to see if it can learn the transition better than the original images. 
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/rankloss.png} 
  \caption{Train vs test loss for original image vectors using CAE prediction model from day 7 to day 10}
  \label{fig:uloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/unpred.png} 
  \caption{Train vs test loss on original image vectors using CAE prediction model from day 7 to day 10}
  \label{fig:unloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/pproblem.png} 
  \caption{Each pair of day 7 to day 10 transition shows changes in size/shape etc. last pair of transition shows mis position of the cancer cell. In the day 7 it was above the middle line of the image. and in day 10 its below of the middle line of the image}
  \label{fig:mispo}
\end{figure}

\subsubsection*{Simclr features}
Set up of data processing same as above original setup.
first I started with the pair of day 7 untreated to predict the day 10 control images setup. Normalized images by dividing them by 65535. resized to 96*96. then
I increased the dataset using same data augmentation technics that I did to original image setup as explained in the above section but with made sure that flips or rotation will be unique to each image with random brightness/sharpness/blur change within a range as above. then I feed to simclr model 
to get features after and before projection head to train the CAE model. 

\textbf{Training setup:} First to ensure the simclr features are scaled I did featurewise minmax scaling. Then I feed those into FeaturePredictor model which is a fully 
connected neural network. It consists of a series of 12 linear layers, where the input size reduces from 512 to 8 and then symmetrically increases back to 512 to
 match the original feature dimensions for before projection head features. For after projection head features,consists of a series of 6 linear layers, where the 
 input size reduces from 20 to 4 and then symmetrically increases back to 20.

Each intermediate layer is followed by:

Batch normalization (BatchNorm1d): Normalizes activations to stabilize training and accelerate convergence.
ReLU activation: Introduces non-linearity to model complex feature relationships.
Dropout (Dropout): Applied with probabilities ranging from 0.2 to 0.4 to prevent overfitting, with deeper layers using higher dropout rates for regularization.
The final layer is a linear transformation to ensure the output feature size matches the day 10 feature vector dimensions which is 512 for before and 20 for after.

batch size = 32. loss = mse loss. optimizer = adam optimizer. learning rate = 0.0001. no of epochs = 2000. used cross validation with 3 fold and  early stop with 
the parameters patience = 500 and delta = 0.0001. training data and validation data divided as 80: 20 ratio.

As we see in the figure below \ref{fig:unloss}, for the training of day 7 to day 10 control images and single dose, the train loss and validation loss decreased gradually  in the 2000 epochs. But for the category of considering single dose as normal ie training  day 7 to day 10 single dose images we can see overfitting behavior in the loss. This is probably due to the fact that for training day 7 to day 10 single dose images we have only 29 images as original. This is the problem of data deficiency. We have tried to mitigate this problem by increasing the dataset size by data augmentation but nevertheless for the explode images training didn't quite worked out.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/overfited.png} 
  \caption{Training loss Vs test loss for each class as normal (ie the class is only used in training). Single dose dataset shows overfitting }
  \label{fig:unloss}
\end{figure}

We calculated inference loss/metric for both  before and after projection head feats and used following metrics as inference loss between target feature vector and predicted feature vector: mse, cosine distance, L2 ( euclidean distance ), L1 distance, Pearson correlation, dot product,jaccard similarity, hamming distance.

The reason to choose these metrics are the metrics mainly used in medical images as explained in literature review.

\textbf{Results:}
 As we can see the figure \ref{fig:bargraph}. Some of the inference metric are able to make a clear separation of the training class to others ie talent one class is have clear separation from other classes as we seen in figure. and some of the metric couldn't make a clear separation for atlas one class ie all of the classes are mixed as we seen in the figure for dot product and jaccard similarity. We observed this same trend  for both before and after projection head features and for all transition category. Ideal situation that we would like to have is all of the classes are separated from each other. so that when we do inference on drug screen images there will be look alike smooth transition from class to other.

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 65.93      & 67.22     & 66.55      & 66.39                  & 66.60                 \\
  Before          & Control        & 50.41      & 68.10     & 53.15      & 66.29                  & \textbf{77.51}                 \\
                  & Explod         & 65.93      & 75.59     & 67.58      & 67.48                  & 68.61                 \\ \midrule
                  & Single Dose    & 62.15      & 65.41     & 61.38      & 51.81                  & 60.39                 \\
  After           & Control        & 35.42      & 38.16    & 33.82      & 35.26                  & 39.66                 \\
                  & Explod         & 48.40      & 64.32     & 61.01      & 36.40                  & 51.96                 \\ \bottomrule
  \end{tabular}
  \caption{Cosine distance as inference metric}
  \label{tab:table_cosine}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.24      & 66.60     & 66.39      & 66.13                  & 66.60                 \\
  Before          & Control        & 52.95      & 67.11     & 55.79      & 66.03                  & \textbf{77.61}                 \\
                  & Explod         & 70.84      & 74.35     & 64.74      & 69.70                  & 71.98                 \\ \midrule
                  & Single Dose    & 64.12      & 68.87     & 64.01      & 47.57                  & 55.12                 \\
  After           & Control        & 35.68      & 36.61     & 33.66      & 35.32                  & 36.71                 \\
                  & Explod         & 51.50      & 56.41     & 37.80      & 44.98                  & 59.98                 \\ \bottomrule
  \end{tabular}
  \caption{Euclidean distance as inference metric}
  \label{tab:table_eucli}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.24      & 66.60     & 66.39      & 66.13                  & 66.60                 \\
  Before          & Control        & 52.95      & 67.11     & 55.79      & 66.03                  & \textbf{77.61}                 \\
                  & Explod         & 70.84      & 74.35    & 64.74      & 69.70                  & 71.98                 \\ \midrule
                  & Single Dose    & 64.12      & 68.87     & 64.01      & 47.57                  & 55.12                 \\
  After           & Control        & 35.68      & 36.61     & 43.80      & 35.32                  & 36.71                 \\
                  & Explod         & 51.50      & 56.41     & 61.48      & 44.98                  & 59.98                 \\ \bottomrule
  \end{tabular}
  \caption{MSE distance as inference metric}
  \label{tab:table_mse}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 65.01      & 66.60     & 66.39      & 65.05                  & 66.60                 \\
  Before          & Control        & 53.83      & 66.75     &  51.34     & 66.03                  & 72.29                 \\
                  & Explod         & 65.15      & 68.20     & 62.56      & 64.79                  & \textbf{76.53}                 \\ \midrule
                  & Single Dose    & 62.36      & 66.13     & 57.91      & 42.24                  & 51.55                 \\
  After           & Control        & 35.52      & 37.44     & 33.61      & 34.64                  & 36.81                 \\
                  & Explod         & 48.71      & 52.12     & 59.10      & 41.68                  & 56.10                 \\ \bottomrule
  \end{tabular}
  \caption{L1 distance as inference metric}
  \label{tab:table_l}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.34      & 66.39     & 66.55      & 66.13                  & 66.44                 \\
  Before          & Control        & 50.26      & 68.51     & 60.34      & 66.24                  & \textbf{75.65}                 \\
                  & Explod         & 64.68      & 67.01     & 65.67      & 65.51                  & 67.01                 \\ \midrule
                  & Single Dose    & 48.50      & 64.68     & 48.24      & 50.52                  & 62.15                 \\
  After           & Control        & 47.78      & 43.02     & 44.26      & 34.49                  & 51.14                 \\
                  & Explod         & 45.04      & 67.11     & 59.26      & 47.21                  & 53.15                 \\ \bottomrule
  \end{tabular}
  \caption{Pearson correlation as inference metric}
  \label{tab:table_pear}
\end{table}


\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 33.61      & 34.13     & 33.40      & 34.23                  & \textbf{53.93}                 \\
  Before          & Control        & 39.97      & 49.28     & 34.90      & 40.12                  & 47.10                 \\
                  & Explode         & 37.75      & 35.32     & 34.85      & 44.93                  & 32.99                 \\ \midrule
                  & Single Dose    & 36.81      & 46.38     & 47.00      & 39.71                  & 34.75                 \\
  After           & Control        & 33.30      & 38.99     & 34.23      & 34.28                  & 33.66                 \\
                  & Explode         & 34.18      & 45.45     & 33.87      & 36.19                  & 33.61                 \\ \bottomrule
  \end{tabular}
  \caption{Dot product as inference metric}
  \label{tab:table_dot}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 55.69      & 66.65     & 48.55      & 62.72                  & 66.44                 \\
  Before          & Control        & 49.95      & 68.46     & 61.89      & 68.67                  & \textbf{69.29}                 \\
                  & Explod         & 54.34      & 66.18     & 53.83      & 67.32                  & 66.91                 \\ \midrule
                  & Single Dose    & 41.78      & 51.50     & 34.33      & 37.59                  & 50.47                 \\
  After           & Control        & 42.14      & 37.18     & 34.33      & 33.09                  & 36.30                 \\
                  & Explod         & 39.14      & 50.78     & 42.55      & 35.47                  & 41.47                 \\ \bottomrule
  \end{tabular}
  \caption{Jaccard similarity as inference metric}
  \label{tab:table_jac}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 61.84      & 66.60     & 62.67      & 59.20                  & 66.44                 \\
  Before          & Control        & 34.80      & 60.24     & 48.60      & 64.84                  & 63.70                 \\
                  & Explod         & 62.82      & 72.60     & 56.36      & 64.06                  & \textbf{67.22}                 \\ \midrule
                  & Single Dose    & 41.73      & 46.74     & 35.94      & 48.55                  & 56.77                 \\
  After           & Control        & 45.76      & 42.97     & 34.33      & 33.92                  & 36.76                 \\
                  & Explod         & 45.24      & 55.17     & 43.07      & 43.07                  & 45.04                \\ \bottomrule
  \end{tabular}
  \caption{Hamming  distance as inference metric}
  \label{tab:table_hammi}
\end{table}
From the above tables, \ref{tab:table_hammi},  \ref{tab:table_jac} ,  \ref{tab:table_dot} ,  \ref{tab:table_pear} ,  \ref{tab:table_l} ,  \ref{tab:table_eucli} ,  \ref{tab:table_mse} and \ref{tab:table_cosine}, 
we can see that cosine distance, euclidean distance, mse, L1 distance and  Pearson correlation coefficient are able to separate the classes moderately comparing to dot product as inference metric is separating the classes least among all metrics which make it as the worst metric for ranking.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5, angle=270]{figures/predall.pdf} 
  \caption{Ranking scale for before projection head SimCLR  features while control class used as normal ie control class only used for training}
  \label{fig:bargraph}
\end{figure}


Since we couldn't separate those 3 classes well that means we won't have good ranking of drug screen images since sd close images and explode images will be mixed while ranking. this calls the need for other ranking strategy such as softmax approach, K means centroid approach, dimensionality reduction technics. where for K means centroid , dimensionality reduction technics, small no of data will not effect that much for the performance of ranking. since there is no training required.

\section{Ranking strategy 2: Softmax approach}

This strategy utilizes the before projection head SimCLR vectors since its linearly separable for downstream task classification as suggested in original SimCLR paper \cite{chen2020simple}.

We are using only before projection head SimCLR features since original images or after projection head SimCLR features are not able to classify 100 percentage as we seen in intermediate evaluation chapter \ref{ch:Methodology for Intermediate evaluation of SimCLR model}

\begin{enumerate}
  \item Train a classification model to classify control and single dose images.
  \item Since we attained 100\% accuracy for the train and test datasets for this binary classification, if we look at the softmax score of all the single dose feature vectors to classify as the single dose class, the values will be equal to or really close to 1. Similarly, if we look at the softmax score of all the control feature vectors to classify as the single dose class, the values will be equal to or really close to 0.
  \item Now, when we do inference with these learned weights and parameters on the same control and single dose features, we will get the softmax value of all the single dose feature vectors to classify as the single dose class. The values will be equal to or really close to 1. Similarly, if we look at the softmax score of all the control feature vectors to classify as the single dose class, the values will be equal to or really close to 0.
  \item Now, we add an explode class in addition to the control and single dose classes in inference. The idea is that since we didn't use the explode class in training, the softmax score of all the explode feature vectors to classify as the single dose class will not be exactly or closer to 1. It can be below the values of single dose class features, which means it can overlap with the values of control features. But what if those explode feature softmax values did not overlap with the values of control? If we use the softmax score to predict the single dose class as an inference metric for ranking, in the range of our ranking scale, control class features come first as the values are closer or equal to 0, then explode class features come next in the scale in between 0 and 1, and single dose class features come last as the values are closer to 1. This is the ideal situation that we would like to have. If we get this kind of ranking scale, we can say that our softmax score is able to separate control, explode, and single dose classes, and we can use the same customized ranking accuracy calculation as we used in the previous ranking strategies.
  \item Use these same steps to classify between control and explode, so that we expect those class softmax values will be at both ends (0 and 1), and the single dose class softmax values will be in between control and explode class values. Likewise, classification between explode and single dose, where control feature softmax values will be in between explode and single dose class softmax values.
\end{enumerate}


Initialy I used original dataset of control (472 images) and single dose images (103 images) as explained in the dataset table and we used explode ( 40 images) 
I train with different parameters for  number of epochs and batch size. following parameters gave higher accuracy: 750 epochs, batch:16. Every other parameter is same as explained in the classification section in the chapter \ref{ch:Methodology for Intermediate evaluation of SimCLR model}.


\begin{table}[H]
  \centering
  \begin{tabular}{@{}lccccc@{}}
  \toprule
  \textbf{Classification} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
  Control vs SD             & 25.04            & 49.75           & 76.91          & 85.85                         & 90                        \\
  Control vs Ex             & 83.41            & 49.27               & 95.93           & 60.49                          &    71.54                 \\
  Ex vs SD                & 97.07           & 96.75          & 93.5           & 87.64                       & 87.31                        \\ \bottomrule
  \end{tabular}
  \caption{Ranking accuracy on inference using all the dataset we have for each class using SimCLR features and}
  \label{tab:ranking_softmax}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}lccc@{}}
  \toprule
  \textbf{Classification}     & \textbf{Control vs SD} & \textbf{Control vs Ex} & \textbf{Ex vs SD} \\ \midrule
  \textbf{Accuracy (\%)}      & 45.53                  & 83.41                  & 21.62             \\ \bottomrule
  \end{tabular}
  \caption{Ranking accuracy on inference using all the dataset we have for each class using original image features}
  \label{tab:classification_accuracies}
  \end{table}
  
As we seen in the above tables \ref{tab:ranking_softmax} and \ref{tab:classification_accuracies}, none of the  data augmentation types are not able to give clear separation between 3 classes. No contrast sweet, resize and sweet data augmentation data types achieved highest accuracy 90 and 95.12 and 97.07  in the category of  Control vs SD and Control vs Ex and Ex vs Sd categories correspondingly. Before head projection SimCLR features still performed comparatively well comparing to original image vectors.

The main limitation of this approach is we can't guarantee when we get new images it will sail separate three classes 100 percentage like we seen above, there can be overlaps, since the classification only promise us that the softmax score of the control will be closer or equal to zero and  single dose will be closer or equal to 1 in control vs sd case. that means classification only promise us that we get clear separation between 2 classes not three classes based on the softmax value.



\section{Ranking strategy 3: K Means approach}

Unlike other 2 strategies this strategy doesn't effect the class imbalance problem since there is no specific training required.

\begin{enumerate}
  \item \textbf{Step 1:} Feed control images SimCLR features into K-means and find the centroid of control cluster based on both cosine distance and the euclidean distance using k-means clustering. 
 
  
  \item \textbf{Step 2:} Calculate the euclidean/cosine distance from this centroid to every simclr features.
  \item \textbf{Step 3:} Rank the images based on the distance from the centroid of control simclr features.
  \item \textbf{Step 4:} Perform the above ranking procedure from centroid of single dose image simclr features to other image features and from centroid of explode image simclr features to other image features correspondingly.
  
  \item \textbf{Step 5:} Perform the same operation  on original images.
\end{enumerate}
Dataset we used : We used all of the control images (472),
single dose image (103) and exploded images (40).

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & 93.33      & 93.50     & 48.23      & 92.52                  & 93.82                 \\
  Before          & Control Mean       & 78.86      & 87.64     & 91.49      & 85.53                  & 92.20                 \\
                  & Explode Mean        & 83.25      & 25.53     & 82.74      & 29.43                  & 82.76                 \\ \midrule
                  & Single Dose Mean   & \textbf{95.12}      & 85.69     & 28.29      & 77.72                  & 83.74                 \\
  After           & Control Mean       & 71.54      & 76.75     & 82.76      & 14.47                  & 20.16                 \\
                  & Explode Mean        & 85.37      & 80.00     & 37.72      & 78.21                  & 80.81                 \\ \bottomrule
  \end{tabular}
  \caption{Performance of K-means centroid approach based on cosine distance as inference metric using SimCLR features}
  \label{tab:table_label}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/allbox.pdf} 
  \caption{Selected inference metric scales for ranking accuracy using SimCLR features. Strong after projection head feature, using single dose as centroid , got best for ranking accuracy while cosine distance as inference metric showed in the first plot and Strong before  projection head feature, using single dose as centroid , got best for ranking accuracy while euclidean distance as inference metric as shown in the second plot}
  \label{fig:cosi_mean}
\end{figure}



\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & \textbf{92.52}      & 90.41     & 31.71      & 83.58                  &  91.38                \\
  Before          & Control Mean       & 69.11      & 37.24     & 76.75      & 39.84                  & 57.89                 \\
                  & Explode Mean        & 82.44      & 82.60    & 67.97      & 23.90                  & 82.28                 \\ \midrule
                  & Single Dose Mean   & 90.57     & 80.33     & 36.42      & 76.75                  & 21.63                 \\
  After           & Control Mean       & 55.70      & 76.75     & 76.75      & 76.75                  & 77.07                 \\
                  & Explode Mean        & 83.09      & 77.40     & 55.93      & 19.84                  &  81.63                \\ \bottomrule
  \end{tabular}
  \caption{Performance of K-means centroid approach based on euclidean distance as inference metric using SimCLR features}
  \label{tab:eucli_mean}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llll@{}}
  \toprule
  Distance From & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Single Dose\\ Mean\end{tabular}} & \multicolumn{1}{c}{Control Mean} & \multicolumn{1}{c}{Explod Mean} \\ \midrule
  Cosine        & 17.56                                                                           & 46.50                            & 28.45                           \\
  Euclidean     & 24.227                                                                           & 50.08                            & 34.47                           \\ \bottomrule
  \end{tabular}
  \caption{Performance of K-means centroid approach on original images}
  \label{tab:orig_mean}
\end{table}

Insights from the above tables \ref{fig:cosi_mean}, \ref{fig:eucli_mean} and \ref{fig:orig_mean} and figure \ref{fig:cosi_mean}: Cosine distance of each after projection head strong simclr feature from the single dose mean have the most separation of classes with accuracy of 95.12. It doesn't separate three classes fully but its something to hope for. With the euclidean distance, before projection head strong simclr feature from the single dose mean have the most separation of classes with accuracy of 92.52. It clearly  separates one class from other but still far away from the result of 3 class separation that we hoped for.
All of the above inference metric scaling  I did it on original image vector and as you can see below performance of the ranking accuracy using cosine and euclidean distance were quite low comparing to the simclr feature vectors.Which basically tells us for this ranking strategy also simclr features are better than original image vectors.



\section{ Ranking strategy 4: Dimensionality reduction technics}



PCA  aka Principal component analysis can be defined as the orthogonal projection of the data onto lower dimensional linear space, known as the principle subspace, such that variance of the projected data is maximized, equally it can defined as the linear projection that minimizes the average projection cost,defines as the mean squared distance between the datapoint and their projection \cite{bishop:2006:PRML}.  PCA is less effective when the intrinsic data structure is highly nonlinear.

t-SNE: In contrast, t-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear technique designed specifically for visualizing high-dimensional data by preserving local neighborhood structures. t-SNE works by converting pairwise distances in high-dimensional space into probabilities that represent similarities, then optimizing a low-dimensional embedding to reflect these local relationships accurately. While t-SNE excels at revealing clusters and subtle local structures that PCA might miss, it is computationally intensive and sometimes struggles to preserve the global structure of the data. These characteristics can limit its effectiveness as a ranking tool when global relationships are important, even though it provides superior visualization of complex patterns.

UMAP: Uniform Manifold Approximation and Projection (UMAP) is another nonlinear dimensionality reduction method that builds on principles from manifold learning and topology to capture both local and global data structures. UMAP tends to be faster than t-SNE and often preserves more of the global structure, making it a promising alternative when a balanced view of local detail and overall data distribution is needed. However, like t-SNE, UMAP is sensitive to hyperparameter tuning and the inherent structure of the data, which can influence its performance.

We will use the PCA technique to identify the direction of maximum variance in the high dimensional feature space in our case 512 D for before projection head feature simclr feature vector, and 20 D for after projection head simclr feature vector and 27648 D for original images and projects data onto this single axis (1D). The resulting First PCA scores (positions along this axis) aim to preserve maximum variance as possible from the original data while reducing redundancy and noise.  Idea is that whether this 1D PCA value can be used as as our ranking metric or not. Similarly we use other dimensionality reduction techniques such as  t-SNE (t-Distributed Stochastic Neighbor Embedding)  and UMAP (Uniform Manifold Approximation and Projection) to reduce the higher dimension of data to single dimension to check whether these single dimension data of t-sne or UMAP values are useful for ranking  the images. 
\begin{table}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  \textbf{Metric}              & \textbf{DR Technique} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
  \multirow{3}{*}{\textbf{Before Projection Head}} 
                               & PCA                   & 73.33           & 51.87          & 49.59           & 34.80                        & 77.56                      \\
                               & UMAP                  & 93.33           & 99.51          & 99.35           & 93.50                        & 99.67                      \\
                               & t-SNE                 & 99.84           & \textbf{100}         & \textbf{100}          & \textbf{100}                       & 99.19                      \\ \midrule
  \multirow{3}{*}{\textbf{After Projection Head}} 
                               & PCA                   & 80.49           & 21.95          & 76.75           & 76.75                        & 31.71                      \\
                               & UMAP                  & 93.50           & 34.47          & 56.26           & 25.53                        & 93.50                      \\
                               & t-SNE                 & 74.63           & 55.61          & 98.70           & 76.75                        & 31.71                      \\ 
  \bottomrule
  \end{tabular}%
  }
  \caption{Comparison of dimensionality reduction techniques (PCA, UMAP, t-SNE) for various metrics before and after the projection head SimCLR features.}
  \label{tab:combined_dr}
  \end{table}

  
\begin{table}[h!]
  \centering
  \begin{tabular}{lccc}
      \toprule
      & \textbf{PCA} & \textbf{t-SNE} & \textbf{UMAP} \\ 
      \midrule
      & 54.47 & 54.80 & 17.56 \\ 
      \bottomrule
  \end{tabular}
  \caption{Comparison of dimensionality reduction techniques.}
  \label{tab:dim_reduction}
\end{table}

From the above table \ref{tab:combined_dr} we see that before projection head SimCLR features out performs generally after projection head SimCLR features and the original image feature vectors in among all dimensionally reduction technic.
While comparing among before projection head: Above table \ref{tab:combined_dr}  shows that PCA reducing data dimension to 1 D doesn't separate 3 classes well compare to the t-SNE and UMAP. While umap achieved closer to 100 percent ranking accuracy, t-SNE performed better among all dimensionality reduction technic  with the 100 percentage ranking accuracy to separate 3 classes in sweet, resize, No contrast data augmentation pipelines. Limitation is that we are not sure that whether these clear separation between classes ensure there is perfect transition of drug efficacy within the class.