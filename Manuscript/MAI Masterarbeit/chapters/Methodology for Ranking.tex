\chapter{Methodology for Ranking}\label{ch:Methodology for Ranking}
 
since we don't have ground truth labels to rank the images except control images (since control images doesn't applied by drug, drug efficacy is basically zero).
My strategy was to simplify the current ranking problem to only scale/order/rank images 
using only control, single dose and exploded images.
The reason to pick these groups is that controls we know that there is no drug applied which means that there is no effect of drug at all. 
single dose images are the category which we apply specific drug concentration and its clinically recommended at the moment as the most effective genearlly (eventhough we don't know how much drug effected or how 
much it killed the cancer) 
and exploded are visually exploded from the original cancer cell meaning we can visually see debris around the cancer cell potentially which may potentially harm the surrounding good cells.
once if we can order those small subset of entire images, we can add the other image as inference to see where they plotted relative to control or single dose or exloded in this scale. For example, If we can order control , single dose and exploded images in an scale using any of the methodologies we use the same methodology to order all of the drug screening images, we expect it will follow an order of images  where drug screening images which are similar to control comes first in the scale then single dose and then exploded images comes next in the scale.

\section*{Ranking customized Accuracy Calculation}

Accuracy formulation: Goal is to check whether we have clear seperation of each class by the inference metric while using as it as a scale. Accuracy below formulated to check whether the first and third classes are well-separated from the middle. there by it also penalizes if first and third classes are overlaped each other. If its clearly seperated from the middile means all 3 classes  are well seperated. 

Detailed explanation of the accuracy calculation:

Once we obtain the inference metric for each individual feature vector in each class, the next steps involve calculating the mean inference metric for each class, identifying the middle class based on the mean values, and proceeding with the remaining computations as described below.

\textbf{Task 1: Mean of Each Class}

The mean inference metric for each class \( C_i \) is calculated as:

\[
\mu_i = \frac{1}{|C_i|} \sum_{\mathbf{x} \in C_i} M(\mathbf{x}, \mathbf{c})
\]

where \( |C_i| \) represents the number of points in class \( C_i \), and \( M(\mathbf{x}, \mathbf{c}) \) is the inference metric between a point \( \mathbf{x} \) and the centroid \( \mathbf{c} \).

\textbf{Task 2: Middle Class Based on Mean}

To determine the middle class, sort the mean values \( \mu_1, \mu_2, \mu_3 \) in ascending order:

\[
\mu_{\text{sorted}} = \{\mu_{\text{min}}, \mu_{\text{mid}}, \mu_{\text{max}}\}
\]

The middle class is the class corresponding to \( \mu_{\text{mid}} \).

\textbf{Task 3: Minimum and Maximum of Middle Class}

For the middle class (denoted as \( C_{\text{mid}} \)), compute the minimum and maximum inference metric values:

\[
\text{middle\_class\_min} = \min_{\mathbf{x} \in C_{\text{mid}}} M(\mathbf{x}, \mathbf{c})
\]
\[
\text{middle\_class\_max} = \max_{\mathbf{x} \in C_{\text{mid}}} M(\mathbf{x}, \mathbf{c})
\]

\textbf{Task 4: Error and Accuracy Calculation}

1. \textbf{Error Calculation}: Count the number of points in the first class \( C_1 \) that exceed the middle class's minimum, and the number of points in the third class \( C_3 \) that are less than the middle class's maximum. The total error is given by:

\[
\text{error}_1 = \left| \{\mathbf{x} \in C_1 : M(\mathbf{x}, \mathbf{c}) > \text{middle\_class\_min} \} \right|
\]
\[
\text{error}_3 = \left| \{\mathbf{x} \in C_3 : M(\mathbf{x}, \mathbf{c}) < \text{middle\_class\_max} \} \right|
\]
\[
\text{total\_errors} = \text{error}_1 + \text{error}_3
\]

2. \textbf{Accuracy Calculation}: Subtract the total errors from the total number of points across all classes to compute the number of non-errors. Divide this by the total number of points to calculate the accuracy:

\[
\text{accuracy} = \frac{\text{total\_points} - \text{total\_errors}}{\text{total\_points}}
\]

where:
\[
\text{total\_points} = |C_1| + |C_2| + |C_3|
\]

\textbf{Summary of Steps}
1. Compute \( \mu_1, \mu_2, \mu_3 \).
2. Identify the middle class using \( \mu_{\text{mid}} \).
3. Compute \( \text{middle\_class\_min} \) and \( \text{middle\_class\_max} \).
4. Count the errors \( \text{error}_1 \) and \( \text{error}_3 \).
5. Calculate accuracy using the formula above.

\section{Day 7 to day 10 predcition using CAE} \label{subsec:day7-to-day10}


\textcolor{red}{mention problem of data lack here instead in research questions?}
\textcolor{red}{Reason why control doesn't work as anomaly normal because when it turns to day 10 it contains darker + gray so it learns to predict both dark and gray. but when it comes to single dose as anomaly normal it learns only contains dark in day 10 which have no similarity to control day10 or explod day10, and also when it comes to explod in day 10 it have explod thats not in both / no similarity in day 10 of single dose and control}

\begin{enumerate}
    \item \textbf{Step 1:} Create a latent space representation of all images, including untreated, clinically recommended, 
    and drug screening images, using SimCLR. 
    The idea is that SimCLR effectively learns efficient features of similar images that are not captured by 
    human-interpretable metrics. We expect the SimCLR feature vectors of similar images will be closer in the latent space. 
    In other words, feature vectors of similar images will be more linearly separable.
  
  \item \textbf{Step 2:} Train a prediction model exclusively on the representations of 
  untreated images from Day 7 to Day 10 using convolutional autoencoder. ( Input: Day 7 feature vector and target: Day 10 feature vector )

  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include untreated, clinically recommended, and drug screening images.
  Since the day 10 prediction model is trained solely on the representations of untreated images, the inference loss/metric 
  (i.e., the difference between the predicted and actual Day 10 image representations) will be very small for untreated images.
   Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of untreated images.
  This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
  with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. 
  
  \item \textbf{Step 4:} so in the above methods we train first solely on untreated /control images then we did inference on all images just like classical anomaly detection approach. 
  with that same idea/concept, but now we considered other gps as normal and will try to find the deviation from that transition. that is
  we train solyliy on day 7 untreated to day 10 sd images then we do the inference on all images so that inference loss will be how much it deviated from the single dose 
  images. repeating the same concept, we train solely on day 7 untreated to day 10 ex images then we do the inference on all images so that inference loss will be
   how much it deviated from the exploded
  \item Perform the above steps on original image features instead of simclr feature vectors for comparitive study.
\end{enumerate}

Lack of dataset problem for day 7 to day 10 images:
Due to environmetal factors sometimes we may get images of day 7  but we won't able to aquaire  day 10 images or sometimes we may have day 10 images but not corresponding day 7 images. 

Hence for the images for prediction from day 7 to day 10, we only have 130 control images of pair transitions. 29 images of pair for single dose and 40 image pairs for explod. 

\subsubsection*{original images}
data preperation: I decided to train with day 7 untreated to day 10 untretaed images.
 normalised the images by dividing them by 65535. reduced the size to 96*96. Added data augmentations such as horizontal/vertical flips and rotations and random brightness/sharpness/blur as explained in the sweet augmentation section  except contrast and cropping. We can't change contrast  as I explained in the augmentation chapter.and also i didn't use cropping because of the idea that inorder to learn whole change or transformation to day 10, it needs to see the whole day 7 image which is already explained in the data augmentation resize chapter. i made sure that 
 1. augmentation to be coupled that means the same  data augmenattion type parameter value used for both day 7 and corresponding day 10 images. 
 2.  flips or rotation will be unique to each image with random brightness/sharpness/blur change. that means in the dataset original image can have different augs with the same parameter value of brightness/sharpness/blur since its random but it won't have the same geomatrical transformation.

The augmentation helps to increase the possible sample distribution that could happen in real life by that we are trying to find a solution for data defficiency and also helps to prevent overfitting. also it helps to be invariant of the brightness/position/blur/sharp changes that can happen due to the microscope mishandling. 


training parameters and archiecture:

Convolutional Autoencoder Architecture:
We developed an autoencoder architecture with an encoder-decoder structure, utilizing convolutional layers, batch normalization, dropout, and activation functions
 to learn feature representations and predict day 10 image from day 7. The number of feature maps is progressively increased and then symmetrically decreased to balance
  feature extraction and construction of day 10 same size image as day 7.   


  \textbf{Encoder}
  The encoder consists of three convolutional layers with a kernel size of $3 \times 3$ and \texttt{same} padding to maintain spatial resolution.  
  \begin{itemize}
      \item Input channels begin with 3, progressively increasing to 16, 32, and 64 to extract deeper hierarchical features.  
      \item Each convolutional layer is followed by \texttt{BatchNorm2d} for stable learning, \texttt{ReLU} activation for non-linearity, and dropout with probabilities of 0.2, 0.3, and 0.4, respectively, to mitigate overfitting.  
      \item Downsampling is performed using \texttt{MaxPool2d} with a kernel size of $2 \times 2$, stride of 2, and padding to reduce spatial dimensions at each step.  
  \end{itemize}
  
  \textbf{Decoder}
  The decoder mirrors the encoder, reconstructing the input from the learned features.  
  \begin{itemize}
      \item It begins with 64 channels and symmetrically decreases the number of channels to 32, 16, and finally 3 to match the input dimensions.  
      \item Each convolutional layer retains a kernel size of $3 \times 3$ with \texttt{same} padding, followed by \texttt{BatchNorm2d}, \texttt{ReLU} activation, and dropout with probabilities of 0.3 and 0.2, respectively.  
      \item Upsampling is performed using \texttt{Upsample(scale\_factor=2, mode='nearest')} to restore spatial dimensions step by step.  
      \item The final layer includes a \texttt{Sigmoid} activation function to ensure output pixel values are in the range $[0, 1]$, making the model suitable for 
      day 10 image construction.  
  \end{itemize}
  

The overall structure employs progressive channel expansion in the encoder (\(3 \to 16 \to 32 \to 64\)) to capture detailed feature hierarchies and symmetric 
channel reduction in the decoder (\(64 \to 32 \to 16 \to 3\)) for day 10 image construction. 
training parameters: bacth size = 32. learning rate = 0.001. optimizer = adam. loss = mse. epochs = 500. 



Result: 
unfortunately, the model didn't learn the features of transition to day 10 well. the loss didn't decrease in the 500 epochs as we see in the figure.  figure reflects that
model struggles to predict the day 10 images from day 7 image, the predicted image is more resemblence visually to day 7 than day 10. For the model its hard to learn probably because in most cases the day 10 image cancer cell is totally changed to different 
shape or size or color pixel intensity. Also it could be the position change due to the microscope handling position. (We used different flip and rotation 
augmentation but it maynot be effective as we think it is.) since the same problem we may have with the day 7 to day 10 single dose or explod images, I decided to move to
simclr features to see if it can learn the transition better than the original images. 
\textcolor{red}{explain why mse and euclidean distance haev same values because of same ratio?}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/rankloss.png} 
  \caption{train vs test loss}
  \label{fig:uloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/unpred.png} 
  \caption{train vs test loss}
  \label{fig:unloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/pproblem.png} 
  \caption{last row shows misposition of the cancer cell. in the day 7 it was above now. and in day 10 its below.}
  \label{fig:unloss}
\end{figure}

\subsubsection*{Simclr feats}
Set up of data processing same as above original setup.
frist I started with the pair of day 7 untreated to predict the day 10 untreated images setup.normalised images by dividing them by 65535. resized to 96*96. then
I increased the dataset of all gps 130 of untreated. by using same data augmentation technics that I did to original image setup as explained in the above section but with made sure that flips or rotation will be unique to each image with random brightness/sharpness/blur change within a range as above. then I feed to simclr model 
to get features after and before projection head to train the CAE model. 

Training setup: first to ensure the simclr features are scaled I did featurewise minmax scaling. Then I feed those into FeaturePredictor model which is a fully 
connected neural network. It consists of a series of 12 linear layers, where the input size reduces from 512 to 8 and then symmetrically increases back to 512 to
 match the original feature dimensions for before projection head features. For after projection head features,consists of a series of 6 linear layers, where the 
 input size reduces from 20 to 4 and then symmetrically increases back to 20.

Each intermediate layer is followed by:

Batch normalization (BatchNorm1d): Normalizes activations to stabilize training and accelerate convergence.
ReLU activation: Introduces non-linearity to model complex feature relationships.
Dropout (Dropout): Applied with probabilities ranging from 0.2 to 0.4 to prevent overfitting, with deeper layers using higher dropout rates for regularization.
The final layer is a linear transformation to ensure the output feature size matches the day 10 feature vectore dimensions which is 512 for before and 20 for after.

batch size = 32. loss = mse loss. optimizer = adam optimizer. learning rate = 0.0001. no of epochs = 2000. used cross validation with 3 fold and  early stopp with 
the parameters patience = 500 and delta = 0.0001. training data and validation data divided as 80: 20 ratio.

As we see in the figure below, for the training of day 7 to day 10 untreated images and single dose, the train loss and validation loss decreased gradually  in the 2000 epochs. But for the category of cosnidering exploded as normal ie training  day 7 to day 10 exploed images we can see overfitting behaviour in the loss. This is probably due to the fact that for training day 7 to day 10 untreated images we have 130 images correspondingly but for day 7 to day 10 exploded images we have only 40 images as original \textcolor{red}{show sd only have 29 original images also shows overfitting}. This is the problem of data deficiency. We have tried to mitigate this problem by increasing the dataset size by data augmentation but neverthless for the exploded images training didn't quite worked out.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/overfit.png} 
  \caption{Wrong heading crct image upload Ohne contrast Resize  before projection head data aug: Explod dataset plot in the figure shows overfitting }
  \label{fig:unloss}
\end{figure}




Do inference loss for both  before and after projection head feats.
used following metrics as inference loss between target feature vector and predicted feature vector.: mse, cosine distance, L2 ( euclidean distance ), L1 distance, Pearson correlation, dot product,jaccard similarity, hamming distance.

Reason to choose cosine distance is basically because the loss function of simclr is designed to make the cosine distance between similar images to be close to 1. the reason to choose other metrics are the metrics mainly used in medical images as explained in literature review.


results:

As explained before, if the cae trained solely on the transition of untreated images when we do the inerence metric calculation between predicted feature vector and target feature vector for these untreated images should be less than other classes  except pearson, dot product and jaccard similarity. since for those metrics similartiy increases as the value of them increases as explained in \textcolor{red}{ metric chapter}. we met that expectation as we can see the figure \ref{fig:bargraph}.  and some of the metric are able to make a clear seperation of the training class to others ie atleat one class is have clear seperation from other classes as we seen in figure. and some of the metric couldn't make a clear seperation for atleast one class ie all of the classes are mixed as we seen in the figure for dot product and jaccard similarity. We observed this same trend  for both before and after projection head features and for all transition category. Ideal situation that we would like to have is all of the classes are seperated from each other. so that when we do inference on drug screen images there will be look alike smooth transition from class to other.

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 65.93      & 67.22     & 66.55      & 66.39                  & 66.60                 \\
  Before          & Control        & 50.41      & 68.10     & 53.15      & 66.29                  & \textbf{77.51}                 \\
                  & Explod         & 65.93      & 75.59     & 67.58      & 67.48                  & 68.61                 \\ \midrule
                  & Single Dose    & 62.15      & 65.41     & 61.38      & 51.81                  & 60.39                 \\
  After           & Control        & 35.42      & 38.16    & 33.82      & 35.26                  & 39.66                 \\
                  & Explod         & 48.40      & 64.32     & 61.01      & 36.40                  & 51.96                 \\ \bottomrule
  \end{tabular}
  \caption{Cosine distance}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.24      & 66.60     & 66.39      & 66.13                  & 66.60                 \\
  Before          & Control        & 52.95      & 67.11     & 55.79      & 66.03                  & \textbf{77.61}                 \\
                  & Explod         & 70.84      & 74.35     & 64.74      & 69.70                  & 71.98                 \\ \midrule
                  & Single Dose    & 64.12      & 68.87     & 64.01      & 47.57                  & 55.12                 \\
  After           & Control        & 35.68      & 36.61     & 33.66      & 35.32                  & 36.71                 \\
                  & Explod         & 51.50      & 56.41     & 37.80      & 44.98                  & 59.98                 \\ \bottomrule
  \end{tabular}
  \caption{Euclidean distance}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.24      & 66.60     & 66.39      & 66.13                  & 66.60                 \\
  Before          & Control        & 52.95      & 67.11     & 55.79      & 66.03                  & \textbf{77.61}                 \\
                  & Explod         & 70.84      & 74.35    & 64.74      & 69.70                  & 71.98                 \\ \midrule
                  & Single Dose    & 64.12      & 68.87     & 64.01      & 47.57                  & 55.12                 \\
  After           & Control        & 35.68      & 36.61     & 43.80      & 35.32                  & 36.71                 \\
                  & Explod         & 51.50      & 56.41     & 61.48      & 44.98                  & 59.98                 \\ \bottomrule
  \end{tabular}
  \caption{MSE distance}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 65.01      & 66.60     & 66.39      & 65.05                  & 66.60                 \\
  Before          & Control        & 53.83      & 66.75     &  51.34     & 66.03                  & 72.29                 \\
                  & Explod         & 65.15      & 68.20     & 62.56      & 64.79                  & \textbf{76.53}                 \\ \midrule
                  & Single Dose    & 62.36      & 66.13     & 57.91      & 42.24                  & 51.55                 \\
  After           & Control        & 35.52      & 37.44     & 33.61      & 34.64                  & 36.81                 \\
                  & Explod         & 48.71      & 52.12     & 59.10      & 41.68                  & 56.10                 \\ \bottomrule
  \end{tabular}
  \caption{L1 distance}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 66.34      & 66.39     & 66.55      & 66.13                  & 66.44                 \\
  Before          & Control        & 50.26      & 68.51     & 60.34      & 66.24                  & \textbf{75.65}                 \\
                  & Explod         & 64.68      & 67.01     & 65.67      & 65.51                  & 67.01                 \\ \midrule
                  & Single Dose    & 48.50      & 64.68     & 48.24      & 50.52                  & 62.15                 \\
  After           & Control        & 47.78      & 43.02     & 44.26      & 34.49                  & 51.14                 \\
                  & Explod         & 45.04      & 67.11     & 59.26      & 47.21                  & 53.15                 \\ \bottomrule
  \end{tabular}
  \caption{Pearson correlation}
  \label{tab:table_label}
\end{table}


\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 33.61      & 34.13     & 33.40      & 34.23                  & 53.93                 \\
  Before          & Control        & 39.97      & 49.28     & 34.90      & 40.12                  & 47.10                 \\
                  & Explod         & 37.75      & 35.32     & 34.85      & 44.93                  & 32.99                 \\ \midrule
                  & Single Dose    & 36.81      & 46.38     & 47.00      & 39.71                  & 34.75                 \\
  After           & Control        & 33.30      & 38.99     & 34.23      & 34.28                  & 33.66                 \\
                  & Explod         & 34.18      & 45.45     & 33.87      & 36.19                  & 33.61                 \\ \bottomrule
  \end{tabular}
  \caption{Dot product}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 55.69      & 66.65     & 48.55      & 62.72                  & 66.44                 \\
  Before          & Control        & 49.95      & 68.46     & 61.89      & 68.67                  & \textbf{69.29}                 \\
                  & Explod         & 54.34      & 66.18     & 53.83      & 67.32                  & 66.91                 \\ \midrule
                  & Single Dose    & 41.78      & 51.50     & 34.33      & 37.59                  & 50.47                 \\
  After           & Control        & 42.14      & 37.18     & 34.33      & 33.09                  & 36.30                 \\
                  & Explod         & 39.14      & 50.78     & 42.55      & 35.47                  & 41.47                 \\ \bottomrule
  \end{tabular}
  \caption{Jaccard similarity}
  \label{tab:table_label}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Normal as       & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose    & 61.84      & 66.60     & 62.67      & 59.20                  & 66.44                 \\
  Before          & Control        & 34.80      & 60.24     & 48.60      & 64.84                  & 63.70                 \\
                  & Explod         & 62.82      & 72.60     & 56.36      & 64.06                  & \textbf{67.22}                 \\ \midrule
                  & Single Dose    & 41.73      & 46.74     & 35.94      & 48.55                  & 56.77                 \\
  After           & Control        & 45.76      & 42.97     & 34.33      & 33.92                  & 36.76                 \\
                  & Explod         & 45.24      & 55.17     & 43.07      & 43.07                  & 45.04                \\ \bottomrule
  \end{tabular}
  \caption{Hamming  distance}
  \label{tab:table_label}
\end{table}

Dot product is seperating the classes least amoung all metrics which make it as the worst metric for ranking.


range of cosine distacnes = 0-2
range of euclidean distacnes = working
range of mse working

range of L1 distacne is working

range of pearson correlation coefficient: -1 to 1 working. 1 linearly correlated 0 no corelation, -1 inversly corelated.
range of dot product  working.
range of jaccard : working 0 to 1
range of hamming distance is working.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5, angle=270]{figures/predall.pdf} 
  \caption{Last row shows the misposition of the cancer cell. On day 7, it was above, and on day 10, it moved below.}
  \label{fig:bargraph}
\end{figure}


Since we couldn't seperate those 3 classes well that means we won't have good ranking of drug screnn images since sd close images and explod images will be mixed while ranking. this calls the need for other ranking strategy such as Softmax approach, K means centroid approach, PCA approach. where for K means centroid , PCA approach small no of data will not effect that much for the performance of ranking. since there is no training required.

\section{Ranking strategy 3: Softmax approach}

This strategy utilizes the before projection head vectors since its linearly seperable for downstream task classification as suggested in original simclr paper.

We are using only before projectoin head simclr features since original images or after projection head simclr features are not able to classify 100 percentage as we seen in intermediate evaluation chapter.

Steps of methodology for softmax approach:
1. Train classification model to classify control and single dose images.
2. Since we attained 100 percentage acuracy for train and test dataset for this binary classification If we look at softmax score of all of the single dose feature vector to classify as single dose class the values will be will be equal to  or really closer to 1 and similarly, If we look at softmax score of all of the control feature vector to classify as single dose class will be equal to or really closer to 0.
3. Now when we do inference with this learned weights and parameters on the same control, single dose features, we will get   softmax value of all of the single dose feature vector to classify as single dose class the values will be equal to  or really closer to 1 and similarly, If we look at softmax score of all of the control feature vector to classify as single dose class the values will be equal to or really closer to 0.
4. Now we add explod class in addition to this control and single dose class in inference. Idea is that, since we didn't used explod class in training, the softmax score of all of the explod feature vector to classify as single dose class  will not be exactly or closer to 1. It can be below  the values of single dose class features. that  means it can overlap with the values of control feature values. But what if those explod feature softmax values did not overlap with the values of control? ie if we use softmax score to predict as single dose class as inference metric for ranking, in the range of our ranking scale control class features comes first as the values are closer or equal to 0, then explod class features comes next in the scale inbetween 0 and 1.and single dose class features comes last as the values are closer to 1. This is the ideal situation that we would like to have. If we get this kind of ranking scale, we can say that our softmax score is able to seperate control, explod and single dose classes. and we use same customized rankng accuracy calculation as we used in the previous ranking strategies.
5. Use this same steps to classify between control and explod so that we expect those class softmax values will be at both ends (0 and 1) and while single dose class softmax values will be inbetween control and explod class values. like wise classification between explod and single dose, where control feature softmax values will be inbetween explod and single dose class softmax values.

Initialy I used original dataset of control (472 images) and single dose images (103 images) as explained in the dataset table and we used explod ( 40 images) that we get from dalias classifier model.
I train with differnet parameters for  number of epochs and batch size. following parameers gave higher accuracy: 750 epochs, batch:16. every other parameter is same as explained in the classification chapter.


\begin{table}[H]
  \centering
  \begin{tabular}{@{}lccccc@{}}
  \toprule
  \textbf{Classification} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
  Control vs SD             & 25.04            & 49.75           & 76.91          & 85.85                         & 90                        \\
  Control vs Ex             & 83.41            & 49.27               & 95.12           & 60.49                          &    71.54                 \\
  Ex vs SD                & 95.78           & 98.94          & 95.08           & 92.98                       & 97.54                        \\ \bottomrule
  \end{tabular}
  \caption{Table description goes here.}
  \label{tab:ranking softmax}
\end{table}

As we seen in the above table, none of the  data augmentation types are not able to give clear seperation between 3 classes. No contrast sweet, resize and sweet data augmentation data types achieved highest accuracy 89.99 and 95.12 and 98.94  in the category of  Cond7 vs SD and Cond7 vs Ex categories correspondingly. In the classification category between Exploded and single dose features every data augmentation type achived above 90 percentage. this could be because explod and single dose features are less numbers or in another words the class imbalance ratio is less and number of errors( number of overlap between classes) also less. Hence the accuracy is stays higher comparing to the other classification categories which involves control of total 472 images.


Since there is more class imbalance for the classification involves control features, inorder to check if No contrast sweet and resize  giving higher accuracy consistanctly for corresponding control vs sd and control vs ex for balanced dataset, I reduced the original control images to 103 (same number of single dose images ) from 472 images. Result is shown in the following table. highest accuracy achieved for the two categories of Cond7 vs SD and Cond7 vs Ex becomes 100 coresponding to the no contrast sweet and resize  data augmentation types as we see in the table. that means in these two cases there is clear seperation between the classes as we see in the figure below.

\begin{table}[H]
  \centering
  \begin{tabular}{@{}lccccc@{}} 
  \toprule
  \textbf{Classification} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
  Control vs SD             & 97.89           & 99.29          & 98.59           & 98.25                       & 100                        \\
  Control vs Ex             & 97.54               & 96.49              & 100               & 98                          &   97.54                        \\ \bottomrule
  \end{tabular}
  \caption{Table description goes here.}
  \label{tab:ranking softmax}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/softmax.pdf} 
  \caption{last row shows misposition of the cancer cell. in the day 7 it was above now. and in day 10 its below.}
  \label{fig:unloss}
\end{figure}

The main limitation of this approach is we can't garantee when we get new images it will stil seperate three classes 100 percentage like we seen above, there can be overlaps, since the classification only promise us that the softmax score of the control will be closer or equal to zero and  single dose will be closer or equal to 1 in control vs sd case. that means classification only promise us that we get clear seperation inbetween 2 classes not three classes based on the softmax value.



\section{Ranking strategy 2: K Means approach}

Unlike other 2 strategies this strategy doesn't have the probelm of class imbalance

\begin{enumerate}
  \item \textbf{Step 1:} Feed control images simclr features into k means and find the centriod of control (untreated) cluster based on both cosine distance and the euclidean distance using kmeans clustering. 
 
  
  \item \textbf{Step 2:} calculate the euclidean/cosine distance from this centroid to every simclr features.
  \item \textbf{Step 3:} Rank the images based on the distance from the centroid of control simclr features.
  \item \textbf{Step 4:} Perform the above ranking procedure from centroid of single dose image simclr features to other image features and from centroid of exploded image simclr features to other image features correspondingly.
  
  \item \textbf{Step 5:} Perform the same operation  on original images.
\end{enumerate}


Dataset we used : As explained in the table, we used all of the control images (472),
single dose image (103) and exploded images (40).

\textcolor{red}{ds close to sd 10 worked 12 didn't worked}



\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & 93.33      & 93.50     & 48.23      & 92.52                  & 93.82                 \\
  Before          & Control Mean       & 78.86      & 87.64     & 91.49      & 85.53                  & 92.20                 \\
                  & Explod Mean        & 83.25      & 25.53     & 82.74      & 29.43                  & 82.76                 \\ \midrule
                  & Single Dose Mean   & \textbf{95.12}      & 85.69     & 28.29      & 77.72                  & 83.74                 \\
  After           & Control Mean       & 71.54      & 76.75     & 82.76      & 14.47                  & 20.16                 \\
                  & Explod Mean        & 85.37      & 80.00     & 37.72      & 78.21                  & 80.81                 \\ \bottomrule
  \end{tabular}
  \caption{Cosine distance}
  \label{tab:table_label}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/allbox.pdf} 
  \caption{train vs test loss}
  \label{fig:unloss}
\end{figure}



\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & \textbf{92.52}      & 90.41     & 31.71      & 83.58                  &  91.38                \\
  Before          & Control Mean       & 69.11      & 37.24     & 76.75      & 39.84                  & 57.89                 \\
                  & Explod Mean        & 82.44      & 82.60    & 67.97      & 23.90                  & 82.28                 \\ \midrule
                  & Single Dose Mean   & 90.57     & 80.33     & 36.42      & 76.75                  & 21.63                 \\
  After           & Control Mean       & 55.70      & 76.75     & 76.75      & 76.75                  & 77.07                 \\
                  & Explod Mean        & 83.09      & 77.40     & 55.93      & 19.84                  &  81.63                \\ \bottomrule
  \end{tabular}
  \caption{Euclidean distance}
  \label{tab:table_label}
\end{table}

As you can see in the above table, cosine distance of each after projection head strong simclr feature from the single dose mean have the most seperation of classes with accuracy of 95.12. It doesn't seperate three classes fully but its something to hope for. 

With the euclidean distance also  each before projection head strong simclr feature from the single dose mean have the most seperation of classes with accuracy of 92.52. It clearly  seperates one class from other but still far awy from the result of 3 class seperation that we hoped for.

By observing the plot of all inference metric scales the general trend of the accuracy metric is that if its achieved 90 percentage means it seperates one class from other while other 2 classes are still be mixed. And if it starts to increase  from   95 percentage accuracy, it means it seperates those mixed  2 classes. 100 percentage means it seperates all classes clearly without any overlap.

For other inference metrics instead of kmeans clustering, I direcly calculated mean vector of each class to get the centroid vector. and treat it as the centroid of each class and calculated the accuracy as we did before. None of them achieved 90 percentage accuracy. Hence for this mean ranking approach cosine distacne as inference metric is the better choice for the ranking. 

from both of the Cosine and euclidan table its evident that distance from single dose centroid using strong simclr fature is the better choice for the ranking comparing to others since it all achieves 90 percentage accuracy.

All of the above inference metric scaling  I did it on original image vector and as you can see below perfromacne of the ranking accuracy using cosine and euclidean distacne were quite low comparing to the simclr feature vectors. Other inference metrics were also very low. Whcih basically tells us for this ranking strategy also simclr features are better than original image vectors.

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llll@{}}
  \toprule
  Distance From & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Single Dose\\ Mean\end{tabular}} & \multicolumn{1}{c}{Control Mean} & \multicolumn{1}{c}{Explod Mean} \\ \midrule
  Cosine        & 17.56                                                                           & 46.50                            & 28.45                           \\
  Euclidean     & 24.227                                                                           & 50.08                            & 34.47                           \\ \bottomrule
  \end{tabular}
  \caption{Your table caption here}
  \label{tab:you_label}
\end{table}

\section{ Ranking strategy 4: PCA variation}

I need to talk about why pca? what pca does do?


why we choosed cluster 40 dataset because thats the one give 100 accuracy for clustering.

\textcolor{red}{mistakenly this is inference i  icluded ds close in sd. so when do real inference do train}
we used clearly differentiable curated cluster 40 dataset. after projection and original iamges couldnt make 100 percentage clustering so we didn't need to do pca.
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    \textbf{Metric}          & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
    \textbf{Cosine Distance} & 100                & 90.0              & 91.67               & 68.33                          & 95.83                          \\
    \textbf{Euclidean Distance} & 99.17           & 79.17              & 69.17               & 87.50                           & 97.50                          \\ \bottomrule
    \end{tabular}
    \caption{Table showing distances for different augmentation strategies.}
    \label{tab:distances}
  \end{table}

using these we can order but in one end of the scale 2 gps will be mixed.


\textcolor{red}{pca inference acc strong cosine: 90.14 sd 40 + ds close 22. pca eucli: 88.73}


