\chapter{Methodology for Ranking}\label{ch:Methodology for Ranking}
 
since we don't have ground truth labels to rank the images except control images.
My strategy was to simplify the current ranking problem to only scale/order/rank images 
using only control, single dose and exploded images.
The reason to pick these groups is that controls we know that there is no drug applied which means that there is no effect of drug at all. 
single dose images are the category which is clinically recommended at the moment (eventhough we don't know how much drug effected or how 
much it killed the cancer) 
and exploded are visually exploded from the original cancer cell meansing we can visually see debris around the cancer cell potentially which
 may potentially harm the surrounding goof cells.
once if we can order those small subset of entire images, we can add the other image as inference to see where they plotted relative to 
control 
or single dose or exloded in this scale.

\section{Day7 to day10 predcition using CAE} \label{subsec:day7-to-day10}


\textcolor{red}{mention problem of data lack here instead in research questions?}

\begin{enumerate}
    \item \textbf{Step 1:} Create a latent space representation of all images, including untreated, clinically recommended, 
    and drug screening images, using SimCLR. 
    The idea is that SimCLR effectively learns efficient features of similar images that are not captured by 
    human-interpretable metrics. We expect the SimCLR feature vectors of similar images will be closer in the latent space. 
    In other words, feature vectors of similar images will be more linearly separable.
  
  \item \textbf{Step 2:} Train a prediction model exclusively on the representations of 
  untreated images from Day 7 to Day 10 using convolutional autoencoder. ( Input: Day 7 feature vector and target: Day 10 feature vector )

  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include untreated, clinically recommended, and drug screening images.
  Since the day 10 prediction model is trained solely on the representations of untreated images, the inference loss/metric 
  (i.e., the difference between the predicted and actual Day 10 image representations) will be very small for untreated images.
   Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of untreated images.
  This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start 
  with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. 
  
  \item \textbf{Step 4:} so in the above methods we train first solely on untreated /control images then we did inference on all images just like classical anomaly detection approach. 
  with that same idea/concept, but now we considered other gps as normal and will try to find the deviation from that transition. that is
  we train solyliy on day 7 untreated to day 10 sd images then we do the inference on all images so that inference loss will be how much it deviated from the single dose 
  images. repeating the same concept, we train solely on day 7 untreated to day 10 ex images then we do the inference on all images so that inference loss will be
   how much it deviated from the exploded
  \item Perform the above steps on original image features instead of simclr feature vectors for comparitive study.
\end{enumerate}



\subsubsection*{original images}
data preperation: I decided to train with day 7 untreated to day 10 untretaed images which we have pair of 231 images as explained in table \ref{tab:dataset}.
 normalised the images by dividing them by 65535. reduced the size to 96*96. Added data augmentations such as horizontal/vertical flips and rotations and random brightness/sharpness/blur as explained in the sweet augmentation section  except contrast and cropping. We can't change contrast  as I explained in the augmentation chapter.and also i didn't use cropping because of the idea that inorder to learn whole change or transformation to day 10, it needs to see the whole day 7 image which is already explained in the data augmentation resize chapter. i made sure that 
 1. augmentation to be coupled that means the same  data augmenattion type parameter value used for both day and corresponding day 10 images. 
 2.  flips or rotation will be unique to each image with random brightness/sharpness/blur change. that means in the dataset original image can have different augs with the same parameter value of brightness/sharpness/blur since its random but it won't have the same geomatrical transformation.





The augmentation helps to increase the possible sample distribution that could happen in real life by that we are trying to find 
a solution for data defficiency and also helps to prevent overfiting.
also it helps to be invariant of the brightness/position/blur/sharp changes that can happen due to the microscope mishandling.


training parameters and archiecture:

Convolutional Autoencoder Architecture:
We developed an autoencoder architecture with an encoder-decoder structure, utilizing convolutional layers, batch normalization, dropout, and activation functions
 to learn feature representations and predict day 10 image from day 7. The number of feature maps is progressively increased and then symmetrically decreased to balance
  feature extraction and construction of day 10 same size image as day 7.   


  \textbf{Encoder}
  The encoder consists of three convolutional layers with a kernel size of $3 \times 3$ and \texttt{same} padding to maintain spatial resolution.  
  \begin{itemize}
      \item Input channels begin with 3, progressively increasing to 16, 32, and 64 to extract deeper hierarchical features.  
      \item Each convolutional layer is followed by \texttt{BatchNorm2d} for stable learning, \texttt{ReLU} activation for non-linearity, and dropout with probabilities of 0.2, 0.3, and 0.4, respectively, to mitigate overfitting.  
      \item Downsampling is performed using \texttt{MaxPool2d} with a kernel size of $2 \times 2$, stride of 2, and padding to reduce spatial dimensions at each step.  
  \end{itemize}
  
  \textbf{Decoder}
  The decoder mirrors the encoder, reconstructing the input from the learned features.  
  \begin{itemize}
      \item It begins with 64 channels and symmetrically decreases the number of channels to 32, 16, and finally 3 to match the input dimensions.  
      \item Each convolutional layer retains a kernel size of $3 \times 3$ with \texttt{same} padding, followed by \texttt{BatchNorm2d}, \texttt{ReLU} activation, and dropout with probabilities of 0.3 and 0.2, respectively.  
      \item Upsampling is performed using \texttt{Upsample(scale\_factor=2, mode='nearest')} to restore spatial dimensions step by step.  
      \item The final layer includes a \texttt{Sigmoid} activation function to ensure output pixel values are in the range $[0, 1]$, making the model suitable for 
      day 10 image construction.  
  \end{itemize}
  

The overall structure employs progressive channel expansion in the encoder (\(3 \to 16 \to 32 \to 64\)) to capture detailed feature hierarchies and symmetric 
channel reduction in the decoder (\(64 \to 32 \to 16 \to 3\)) for day 10 image construction. 
training parameters: bacth size = 32. learning rate = 0.001. optimizer = adam. loss = mse. epochs = 500. 



Result: 
unfortunately, the model didn't learn the features of transition to day 10 well. the loss didn't decrease in the 500 epochs as we see in the figure.  figure reflects that
model struggles to predict the day 10 images from day 7 image, the predicted image is more resemblence visually to day 7 than day 10. For the model its hard to learn probably because in most cases the day 10 image cancer cell is totally changed to different 
shape or size or color pixel intensity. Also it could be the position change due to the microscope handling position. (We used different flip and rotation 
augmentation but it maynot be effective as we think it is.) since the same problem we may have with the day 7 to day 10 single dose or explod images, I decided to move to
simclr features to see if it can learn the transition better than the original images. 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/rankloss.png} 
  \caption{train vs test loss}
  \label{fig:unloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/unpred.png} 
  \caption{train vs test loss}
  \label{fig:unloss}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/pproblem.png} 
  \caption{last row shows misposition of the cancer cell. in the day 7 it was above now. and in day 10 its below.}
  \label{fig:unloss}
\end{figure}

\subsubsection*{Simclr feats}
Set up of data processing same as above original setup.
frist I started with the pair of day 7 untreated to predict the day 10 untreated images setup.normalised images by dividing them by 65535. resized to 96*96. then
I increased the dataset of all gps 231 of untreated , 103 of single dose and 40 of exploded to 750 by using same data augmentation technics that I did to original image setup as explained in the above section but with made sure that flips or rotation will be unique to each image with random brightness/sharpness/blur change within a range as above. then I feed to simclr model 
to get features after and before projection head to train the CAE model. 

Training setup: first to ensure the simclr features are scaled I did featurewise minmax scaling. Then I feed those into FeaturePredictor model which is a fully 
connected neural network. It consists of a series of 12 linear layers, where the input size reduces from 512 to 8 and then symmetrically increases back to 512 to
 match the original feature dimensions for before projection head features. For after projection head features,consists of a series of 6 linear layers, where the 
 input size reduces from 20 to 4 and then symmetrically increases back to 20.

Each intermediate layer is followed by:

Batch normalization (BatchNorm1d): Normalizes activations to stabilize training and accelerate convergence.
ReLU activation: Introduces non-linearity to model complex feature relationships.
Dropout (Dropout): Applied with probabilities ranging from 0.2 to 0.4 to prevent overfitting, with deeper layers using higher dropout rates for regularization.
The final layer is a linear transformation to ensure the output feature size matches the day 10 feature vectore dimensions which is 512 for before and 20 for after.

batch size = 32. loss = mse loss. optimizer = adam optimizer. learning rate = 0.0001. no of epochs = 2000. used cross validation with 3 fold and  early stopp with 
the parameters patience = 500 and delta = 0.0001. training data and validation data divided as 80: 20 ratio.

As we see in the figure below, for the training of day 7 to day 10 untreated images and single dose, the train loss and validation loss decreased gradually  in the 2000 epochs. But for the category of cosnidering exploded as normal ie training  day 7 to day 10 exploed images we can see overfitting behaviour in the loss. This is probably due to the fact that for training day 7 to day 10 untreated images and single dose we have 231 and 103  images correspondingly but for day 7 to day 10 exploded images we have only 40 images as original. This is the problem of data deficiency. We have tried to mitigate this problem by increasing the dataset size by data augmentation but neverthless for the exploded images training didn't quite worked out.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/overfit.png} 
  \caption{Ohne contrast Resize  before projection head data aug: Explod dataset plot in the figure shows overfitting }
  \label{fig:unloss}
\end{figure}


Do inference loss for both  before and after projection head feats.
used following metrics as inference loss between target feature vector and predicted feature vector.: mse, cosine distance, L2 ( euclidean distance ), L1 distance, Pearson correlation, dot product,jaccard similarity, hamming distance.

Reason to choose cosine distance is basically because the loss function of simclr is designed to make the cosine distance between similar images to be close to 1. the reason to choose other metrics are the metrics mainly used in medical images as explained in literature review.


results:

as explained before, if the cae trained solely on the transition of untreated images when we do the inerence metric calculation between predicted feature vector and target feature vector for these untreated images should be less than other classes. we met that expectation as 
we can see the figure. the inference loss for untreated images is less than the other classes except pearson, dot product and jaccard similarity. since for those metrics similartiy increases as the value of them increases as explained in literature review. ad some of the metric are able to make a clear seperation of the training class to others ie atleat one class is have clear seperation from other classes as we seen in figure. and some of the metric couldn't make a clear seperation for atleast one class ie all of the classes are mixed as we seen in the figure. Ideal situation is that all of the classes are seperated from each other. so that when we do inference on drug screen images there will be look alike smooth transition from class to other.

We observed this same trend  for both before and after projection head features and for all transition category.


range of cosine distacnes = 0-2
range of euclidean distacnes = working
range of mse working

range of L1 distacne is working

range of pearson correlation coefficient: -1 to 1 working. 1 linearly correlated 0 no corelation, -1 inversly corelated.
range of dot product  working.
range of jaccard : working 0 to 1
range of hamming distance is working.
\textcolor{red}{show figures of each distance as example show that one gp is seperated as expected but others mixed.}


Since we couldn't seperate those 3 classes well that means we won't have good ranking of drug screnn images since sd close images and explod images will be mixed while ranking. this calls the need for other ranking strategy such as Softmax approach.

Hence we nned better approach for ranking where small no of data will not effect that much for the performance such as K means centroid approach, PCA approach. 

\section{Ranking strategy 2: K means centroid approach}

This strategy utilizes the after projection head vectors since simclr loss function designed that after projection head vectors have cosine similarity between similar 
group.

\begin{enumerate}
  \item \textbf{Step 1:} Feed control images into k means and find the centriod of control (untreated) cluster based on both cosine distance and the euclidean distance. 
  ( we can choose the distacne metric if we have time)
  
  \item \textbf{Step 2:} calculate the euclidean/cosine distance from this centroid to every images.
  
  \item \textbf{Step 3:} Perform the same operation for simlcr features and on original images.
\end{enumerate}

\textcolor{red}{change metric to mistakes from center gp to teft and right.}

\subsection*{Group-Wise Ranking Accuracy: Mathematical Definition and Process}

\textcolor{red}{below maybe wrong because I didN't added that gp order determine by calculating mean of cosine distance}

\subsubsection*{Definitions}

Let \( G_1, G_2, G_3, \dots, G_n \) represent \( n \) different groups of distances.  
The distances in each group \( G_i \) are denoted as:
\[
D_i = \{d_{i1}, d_{i2}, \dots, d_{im_i}\},
\]
where \( m_i \) is the number of elements in group \( G_i \).  

Let:
\[
\{D_1, D_2, \dots, D_n\}
\]
represent the collection of all groups.

After sorting all the distances across the groups into a single list, we check if the order of groups is maintained, i.e., whether all distances in \( G_1 \) are less than those in \( G_2 \), all in \( G_2 \) are less than those in \( G_3 \), and so on.

\subsubsection*{Mathematical Formula for Group-Wise Ranking Accuracy}

\paragraph{Correct Transitions}
A correct transition between two groups \( G_i \) and \( G_j \) (with \( i < j \)) occurs if all elements of \( G_i \) are less than all elements of \( G_j \) after sorting.  

This can be expressed mathematically as:
\[
\text{Correct Transition}(G_i \to G_j) = 
\left[
\forall d_{ik} \in G_i, \forall d_{jl} \in G_j : d_{ik} < d_{jl}
\right], \quad \text{for } i < j.
\]

\paragraph{Total Possible Transitions}
The total number of possible transitions is the number of adjacent group pairs:
\[
T_{\text{total}} = n - 1.
\]

\paragraph{Group-Wise Ranking Accuracy}
The ranking accuracy is the ratio of correct transitions (\( T_{\text{correct}} \)) to total possible transitions (\( T_{\text{total}} \)):
\[
\text{Accuracy} = \frac{T_{\text{correct}}}{T_{\text{total}}}.
\]

\subsubsection*{Step-by-Step Process}

1. \textbf{Sort the Distances:} Sort all the distances from all groups into a single list while keeping track of the group each distance belongs to.

2. \textbf{Check for Correct Transitions:} For each adjacent group pair \( (G_i, G_j) \), check if the condition:
\[
\forall d_{ik} \in G_i, \forall d_{jl} \in G_j : d_{ik} < d_{jl}
\]
holds true.

3. \textbf{Count Correct Transitions:} If the condition holds for a group pair, increment \( T_{\text{correct}} \).

4. \textbf{Compute Accuracy:} Finally, compute the group-wise ranking accuracy as:
\[
\text{Accuracy} = \frac{T_{\text{correct}}}{T_{\text{total}}}.
\]






Curated control dataset: 280
sd: full: 103
exploded full: 40

ds close to sd 10

\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & 92.42      & 91.71     & 92.42      & 91.47                  & 92.42                 \\
  Before          & Control Mean       & 94.55      & 92.65     & 93.84      & 95.73                   & 94.55                 \\
                  & Explod Mean        & 82.23      & 82.23     & 92.65     & 83.65                  & 81.52                \\ \midrule
                  & Single Dose Mean   & \textbf{97.39}  & 87.68     & 92.89     & 82.94                  & 83.65                 \\
  After           & Control Mean       & 86.97      & 76.78    & 90.76      & 76.30                 & 82.70                 \\
                  & Explod Mean        & 92.18      & 79.62    & 88.86      & 77.96                  & 82.23                 \\ \bottomrule
  \end{tabular}
  \caption{Cosine distance}
  \label{tab:your_table_label}
\end{table}


\begin{table}[H]
  \centering
  \begin{tabular}{@{}llccccc@{}}
  \toprule
  Projection Head & Distance From      & Strong & Sweet & Resize & No Contrast Resize & No Contrast Sweet \\ \midrule
                  & Single Dose Mean   & 91      & 89.10     & 91.23      & 86.49                  & 90.52                 \\
  Before          & Control Mean       & 87.68      & 86.73     & 88.15      & 82.94                  & 86.73                 \\
                  & Explod Mean        & 81.99      & 81.52     & 92.65      & 83.18                  &   81.52               \\ \midrule
                  & Single Dose Mean   & 90.05      & 81.28     & 91.94      & 77.25                  & 78.67                 \\
  After           & Control Mean       & 82.23      & 76.07     & 84.12      & 76.54                  & 74.17                 \\
                  & Explod Mean        & 90.52      & 78.20    & 94.79      & 75.83                  & 81.75                 \\ \bottomrule
  \end{tabular}
  \caption{Euclidean distance}
  \label{tab:your_table}
\end{table}


\textcolor{red}{ add figures below. }

other notable metrics that achieved around 95 percentage accuracy are pearson distance by resize before explod mean with 96.69. hence we need supervised classifier for explod then its fine.






With this costumized metric, we get around 95 if there is slightest seperation for 3 gps.

after projection head: strong seperates perfectly but the problem is since the control is seperated at the middle ordering. Hence we can find the less effective in the middle,

\textcolor{red}{clear seperation between sd and ex ( 2 gp) and max mean difference between them table:}


\begin{table}[H]
  \centering
  \begin{tabular}{@{}llll@{}}
  \toprule
  Distance From & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Single Dose\\ Mean\end{tabular}} & \multicolumn{1}{c}{Control Mean} & \multicolumn{1}{c}{Explod Mean} \\ \midrule
  Cosine        & 81.28                                                                           & 86.02                            & 76.78                           \\
  Euclidean     & 83.18                                                                           & 78.44                            & 80.81                           \\ \bottomrule
  \end{tabular}
  \caption{Your table caption here}
  \label{tab:you_label}
\end{table}


\section{Ranking strategy 3: Softmax approach}

This strategy utilizes the before projection head vectors since its linearly seperable for downstream task classification as suggested in original simclr paper.

We will use simclr features since original images are not able to classify 100 percentage as we seen in intermediate evaluation.
1. Train classification model to classify cond7 and sd. 
2. Then do inference on them and take softmax probability as metric for ranking.
below table used 750 epochs, batch:8: all of these value can improve by performing regularizytion and more layers or less layers basically hyper parameter tuning. 
i didn't do it, since i was focused on methodolgy.
Below trained for 750 epochs:batch:8
\begin{table}[H]
  \centering
  \begin{tabular}{@{}lccccc@{}}
  \toprule
  \textbf{Classification} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
  Cond7 vs SD             & 97.89           & 99.29          & 98.59           & 98.25                       & 100                        \\
  Cond7 vs Ex             & 97.54               & 96.49              & 98.94               & 100                           &   97.54                        \\
  Ex vs SD             & 95.78               & 98.94              & 95.08               & 92.98                           & 97.54                         \\ \bottomrule
  \end{tabular}
  \caption{Table description goes here.}
  \label{tab:ranking softmax}
\end{table}

\textcolor{red}{Depend on time do ex twenty nine}
  
\textcolor{red}{this questions if strong or No contrast sweet is betterr? because previously strong was better. Now sweet performed well for this task. 
to understand why sweet performs better we need to see crop change and brghtness, contrast change seperatly.}

When we used curated dataset percentage of gp wise accuracy went down where when we used control as the same number of images of sd accuracy is 100. 
it worked maybe because of class balancing.

How do we check which data aug is working?
it should seperate cond7, ex, sd: 100 percentage. because if get 100 that means it is able to make gp of control and sd for sure, from ex mixing.

why are we doing this because every aug can  seperate cond 7 and sd. thats obivious. because there is no other gps. so we have to add new gp as inference to see if 
cond7 and sd still gp together. same principle in kmeans approach.

so, with this approach from cond7 to sd classsification softmax probability score we get mixture of exploded and gray from drug screen.

if we want to avoid that we can train supervised classification to filter out the exploded from there then put them on right side of sd so that we get clean scale.
for that training we can use simclr feature vectors because it gave more accuracy than the original image accuracy.

\textcolor{red}{put the below table in classification and also do classification of ex fourty others because we are usign it for softmax approach} 

\begin{table}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{llccccc}
  \toprule
  \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
  \multirow{4}{*}{\textbf{Before Projection Head}} 
      & Train Accuracy (\%) & 97.07 & 96.23 & 96.23 & 97.91 & 97.07 \\
      & Train Epoch         & 30 & 41 & 128 & 465 & 485 \\
      & Test Accuracy (\%)  & 95 & 91.67 & 93.33 & 93.33 & 95 \\
      & Test Epoch          & 5 & 5 & 21 & 12 & 17 \\ 
  \bottomrule
  \end{tabular}%
  }
  \caption{Performance metrics for different augmentation strategies before the projection head.}
  \label{tab:augmentation}
\end{table}

\textcolor{red}{include other classification combinations in table}

Why its not giving 100 because now the problem is harder. we had 2 classes, explod vs all other drug screen,where drug screen look intermediate to explod and sigle dose.
Also we can improve this result by adding more hidden layers instead of just one linear layer and so more. at the moment i didn't since we want to know which data aug works for harder problem.
because in the intermediate evaluation  most of them gave 100 percenatage making it not  evaluator for evaluating inbetween data augs. there we get that orig vs data augs.

\begin{table}[h!]
  \centering
  \caption{Original Image Results}
  \label{tab:original_image_results}
  \begin{tabular}{lcccc}
  \toprule
  \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Test Accuracy (\%)} & \textbf{Test Epoch} \\ \midrule
  \textbf{Original Image} & 84.94                        & 434                   & 78.33                        & 19                  \\ 
  \bottomrule
  \end{tabular}
\end{table}


ordered images can found in the below gdrive links for both mixed and cleaned order for ds and ex.
show them ordered images for 80 percentage gp wise accuracy ie only seperated 2 gps, show them its worse.

\textcolor{red}{its better to not use exploded and harm people sentence from intro and data intro just say debris, because cond10 and exploded have debris. cond10 
have debris even without drug so debris can happen without drug.it is probabliy because its cultivated in lab where these things happen.} 

\section{ Ranking strategy 4: PCA variation}

- First cluster points that exceed the minimum value of the middle cluster
- Third cluster points that fall below the maximum value of the middle cluster

why we choosed cluster 40 dataset because thats the one give 100 accuracy for clustering.

\textcolor{red}{mistakenly this is inference i  icluded ds close in sd. so when do real inference do train}
we used clearly differentiable curated cluster 40 dataset. after projection and original iamges couldnt make 100 percentage clustering so we didn't need to do pca.
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    \textbf{Metric}          & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{No Contrast Resize} & \textbf{No Contrast Sweet} \\ \midrule
    \textbf{Cosine Distance} & 100               & 90.0              & 91.67               & 68.33                           & 95.83                          \\
    \textbf{Euclidean Distance} & 99.17           & 79.17              & 69.17               & 88.00                           & 97.50                          \\ \bottomrule
    \end{tabular}
    \caption{Table showing distances for different augmentation strategies.}
    \label{tab:distances}
  \end{table}

using these we can order but in one end of the scale 2 gps will be mixed.


\textcolor{red}{pca inference acc strong cosine: 90.14 sd 40 + ds close 22. pca eucli: 88.73}


