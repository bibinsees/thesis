\chapter{Conclusion}

\section{Final Evaluation}
Now out of the 4 methodologies which methodology is more reliable and consistant?
I picked 22 sinle dose  similar images from drug screening images (Note: This final Evaluation is not relaible in the sense that the picked images are not verified by any biology expert that those are the most similar images to single dose images from drug screening. so eventhough this final evaluation is not scientific its better than nothing). Idea is that we will pick the best performed data augmentation type/category from each methodology and check whether these images are still positioned in the range of the single dose image range.
\begin{table}[H]
	\centering
	\small
	\begin{tabular}{@{}lc@{}}
	\toprule
	\textbf{Ranking Strategy}                                                                       & \textbf{Out of single dose range} \\ \midrule
	Softmax: cond7 vs sd                                                                            & 0                   \\ 
	Softmax: cond7 vs Ex                                                                            & 0                   \\ 
	\begin{tabular}[c]{@{}l@{}}K-means: After cosine strong\\ using single dose mean\end{tabular}   & 12   controlil kedakkane              \\ 
	PCA: strong cosine distance                                                                     & 12 controlil kedakkane                   \\ 
	PCA: Euclidean strong                                                                           & 11  controlil               \\ \bottomrule
	\end{tabular}
	\caption{Performance metrics across different ranking strategies.}
	\label{tab:ranking_strategies}
\end{table}



	
	Conclusions - yes, what are they? What is the outcome. Important - it is fine to say that despite your best efforts the methods do not seem to
	 deliver useful results. Perhaps they do not beat the baseline, this is very common in research and is perfectly fine, correct and useful to admit.
	  Or they actually seem to perform randomly and are not reliable. This may be due to not enough data and is also ok to say, admit and point out. Or 
	  they do something but you are not actually sure, how to use for the final objective or ranking. This is also ok and you shall explain why you are 
	  not convinced. Perhaps the ranking is simply a badly formulated problem and it should not be approached this way at all. This is also ok to say and 
	  be open about it. Simply, even if it "does not work" it is still VERY useful if we understand what you have done and why you think it does not work.	
How do you know which ranking strategy is better visually? inference from ds close tosd for the worked one from each strategy and look which one works 
better?

References - clean these. Correct references shall have not only the names and year but also the publication venue (journal, conference, arxiv... etc.)

\section{Future research direction}

\begin{enumerate}
    \item Hybrid: Include human-interpretable features in the unsupervised learning features combined effect.
    \item Weakly supervised DINO transformer-based approach. Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
	Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.
    \item Weakly supervised SimCLR approach.
    \item Other distance-based approaches.
    \item kl density estimation probability
    \item Image size for simclr 96 vs 256 vs 512 as well as original images 96 vs 256 vs 512 vs even original size 2054?
    \item other architectures. Resnet vs unet in Simclr
    \item 3 channel vs 1 channel (mis alignment probelm or compuatoinal faster?)
    \item Batch size 16 vs 64 vs 128 vs 256
    \item \textcolor{red}{after projectoin add l2 norm so that it will exactly like the loss fn gives good cosine sim} 

\end{enumerate}