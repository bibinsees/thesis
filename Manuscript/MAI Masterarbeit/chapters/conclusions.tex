\chapter{Conclusion}

\section{Final Evaluation}
Now out of the 4 methodologies which methodology is more reliable and consistant in terms of relative positioning? This is the question we try to answer below.
We will evaluate the performance of the specific method out of each main 4 methodologies that performed well in ranking accuracy.
I picked 22 sinle dose  similar images from drug screening images, lets call them inference images in this chapter (Note: This final Evaluation is not relaible in the sense that the picked images are not verified by any biology expert that those are the most visually similar images to single dose images from drug screening. so eventhough this final evaluation is not scientificly right, its better than nothing). Idea is that we will pick the best performed data augmentation type/category from each methodology and check whether these 22 images are still positioned in the range (min and max of single dose metric value) of the single dose image range. Limitation is that we are not sure whether the picked images are the most similar images to single dose images from drug screening interms of visual similartiy or drug efficay since I'm not expert in biology.

\begin{table}[H]
	\centering
	\small
	\begin{tabular}{@{}lcc@{}}
	\toprule
	\textbf{Ranking Strategy}                                                                       & \textbf{Out of Single Dose Range} & \textbf{Ranking Accuracy} \\ \midrule
	Prediction model Euclidean distance Before                                                     & 0                   & 76.93                   \\  
	tsne: before sweet                                                                              & 0                   & 100                     \\ 
	tsne: resize before                                                                             & 0                   & 100                     \\ 
	tsne: before no contrast resize                                                                 & 0                   & 100                     \\ 
	Softmax: cond7 vs Ex Resize                                                                     & 1                   & 97.33                   \\ 
	Softmax: Ex vs sd Strong                                                                        & 7                   & 92.46                   \\ 
	\begin{tabular}[c]{@{}l@{}}K-means: After cosine strong\\ using single dose mean\end{tabular}   & 12                  & 93.17                   \\ 
	\bottomrule
	\end{tabular}
	\caption{Performance metrics across different strategies.}
	\label{tab:ranking_strategies}
\end{table}

From above table that some of the selected out of 22 are out of the single dose image ranking metric for Softmax and K means strategy. that makes these strategies least reliable comparing to other approaches. Infact K means approach the 12 images which are out of single dose image ranking metric range positioned amoung the control class that makes K means strategy most unreliable. Interestingly Predictoin model and tsne strategy have 0  number of inference images that are out of range of single dose. While predictoin model still struggles to have clear seperation between  3 classes (This is probably due to the lack of data for predicting day 7 to day 10. we had only 29, 40,130 pair of tansition coresspondingly to  single dose, explod,control ), tsne strategy maintained clear seperation between 3 classes achieving 100 ranking accuracy as we see in the figure below. \textcolor{red}{add figure} Limitation is that we are still not sure that whether these clear seperation between classes ensure there is perfect transition of druf efficacy within the class or inbetween classes.

One finding taht we are sure about is that from every results including ranking strategy and intermediate evaluation methods, generally simclr before projection head outperforms the after projection head features and original image features except in K-means: cosine strong where after projection head performed better but we can disgard it since turn out to be unreliable strategy from the above table.

While using before projection head features, inbetween different data augmentation pipeline methods such as strong, sweet, resize, no contrast sweet, nocontrast resize. which one performs better? If we look at different ranking startegies and intermediate evaluation, each of them performed best in different ranking and intermediate evaluation strategies. point is, there is no consistancy in terms of one data augmentation pipileine showed best prformance overall. in another words they actually seem to perform randomly and which raises the question whether we can rely on them. This may be due to not enough data. If we have more data maybe the same methodolgy reveals which one better. At the moment they give some results but I'm not actually sure, whether its useful for the final objective or ranking. because eventhough simclr before projection head features with different  data aug pipelines gives clear seperation between classes with 100 percentage ranking accuracy using tsne, that doesn't mean there is ideal transition of druf efficacy within the class or inbetween classes we are looking for our objective. Hence I'm not convinced the final evaluation that we get in the above table for relative positioning of drug screen images to three classes make sense at the end. 

One of the main point to take awy is that for intermediate evaluations clustering using simclr before head feature vectors perfromed very well and gave around 99 percentage accuracy. This is interesting result because using completely unsupervised learning, we could cluster the three classes of control (untreated) , single dose, explod images very well without using any manually handcrafted feature extraction like Size/Area, Circularity/Diameter/Perimeter, Pixel intensity or color change during the changes it induces in the bright-field microscopy images from day 7 to day 10, which is also a comparably equal result to supervised resnt \textcolor{red}{add resnet classification}.

Perhaps the ranking is simply a badly formulated problem and instead of approaching this way, we should have been approach which group/cluster of specific drug screening concentration applied images are more closer to the clsuetr of single dose image cluster


Despite your best efforts the methods do not seem to deliver useful results.



\section{Things that doesn't work}
manhatton distance 
pca weighted 
prediction with day 10 to 10 classical anamoly detection methodologies
predict directly the mse change, predicting diectly cosine change

\section{Future research direction}

\begin{enumerate}
    \item Hybrid: Include human-interpretable features in the unsupervised learning features combined effect.
    \item Weakly supervised DINO transformer-based approach. Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
	Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.
    \item Weakly supervised SimCLR approach.
    \item Other distance-based approaches.
    \item kl density estimation probability
    \item Image size for simclr 96 vs 256 vs 512 as well as original images 96 vs 256 vs 512 vs even original size 2054?
    \item other architectures. Resnet vs unet in Simclr
    \item 3 channel vs 1 channel (mis alignment probelm or compuatoinal faster?)
    \item Batch size 16 vs 64 vs 128 vs 256
    \item new paper about simclr in linkedin magda

\end{enumerate}