\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

**if we do kmeans withcosine distance distance then it only compare cosine sim of whole image thats why we need to do normal kmeans with euclidean dist to show 
the magnitude similarity** 

Evaluation of the SSL model depends on the  time series inference loss/accuracy metric, neverthless we can use other evaluation metrics, such as downstream task 
like classification. 

**suppose I have a vector of image. if I normalise it unit length, willl I loose critical information?**
**https://chatgpt.com/share/671a6a63-ca7c-8010-92a0-23b6bd25ef05**

\subsubsection{classification}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single, linear layer that maps these representations to class predictions, where the two categories, 'untreated' and 'single dose,' 
 serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. 
 Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
  We implemented a simple pipeline for a Logistic Regression setup, where the images are encoded into their feature vectors.

2. Baseline comparison: As a baseline for comparison  to our results above in the section \ref{sec:variations_implementations}, we will train a standard ResNet-18 with
 random initialization on the labeled training set, consisting of the 'untreated' and 'single dose' categories. The results will help us assess the advantages of contrastive 
 learning on unlabeled data compared to purely supervised training. It is evident that ResNet-18 easily overfits the training data since its parameter count is over 1,000 times 
 larger than the dataset size. To ensure a fair comparison with the contrastive learning models, we apply similar data augmentations as before, including crop-and-resize 
 and color jittering.
\subsubsection{kmeans clustering}
Idea is whether the learned representation from simlcr outperforms the original images in clustering the images (unsupervised manner) For that we use simple kmeans clusetring.
We will cluster them based on both euclidean distance as well as cosine distance.

Derivation of K-Means Clustering using Euclidean Distance and Mean

Objective Function
The k-means algorithm aims to minimize the total squared Euclidean distance between data points and their assigned cluster centroids. The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): The \( i \)-th data point,
    \item \( \boldsymbol{\mu}_k \): The centroid of the \( k \)-th cluster,
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\subsection*{Cluster Assignment Step}
For a fixed set of centroids \( \{ \boldsymbol{\mu}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest centroid. This minimizes:

\[
r_{ik} =
\begin{cases} 
1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
0, & \text{otherwise.}
\end{cases}
\]

\subsection*{Centroid Update Step}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \boldsymbol{\mu}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

Focus on a single cluster \( k \). The term involving \( \boldsymbol{\mu}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
= \sum_{i=1}^{n} r_{ik} \left( \mathbf{x}_i^\top \mathbf{x}_i - 2 \mathbf{x}_i^\top \boldsymbol{\mu}_k + \boldsymbol{\mu}_k^\top \boldsymbol{\mu}_k \right)
\]

Take the derivative with respect to \( \boldsymbol{\mu}_k \) and set it to zero:

\[
\frac{\partial}{\partial \boldsymbol{\mu}_k} \sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2 =
-2 \sum_{i=1}^{n} r_{ik} \mathbf{x}_i + 2 \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k = 0
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k
\]

Factor out \( \boldsymbol{\mu}_k \):

\[
\boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the points in cluster \( k \).

\subsection*{Algorithm Summary}
The k-means algorithm alternates between the following two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the nearest cluster:
    \[
    r_{ik} =
    \begin{cases} 
    1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
    0, & \text{otherwise.}
    \end{cases}
    \]
    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the mean of its assigned points:
    \[
    \boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
    \]
\end{enumerate}

This iterative process continues until the assignments \( r_{ik} \) and centroids \( \boldsymbol{\mu}_k \) no longer change or the change is below a threshold.

\subsection*{Cosine distance}

\section*{Normalization}

To calculate the cosine distance, we first \textbf{normalize} all data points and centroids. Suppose the normalized data points and centroids are \( \mathbf{x}_i \) and \( \mathbf{c}_k \), respectively, then:

\[
\|\mathbf{x}_i\| = 1 \quad \text{and} \quad \|\mathbf{c}_k\| = 1.
\]

This ensures all vectors are on the unit sphere. The cosine similarity between two vectors \( \mathbf{x}_i \) and \( \mathbf{c}_k \) is defined as:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|}
\]

Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we substitute these values into the equation:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{1 \times 1}
\]

This simplifies to:

\[
\text{cosine similarity} = \mathbf{x}_i^\top \mathbf{c}_k.
\]

Thus, the \textbf{cosine distance} becomes:
\[
\text{cosine distance} = 1 - \mathbf{x}_i^\top \mathbf{c}_k.
\]

\section*{Relating Cosine Distance to Euclidean Distance}

For normalized vectors, we derive the relationship between \textbf{Euclidean distance} and \textbf{cosine distance}. The squared Euclidean distance between a data point \( \mathbf{x}_i \) and a centroid \( \mathbf{c}_k \) is:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \sum_{j} (x_{ij} - c_{kj})^2.
\]
Expanding this:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we get:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Simplify:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k).
\]
Thus, for normalized vectors, the Euclidean distance is proportional to the cosine distance:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2 \cdot \text{cosine distance}.
\]
Rearranging to express the cosine distance:
\[
\text{cosine distance} = \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]




\section*{Objective Function}

The k-means algorithm with cosine distance aims to minimize the cosine distance between data points \( \mathbf{x}_i \) and their assigned cluster centroids \( \mathbf{c}_k \). The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|} \right),
\]
where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): \( i \)-th data point,
    \item \( \mathbf{c}_k \): Centroid of cluster \( k \) (normalized to unit length),
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\section*{Objective Function in Terms of Euclidean Distance}

Using the above result, the k-means objective function with cosine distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]
can be rewritten in terms of Euclidean distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]
Here, the factor \( \frac{1}{2} \) accounts for the scaling difference.

\section*{Cluster Assignment Step}

For a fixed set of centroids \( \{ \mathbf{c}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest cluster based on the \textbf{cosine similarity} (or equivalently, minimize cosine distance):
\[
r_{ik} =
\begin{cases}
1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
0, & \text{otherwise.}
\end{cases}
\]
\subsection*{Centroid Update Step for Cosine Distance}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \mathbf{c}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
\]

Focus on a single cluster \( k \). The term involving \( \mathbf{c}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
= \sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Since the vectors are normalized, \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we have:

\[
\sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Now, take the derivative of the above with respect to \( \mathbf{c}_k \) and set it to zero:

\[
\frac{\partial}{\partial \mathbf{c}_k} \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Factor out \( \mathbf{c}_k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the normalized points in cluster \( k \).

\section*{Centroid Update Step}

For a fixed cluster assignment \( \{ r_{ik} \} \), update the centroids \( \{ \mathbf{c}_k \} \) as the \textbf{normalized mean} of all data points assigned to cluster \( k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
\]

\section*{Algorithm Summary}

The k-means algorithm with cosine distance alternates between two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the cluster with the highest cosine similarity:
    \[
    r_{ik} =
    \begin{cases}
    1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
    0, & \text{otherwise.}
    \end{cases}
    \]

    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the normalized mean of its assigned points:
    \[
    \mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
    \]
\end{enumerate}

\section*{Conclusion}

By normalizing the data points and centroids, the k-means clustering objective can be expressed in terms of both cosine distance and Euclidean distance. The equivalence:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k)
\]
enables seamless interpretation and implementation in the algorithm.








\subsubsection{Direct day 7 to day 10 distance evaluation}
calculate distance between day 7 untreated and its corresponding day 10 treated one.
Idea is:
1. it should give same distance between day 7 and its corresponding day 10
 even its changed in position/flipped. ie checking whether simlcr learned to 
 be invariant to position error in microscope error because of manual 
 handling/transporting.
2. it sould give same distance even if its blured/sharpened
3. it should give sma edistance even if its changed in brightness
4. main test for k means centroid approach. it should give different distance to single dose and exploded.