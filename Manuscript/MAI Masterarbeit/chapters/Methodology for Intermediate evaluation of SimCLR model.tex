\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

The final evaluation of the SimCLR model depends on the ranking task. Nevertheless, other evaluation metrics, such as downstream tasks like classification and clustering, can be used to verify if SimCLR has effectively learned to differentiate between control, single dose, and explode images.

\section{Classification using Logistic Regression on the SimCLR features and original image vectors}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single linear layer that maps these representations to class predictions, where the three categories, 'control' and 'single dose', 'explode' serve as our classes. The Logistic regression model can only perform well if the learned representations capture all the relevant features necessary for the task. Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
 

2. Baseline comparison to original images. We classify the original image vectors to see if SimCLR feature vectors are better or worse than original image to classify.

The logistic regression model is implemented as a single linear layer, where the input dimension corresponds to the feature dimension of feature spits out from SimCLR, and the 
output dimension corresponds to the number of classes. Specifically, the model uses a feature dimension of \(512\) if the feature is before projection head or  \(20\) if the
 feature is after projection head and outputs predictions for \(3\) classes. The mathematical representation of the model is as follows:

\[
\hat{y} = xW^T + b
\]

where:

- \(x \in \mathbb{R}^{N \times d}\) is the input feature vector, where \(d = 512\) if the feature is extracted before the projection head, or \(d = 20\) if the feature is 
extracted after the projection head.  
- \(W \in \mathbb{R}^{3 \times d}\) represents the learnable weights.  
- \(b \in \mathbb{R}^3\) is the bias term,
- \(\hat{y} \in \mathbb{R}^{N \times 3}\) are the logits representing the unnormalized class scores for \(N\) samples.

This model is trained to minimize the cross-entropy loss for multi-class classification.
3 classes trained for 250 epochs. The dataset was divided into a training set (80\%) and a validation set (20\%). Batch size = 8.  A learning rate of \( 5 \times 10^{-4} \). 

The learning rate scheduler used is the \texttt{MultiStepLR}, which reduces the learning rate at specific milestones during training. In this case, the learning rate is 
reduced by a factor of 0.1 at the epochs corresponding to 60\% and 80\% of the total training epochs. These milestones are defined as:
\[
\text{milestones} = \left[ \text{int}(T_{\text{max}} \times 0.6), \, \text{int}(T_{\text{max}} \times 0.8) \right]
\]
where \( T_{\text{max}} \) represents the total number of training epochs. The \texttt{gamma} parameter specifies the factor by which the learning rate is multiplied at each 
milestone, which in this case is 0.1.

\begin{table}[h!]
    \centering
    \caption{Dataset Summary from Drug Screening Experiments}
    \label{tab:dataset_summary}
    \begin{tabular}{lc}
    \toprule
    \textbf{Dataset}                              & \textbf{Total Number of Images} \\ 
    \midrule
    Control Dataset                             & 472                            \\ 
    Single Dose Dataset                           & 103                            \\ 
    Explode dataset from Drug Screening Experiments & 40                             \\ 
    \bottomrule
    \end{tabular}
\end{table}
Table \ref{tab:dataset_summary} shows the total number of images in each dataset used for classification. Additional images from drug screening experiments were not included, as their class labels are uncertain. Some images exhibit low to medium resemblance to the three defined classes, while others differ significantly, making it difficult to use them for classification task with confidence.
\subsection{Comparison of Classification Accuracy and Epochs for Different Data Augmentations}

As explained in the data augmentation section, we use 'strong,' 'sweet,' 'resize,' 'sweet no contrast,' and 'resize no contrast' SimCLR features to compare their performance against each other and the original images in classifying the three defined classes. We calculate the training accuracy and testing accuracy for each data augmentation pipeline as well as for the original image vectors. Train epoch in the table: The number of epochs required to reach the best training accuracy.
Test epoch in the table: The number of epochs required to reach the best test accuracy.
To use original images directly first we resized to 96*96 since we did that for simclr so that it could be fair comparison. Then we normalised each by dividing it by 65535 
(16 bit). Each image flatten into vectors of ([1,96*96*3]where 96 =H=W and 3 = no of channels) suitable for logistic regression model. and we use same parameters that we used 
for simclr feats classification lke learning rate , batch size etc for fair comparison.
From table \ref{tab:augmentation_metric} and table \ref{tab:original_image_results} we show that when using the features Before Projection Head all augmentation methods reached 100 train and test accuracy. And when using the features original image vectors both accuracy reached upto 99 percentage. So comparing before projection head SimCLR features to original image vectors there is only subtle difference. Their almost indistinguishable performance leads to the conclusion that it is difficult to determine whether SimCLR features have been learned efficiently from classification task. But it is clear that the features extracted before the projection head perform far better than those extracted after the projection head, as seen in the SimCLR \Cite{chen2020simple}paper. Also, the number of training and test epochs required to reach the best accuracy is lower for the features extracted before the projection head than those extracted after the projection head as we seen in figures \ref{fig:test} and \ref{fig:train}. Interestingly, original image vectors performed better than the after projection head SimCLR features.

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llcccccc}
    \toprule
    \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
    \multirow{4}{*}{\textbf{After Projection Head}}  
        & Train Accuracy (\%) & 66.67 & 47.76 & 89.63 & 63.41 & 54.67 \\
        & Train Epoch         & 246   & 249   & 250   & 250   & 245   \\
        & Test Accuracy (\%)  & 68.29 & 55.28 & 90.24 & 63.41 & 56.10 \\
        & Validation Epoch          & 228   & 246   & 223   & 202   & 244   \\ \midrule
    \multirow{4}{*}{\textbf{Before Projection Head}} 
        & Train Accuracy (\%) & 100 & 100 & 100 & 100 & 100 \\
        & Train Epoch         & 190    & 3     & 13     & 12     & 21     \\
        & Validation Accuracy (\%)  & 100 & 100 & 100 & 100 & 100 \\
        & Validation Epoch          & 1      & 1      & 1      & 2      & 2      \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for different augmentation strategies before and after the projection head using SimCLR}
    \label{tab:augmentation_metric}
\end{table}

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Validation Accuracy (\%)} & \textbf{Validation Epoch} \\ \midrule
    \textbf{Original Image} & 99.39                        & 249                  & 99.19                        & 17                  \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for original image vectors}
    \label{tab:original_image_results}
\end{table}

Since the train and test accuracy are both 100 percent for before projection head features, which may indicate overfitting, I performed a 10-fold cross-validation as an additional check to ensure the model's performance generalizes well. The results are shown in Tables \ref{tab:aug_metric} and \ref{tab:original_results}. The train and test accuracy for the features before the projection head drops from 100 percent to around 99 percent, and for the original image vectors, it drops from 99 percent to approximately 96 percent. This shows that the trend of the features before the projection head being slightly superior, as observed in earlier results, is also reflected here. Additionally, the after-projection head features still perform considerably worse than the before-projection head and original image vectors. Comparing this with classification using the ResNet18 architecture shows that the before-projection head SimCLR feature vectors and ResNet18 feature vectors achieve similar performance. However, between the data augmentation pipelines when using the features before the projection head, it is difficult to evaluate which one is better, as all of them show subtle differences in accuracy.
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llcccccc}
    \toprule
    \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
    \multirow{2}{*}{\textbf{After Projection Head}}  
        & Average train Accuracy (\%) & 35.17 & 36.60 & 49.44 & 35.28 & 34.57 \\
        & Average test Accuracy (\%)  & 33.48 & 34.62 & 54.43 & 33.77 & 34.65 \\ \midrule
    \multirow{2}{*}{\textbf{Before Projection Head}} 
        & Average train Accuracy (\%) & 99.81 & 99.90 & 99.86 & 99.90 & 99.89 \\
        & Average validation Accuracy (\%)  & 99.71 & 99.77 & 99.49 & 99.72 & 99.61 \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for different augmentation strategies before and after the projection head using cross validation with 10 k folds}
    \label{tab:aug_metric}
\end{table}

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcc}
    \toprule
    \textbf{Metric}         & \textbf{Logistic Reg Average Accuracy (\%)} & \textbf{ResNet18 Average Accuracy (\%)} \\ \midrule
    \textbf{Train}          & 96.73                                                     & 99.86                                             \\ 
    \textbf{Validation}     & 96.50                                                     & 99.93                                              \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for original image vectors using cross-validation with 10 k folds}
    \label{tab:original_results}
\end{table}
          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/test_class.png} 
            \caption{Test accuracy vs Test epochs of original image vectors and Simclr features}
            \label{fig:test}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/train_class.png} 
            \caption{Train accuracy vs Train epochs of original image vectors and Simclr features }
            \label{fig:train}
          \end{figure}
\subsection{Inference}

Twenty-two images that closely resemble single-dose images from drug screening were manually selected for inference and refer them as 'inference images'. These images were not labeled by biology experts, and therefore, any inference results do not correspond to the actual ground truth. For the inference process, features before the projection head were utilized, as they achieved 100 percent accuracy during training and validation.
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{Type} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize no Contrast} & \textbf{Sweet no Contrast} \\ \midrule
    Before Projection Head & 90.91 & 100.00 & 100.00 & 100.00 & 95.45 \\ \midrule
    Original Images & & & & 68.18 & \\ \bottomrule
    \end{tabular}
    \caption{Performance Metrics Before and After the Projection Head}
    \label{tab:performance_metrics}
\end{table}

From the inference experiment table \ref{tab:performance_metrics}, it can be seen that, except for 'strong' and 'sweet no contrast', all other augmentations were able to achieve 100 percent accuracy in classifying the inference images as belonging to the single-dose image class. Using the original image features, the accuracy was 68.18 percent, while SimCLR features before the projection head achieved at least 90 percent accuracy.
\section{K-means clustering}

The idea is to determine whether the learned representation from SimCLR outperforms the original images in clustering the images in an unsupervised manner. To achieve this, we use simple K-means clustering as explained in \cite{bishop:2006:PRML}. We use both euclidean distance and cosine distance to find the nearest data point to the cluster centroid as two different approach. When performing K-means with cosine distance, it compares only the direction of the vector. Therefore, we also perform standard K-means with Euclidean distance to evaluate the magnitude similarity. 

\subsection{Evaluation}

Full dataset (unbalanced): contains all control(untreated): 472, all single dose: 103, all exploded: 40 images, all drug screen visualy similar/closer to single dose.

40 subset (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded): contains random but make sure it have exploded like 
controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

Curated Full dataset (unbalanced): contains all control except controls have debris: 280, all single dose: 103, all exploded: 40 images, all drug screen visualy 
similar/closer to single dose: 22.

Curated 40 subset  (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded but excluded explod look alike from control):
contains random but make sure it doesn't have exploded like controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/controlex.png} 
    \caption{orig}
    \label{fig:controlex.png}
\end{figure}

\begin{table}[H]
\centering
\caption{Summary of Datasets}
\label{tab:dataset_summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}             & \textbf{Control (C)} & \textbf{Single Dose (SD)} & \textbf{Exploded (E)} & \textbf{SD close for inference} \\ \midrule
Full Dataset (Unbalanced)    & 472                  & 103                       & 40                    & all 22                          \\ 
40 Subset (Balanced)         & (20 with debris + 20 without debris)                   & 40                        & 40                    &    20          \\ 
Curated Full Dataset         & 280                  & 103                       & 40                    & all 22                                \\ 
Curated 40 Subset (Balanced) & 40 (all without debris)                  & 40                        & 40                    & 20           \\ 
\bottomrule
\end{tabular}%
}
\end{table}

    

run 400 times for different random initilisation. Note: Drug screen images are visually similar to single dose treatment. All balanced datasets
 are normalized to the exploded group size (n=40).
 \textcolor{red}{the curated full data set doesn't give any infromation ? if no remove the column}
 \textcolor{red}{class imbalnce problem nn orappich parayanda , say this could be  because of weight imbalance problem.}

    \begin{table}[H]
        \centering
        \caption{Evaluation Results on Different Datasets and Augmentations with cosine distance}
        \label{tab:evaluation_results_cosine_distance}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
            \multirow{5}{*}{\textbf{Before}} 
            & Strong                & \textbf{83.90} & \textbf{100} & \textbf{100} & \textbf{100} \\
            & Sweet                 & \textbf{99.18} & \textbf{100} & \textbf{100} & \textbf{100} \\
            & Resize                & 65.7 & 97.5 & 97.87 & 100 \\
            & Resize No Contrast    & 69.92 & 90 & 86.76 & 100 \\
            & Sweet No Contrast     & \textbf{98.21} & \textbf{99.17} & \textbf{99.52}  & \textbf{99.17} \\ \midrule
            \multirow{5}{*}{\textbf{After}} 
            & Strong                & 72.68 & 95.83 & 74.00 & 100 \\
            & Sweet                 & 47.32 & 74.17 & 55.08 & 78.33 \\
            & Resize                & 60.32 & 92.50 & 74.23 & 100 \\
            & Resize No Contrast    & 44.72 & 64.17 & 47.28 & 66.67 \\
            & Sweet No Contrast     & 52.20 & 86.67 & 58.16 & 89.17 \\ \bottomrule
        \end{tabular}% 
        }
    \end{table}
    

cosine distance:

The results show a clear trend of superior clustering performance before the projection head compared to after the projection head.

Before: 

data augmentatoin ‘sweet’ performs the best over other data augmentations and achieved consistant accuracy over all different data set types. 

since sweet and strong performed 100 percentage accurcay across cured and uncured balanced dataset its evidant that the problem for having less accracy is nothing
 to do with exploeded look like images in control instead its weight unbalance class problem. 

comparing 'sweet' and 'strong' they both achieved 100 percenatage accuracy ie (same performance)over all different data set types except the full dataset.
for full dataset sweet performed far better than strong , sweeet even achieved 99.18 where strong reached only 83.90.

comparing all data augmentations 'resize' and 'reesize no contrast performed the least giving the insight that cropping is essential to get better cluster performance
 using cosine distance as distance metric.

 as a breif conclusion its pretty clear that cropping 0.35 to 1 percentage of whole image is crucial especially cropping have huge effect for cluster performance using cosine distance as distacne metric.

 One thing that need to keep in mind that all above explanations or achievements by different data augs are corresponds to clustering performance doesn't mean that
  it could work for rankin strategies too.
    
  \begin{table}[H]
    \centering
    \caption{Evaluation Results on Different Datasets and Augmentations with euclidean distance}
    \label{tab:evaluation_results_euclidean}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & \textbf{88.45} & \textbf{100} & \textbf{100} & \textbf{100} \\
        & Sweet                 & \textbf{98.21} & \textbf{91.67} & \textbf{99.29} & \textbf{95.83} \\
        & Resize                & 70.40 & 78.33 & 66.66 & 88.33 \\
        & Resize No Contrast    & 68.62 & 90.00 & 86.28 & 100 \\
        & Sweet No Contrast     & \textbf{99.02} & \textbf{97.50} & \textbf{99.76} & \textbf{99.17} \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & \textbf{75.45} & \textbf{100} & \textbf{99.53} & \textbf{100} \\
        & Sweet                 & 51.54 & 74.17 & 60.52 & 79.17 \\
        & Resize                & 43.25 & 83.33 & 53.90 & 87.5 \\
        & Resize No Contrast    & 47.8 & 63.33 & 47.04 & 65.83 \\
        & Sweet No Contrast     & 51.87 & 85.0 & 57.68 & 88.33 \\ \bottomrule
    \end{tabular}% 
    }
\end{table}

    
Euclidean distance:
Just like  cosine distance, with eulcidean distacne as distacne metric also established a superior performance trend for before the projection head compared to after the
 projection head.
 Just like  cosine distance, inside the before projection head, data augs with cropping have far better performance than without cropping dataaugs such as resize and 
 resize no contrast.

 Eventhough 'strong' achieved 100 percentage accuracy across all dataset types except the full dataset unbalanced. when it comes to full dataset, sweet and sweet nocontrast pperforms better than all others.
 and among all data augs we can see sweet no contrast' maintain the cosistancey in performance across all dataset types with more than 97 percentage accuracy and also perfromed better than just 'sweet' across all dataset.
 
 interesting thing to notice here is the after projection head  'strong' it achieved higher clusterig performance 99 and above across all dataset types except 
 the full dataset. which is a good indication that we should also consider the after projection head features for ranking task.

    \begin{table}[H]
        \centering
        \caption{Evaluation Results Using Different Distance Metrics for original images}
        \label{tab:distance_metrics}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            & \textbf{Metric} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset (Unbalanced)} & \textbf{Curated 40 Subset (Balanced)} \\ \midrule
            \multirow{2}{*}{\textbf{Original Images}} 
            & Cosine Distance    & 62.76 & 72.5 & 58.15 & 77..50 \\ 
            & Euclidean Distance & 55.28 & 76.67 & 53.66 & 77.50 \\ \bottomrule
        \end{tabular}%
        }
    \end{table}
    
Original images:
comparing original images feats to simclr feats we can see that entire data augs for  before head feats outperformed entirly for both cosine distacne and euclidean distacne.
while comparing original image to after projection head the performance is comparable except for the strong data aug where after head still have better perfromance.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cluster.png} 
    \caption{orig}
    \label{fig:cluster}
\end{figure}



\textcolor{red}{Include PCA 1 here? maybe in ranking. but we can add PCA 2 fugures.}
\textcolor{red}{remove inference data from table.}

\textcolor{red}{do sd VS others all? yes depend on time}

Inference:



Since before projection head for cosine distance gives far better performance, i do  inference on them. I choose 22 images that have close resemblence in my eyes 
to single dose images from drug screen for inference. We name those images as 'sd close'( I repeat these are not biology expert labeled drug screened image. so any inference result is irrelavant
 in the sense to actual ground truth )

we used all 22 sd close images for full dataset along with the 3 classes that describe in the table. But for balanced datasets except control and exploded
 we changed the  dataset setup described in the table as follows for keeping the dataset balancing: 20 sd and 20 sd close images. so that it will stil be 40 as other classes.



 so during 3 class clustering 'sweet' (99.18 ) perfromed better than sweet no contrast (98.21) for all full dataset unbalanced. and in others sweet achieved 100 percentage accuracy
 whihc makes clear that for 3 class problem sweet have higher perfromacne comparing to other data augs. but during inference sweet accuracy reduced to 95.76  with -3.42 
 change in reduction while sweet no contrast reduced to  97.33 with -0.88 reduction change. and sweet no contrast kept the highest accuracy during inference. figure 8.8 shows that 
 with sweet no contrast data aug it classified all 103 sd images correctly and during inference it clustered all sd close image images in the cluster of sd images.
 
 

 \begin{table}[H]
    \centering
    \caption{Evaluation Results on COSINE}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 85.09 (+1.19) & 100 (0) & 98.43 (-1.57) & 100 (0) \\ 
        & Sweet              & 95.76 (-3.42) & 100 (0) & 100 (0) & 100 (0) \\ 
        & Resize             & 67.03 (+1.33) & 97.50 (0) & 98.42 (+0.55) & 100 (0) \\ 
        & Resize No Contrast & 71.89 (+1.97) & 90.83 (+0.83) & 87.42 (+0.66) & 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{97.33 (-0.88)} & \textbf{98.33 (-0.84)} & \textbf{99.55 (+0.03)} & \textbf{99.17 (0)} \\ 
        \bottomrule
    \end{tabular}% 
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cconfusion.png} 
    \caption{orig}
    \label{fig:cconfusion}
\end{figure}

\textcolor{red}{How do you know in inference dsclose was the one confused? maybe confusion matrix?  sweet no contrast before cosine}

\begin{table}[H]
    \centering
    \caption{Evaluation Results on Euclidean}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 84.61 (-3.84) & 100 (0) & 99.10 (-0.9)& 100 (0) \\ 
        & Sweet              & 98.27 (+0.06) & 91.67(0) & 99.32(+0.03) & 95.83 (0) \\ 
        & Resize             & 72.06 (+1.66) & 79.17 (+0.84) & 68.54 (+1.88) & 86.67(-1.66) \\ 
        & Resize No Contrast & 71.11 (+2.49) & 100 (+10) & 87.42 (1.14)& 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{99.06 (+.04)} & \textbf{97.50 (0)} & \textbf{99.78 (+.02)} & \textbf{99.17 (0)} \\ 
        \midrule
        \textbf{After Projection Head} & Strong & 75.51 (+0.06) & 92.5 (-7.5)& 72.58 (-26.95) & 95 (-5 )\\  
        \bottomrule
    \end{tabular}% 
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/econfuse.png} 
    \caption{orig}
    \label{fig:econfuse.png}
\end{figure}


For euclidean distance, during 3 class 'sweet no contrast' have he highest performance with 99.02 accuracy without any misclassification for sd images as we seen 
in figure 8.9. and during inference it clustered all sd close image and sd images together in the cluster of sd images corrrectly as we seen in figure 8.9.


From this we can conclude that 'sweet no contrast' data aug before projection head is the best data aug for clustering task for both
 cosine distance and euclidean distance as distance metric. which basically means that sweet cropping of 0.35 to 1 (less intense cropping is essential for clustering performance.)



