\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

\textcolor{red}{change the name to evaluatoin to check whether simclr learned something? instead of intermediate evaluatoin?}  

If we do kmeans with cosine distance distance then it only compare direction of the vector thats why we need to do normal kmeans with euclidean dist to show 
the magnitude similarity

Final evaluation of the SimCLR model depends on the  Ranking task, neverthless we can use other evaluation metrics, such as downstream task 
like classification  and clusetring to check if simclr atleast learned to differentiate between untreated , single dose and exploded images.

Distance measurement approach is used to whats the performance of simclr feats derived from  5 different aug pipelines  in terms of cosine distance between 2 augs derived 
from same image.

\section{Classification using Logistic Regression on the SimCLR features}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single, linear layer that maps these representations to class predictions, where the three categories, 'untreated' and 'single dose', 'explod'  
 serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. 
 Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
 

2. Baseline comparison to original images. We classify the original images to see if simclr feature vectors are better or worse than original image to classify.

The logistic regression model is implemented as a single linear layer, where the input dimension corresponds to the feature dimension of feature spits out from simclr, and the 
output dimension corresponds to the number of classes. Specifically, the model uses a feature dimension of \(512\) if the feature is before projection head or  \(20\) if the
 feature is after projection head and outputs predictions for \(3\) classes. The mathematical representation of the model is as follows:

\[
\hat{y} = xW^T + b
\]

where:

- \(x \in \mathbb{R}^{N \times d}\) is the input feature vector, where \(d = 512\) if the feature is extracted before the projection head, or \(d = 20\) if the feature is 
extracted after the projection head.  
- \(W \in \mathbb{R}^{3 \times d}\) represents the learnable weights.  
- \(b \in \mathbb{R}^3\) is the bias term,
- \(\hat{y} \in \mathbb{R}^{N \times 3}\) are the logits representing the unnormalized class scores for \(N\) samples.

PyTorch's CrossEntropyLoss combines the softmax operation with the log loss computation for numerical stability, which is why we pass the logits (unnormalized outputs) directly
to the loss function.

This model is trained to minimize the cross-entropy loss for multi-class classification.
3 classes trained for 250 epochs. The dataset was divided into a training set (80\%) and a validation set (20\%). Batch size = 8. loss function = 
cross entropy loss.  a learning rate of \( 5 \times 10^{-4} \). 

The learning rate scheduler used is the \texttt{MultiStepLR}, which reduces the learning rate at specific milestones during training. In this case, the learning rate is 
reduced by a factor of 0.1 at the epochs corresponding to 60\% and 80\% of the total training epochs. These milestones are defined as:
\[
\text{milestones} = \left[ \text{int}(T_{\text{max}} \times 0.6), \, \text{int}(T_{\text{max}} \times 0.8) \right]
\]
where \( T_{\text{max}} \) represents the total number of training epochs. The \texttt{gamma} parameter specifies the factor by which the learning rate is multiplied at each 
milestone, which in this case is 0.1.

\begin{table}[h!]
    \centering
    \caption{Dataset Summary from Drug Screening Experiments}
    \label{tab:dataset_summary}
    \begin{tabular}{lc}
    \toprule
    \textbf{Dataset}                              & \textbf{Total Number of Images} \\ 
    \midrule
    Untreated Dataset                             & 472                            \\ 
    Single Dose Dataset                           & 103                            \\ 
    Exploded Dataset from Drug Screening Experiments & 40                             \\ 
    \bottomrule
    \end{tabular}
\end{table}

I didn't add any other comibnation of drug screening experiments since we are not sure whether they belong to which group. Some of them have medium resemblence of the
 above 3 classes but we can't be sure.


\subsection{Comparison of Classification Accuracy and Epochs for Different Data Augmentations}

As explained in Data augmentation section we use  'strong', 'sweet', 'resize', 'sweet no contrast', 'resize no contrast' simclr features to compare their performance inbetween
 and also against original images which one classify these 3 classes better.

We calculate the train accuracy and the test accuracy for each data aug pipelines as well as for original images.

Train epoch in the table: The number of epochs required to reach the best training accuracy.
Test epoch in the table: The number of epochs required to reach the best test accuracy.

\textcolor{red}{how do we turn raw images to orig images to feats}
To use original images directly first we resized to 96*96 since we did that for simclr so that it could be fair comparison. Then we normalised each by dividing it by 65535 
(16 bit). Each image flatten into vectors of ([1,96*96*3]where 96 =H=W and 3 = no of channels) suitable for logistic regression model. and we use same parameters that we used 
for simclr feats classification lke learning rate , batch size etc for fair comparison.


\textcolor{red}{Table 8.1.1 - are you saying that you classification accuracy is 100?
 That seems far too good to be true. Why do the train runs have different number of epochs? And what do you mean by "Test Epoch"?!}


 \textcolor{red}{general comment to tables with result - after giving the tables you need to write a text which helps us to interpret it. What are the most important 
 number we shall look at? What shall we take out from the table as a message. For example something like (please do not use this directly. I have no clue 
 if this is the message you want us to take away from the table) "In Table 8.1. we show that when using the features Before Projection Head all 
 augmentation methods reached 100 train and test accuracy. The Resize approach needed the fewest epochs."}
    
    \begin{table}[h!]
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{llcccccc}
        \toprule
        \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
        \multirow{4}{*}{\textbf{After Projection Head}}  
            & Train Accuracy (\%) & 66.67 & 47.76 & 89.63 & 63.41 & 54.67 \\
            & Train Epoch         & 246   & 249   & 250   & 250   & 245   \\
            & Test Accuracy (\%)  & 68.29 & 55.28 & 90.24 & 63.41 & 56.10 \\
            & Validation Epoch          & 228   & 246   & 223   & 202   & 244   \\ \midrule
        \multirow{4}{*}{\textbf{Before Projection Head}} 
            & Train Accuracy (\%) & 100 & 100 & 100 & 100 & 100 \\
            & Train Epoch         & 190    & 3     & 13     & 12     & 21     \\
            & Validation Accuracy (\%)  & 100 & 100 & 100 & 100 & 100 \\
            & Validation Epoch          & 1      & 1      & 1      & 2      & 2      \\ 
        \bottomrule
        \end{tabular}%
        }
        \caption{Performance metrics for different augmentation strategies before and after the projection head.}
        \label{tab:augmentation_metric}
        \end{table}
        

        \begin{table}[h!]
            \centering
            \caption{Original Image Results}
            \label{tab:original_image_results}
            \begin{tabular}{lcccc}
            \toprule
            \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Validation Accuracy (\%)} & \textbf{Validation Epoch} \\ \midrule
            \textbf{Original Image} & 99.39                        & 249                  & 99.19                        & 17                  \\ 
            \bottomrule
            \end{tabular}
        \end{table}
           
Train loss vs Val loss . Train accurcay  vs val accuracy 
          

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.95]{figures/before.pdf} 
            \caption{before}
            \label{fig:before}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.95]{figures/after.pdf} 
            \caption{after}
            \label{fig:after}
          \end{figure}

          
          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.425]{figures/orig_classi.png} 
            \caption{orig}
            \label{fig:orig}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/test_class.png} 
            \caption{before}
            \label{fig:before}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/train_class.png} 
            \caption{before}
            \label{fig:before}
          \end{figure}
\subsection{Inference}

I choose my self 22 images that have close resemblence in my eyes to single dose images from drug screen for inference. ( I repeat these are not biology expert labeled drug 
screened image. so any inference result is irrelavant in the sense to actual ground truth )

For doing inferecne I choosed before head projection features since thats the one gave 100 perentage accuracy while train and validation.


\begin{table}[H]
    \centering
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{Type} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize no Contrast} & \textbf{Sweet no Contrast} \\ \midrule
    Before Projection Head & 90.91 & 100.00 & 100.00 & 100.00 & 95.45 \\ \midrule
    Original Images & & & & 68.18 & \\ \bottomrule
    \end{tabular}
    \caption{Performance Metrics Before and After the Projection Head}
    \label{tab:performance_metrics}
\end{table}



From the above table we can see that during the inference except 'strong' and 'sweet no contrast' all other augs still able to reach 100 percentage accuracy to classify the ds 
close to sd images to sd based on visual resemblence. Using raw Original images features it got  68.18 percentage accuracy while Simclr before projectoin head could classify atleast 90 percentage.


\textcolor{red}{table 8.2 can you somehow bring this into table 8.1 e.g. as another column? Would be easier to read. The first column 
"Augmentation Type" could be made smaller if you wrap the text so that it is on multiple lines.} 


In Table 8.1. we show that when using the features Before Projection Head, all augmentation methods reached 100 percent train and test accuracy. It is clear that the features extracted 
before the projection head perform far better than those extracted after the projection head, as seen in the original similar paper. Also, the number of epochs required
to reach the best accuracy is lower for the features extracted before the projection head than those extracted after the projection head.

While original images also classify with 99 percent accuracy, they perform better than the features we use after projection. Also, their almost indistinguishable performance 
from the features extracted before the projection head leads to the conclusion that we cannot make an informed decision based on this classification task.



\section{kmeans clustering}

Idea is whether the learned representation from simlcr outperforms the original images in clustering the images (unsupervised manner) For that we use simple kmeans 
clusetring. We will cluster them based on both euclidean distance as well as cosine distance.





\subsection{Evaluation}

Full dataset (unbalanced): contains all control(untreated): 472, all single dose: 103, all exploded: 40 images, all drug screen visualy similar/closer to single dose.

40 subset (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded): contains random but make sure it have exploded like 
controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

Curated Full dataset (unbalanced): contains all control except controls have debris: 280, all single dose: 103, all exploded: 40 images, all drug screen visualy 
similar/closer to single dose: 22.

Curated 40 subset  (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded but excluded explod look alike from control):
contains random but make sure it doesn't have exploded like controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

\begin{table}[H]
\centering
\caption{Summary of Datasets}
\label{tab:dataset_summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}             & \textbf{Control (C)} & \textbf{Single Dose (SD)} & \textbf{Exploded (E)} & \textbf{DS Closer to SD (DS-SD)} \\ \midrule
Full Dataset (Unbalanced)    & 472                  & 103                       & 40                    & Included                          \\ 
40 Subset (Balanced)         & 40                   & 30                        & 40                    & 10                                \\ 
Curated Full Dataset         & 280                  & 103                       & 40                    & 22                                \\ 
Curated 40 Subset (Balanced) & 40                   & 30                        & 40                    & 10                                \\ 
\bottomrule
\end{tabular}%
}
\end{table}

    

run 100 times for different random initilisation.

\begin{table}[htbp]
    \centering
    \caption{Dataset Composition Overview}
    \begin{tabular}{lccccl}
    \toprule
    Dataset Type & Control & Single Dose & Exploded & Drug Screen & Notes \\
    \midrule
    \multicolumn{6}{l}{\textbf{Raw Datasets}} \\
    \midrule
    Full & 472 & 103 & 40 & -- & Unbalanced \\
    Balanced-40 & 40 & 30 & 40 & 10 & Minimum set balanced \\
    \midrule
    \multicolumn{6}{l}{\textbf{Curated Datasets}} \\
    \midrule
    Full & 280 & 103 & 40 & 22 & Debris-free controls \\
    Balanced-40 & 40 & 30 & 40 & 10 & No exploded-like controls \\
    \bottomrule
    \end{tabular}
    \begin{flushleft}
    \small
    Note: Drug screen images are visually similar to single dose treatment. All balanced datasets are normalized to the exploded group size (n=40).
    \end{flushleft}
    \end{table}

    \begin{table}[H]
        \centering
        \caption{Evaluation Results on Different Datasets and Augmentations with cosine distance}
        \label{tab:evaluation_results_cosine_distance}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
            \multirow{5}{*}{\textbf{Before}} 
            & Strong                & 99.18 & 100 & 100 & 100 \\
            & Sweet                 & 99.18 & 100 & 100 & 100 \\
            & Resize                & 65.7 & 97.5 & 97.87 & 100 \\
            & Resize No Contrast    & 69.92 & 90 & 86.76 & 100 \\
            & Sweet No Contrast     & 96.74 & 99.17 & 99.52  & 99.17 \\ \midrule
            \multirow{5}{*}{\textbf{After}} 
            & Strong                & 60.04 & 92.5 & 55.08 & 100 \\
            & Sweet                 & 48.13 & 67.5 & 53.65 & 71.67 \\
            & Resize                & 59.83 & 92.50 & 74.23 & 100 \\
            & Resize No Contrast    & 45.85 & 58.33 & 45.62 & 62.5 \\
            & Sweet No Contrast     & 49.26 & 82.5 & 57.21 & 85.83 \\ \bottomrule
        \end{tabular}%
        }
    \end{table}
    
    \begin{table}[H]
        \centering
        \caption{Evaluation Results on Different Datasets and Augmentations with euclidean distance}
        \label{tab:evaluation_results_euclidean}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
            \multirow{5}{*}{\textbf{Before}} 
            & Strong                & 83.74 & 100 & 100 & 100 \\
            & Sweet                 & 98.05 & 91.67 & 99.29 & 95.83 \\
            & Resize                & 70.24 & 79.17 & 66.66 & 87.50 \\
            & Resize No Contrast    & 59.51 & 90.00 & 86.28 & 100 \\
            & Sweet No Contrast     & 99.02 & 97.50 & 99.76 & 99.17 \\ \midrule
            \multirow{5}{*}{\textbf{After}} 
            & Strong                & 71.70 & 100 & 72.57 & 100 \\
            & Sweet                 & 47.00 & 68.33 & 52.71 & 74.17 \\
            & Resize                & 43.25 & 83.33 & 53.90 & 87.5 \\
            & Resize No Contrast    & 48.45 & 59.17 & 44.20 & 60.83 \\
            & Sweet No Contrast     & 49.75 & 81.67 & 52.00 & 81.67 \\ \bottomrule
        \end{tabular}%
        }
    \end{table}
    
    \begin{table}[H]
        \centering
        \caption{Evaluation Results Using Different Distance Metrics}
        \label{tab:distance_metrics}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            & \textbf{Metric} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset (Unbalanced)} & \textbf{Curated 40 Subset (Balanced)} \\ \midrule
            \multirow{2}{*}{\textbf{Original Images}} 
            & Cosine Distance    & 60.16 & 69.17 & 58.15 & 70.00 \\ 
            & Euclidean Distance & 55.28 & 72.5 & 53.65 & 75.00 \\ \bottomrule
        \end{tabular}%
        }
    \end{table}
    



\textcolor{red}{Include PCA 1 here? maybe in ranking. but we can add PCA 2 fugures}

\textcolor{red}{do sd VS others all?}

Inference

\begin{table}[H]
    \centering
    \caption{Evaluation Results on COSINE}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 92.62 & 100 & 100 & 100 \\ 
        & Sweet              & 98.74 92.00 & 100 & 100 & 100 \\ 
        & Resize             & 67.03 & 97.50 & 98.42 & 100 \\ 
        & Resize No Contrast &  71.89    & 90.83    & 87.42     & 100 \\ 
        & Sweet No Contrast  & 97.33     & 98.33    & 99.55     & 99.17   \\ 
        \midrule
        \multirow{2}{*}{\textbf{After Projection Head}} 
        & Strong             & -     & -    & -     & 94.17 \\ 
        & Resize             & -     & -    & -     & 100    \\ 
        \bottomrule
    \end{tabular}%
    }
\end{table}

\textcolor{red}{How do you know in inference dsclose was the one confused? maybe confusion matrix?  sweet no contrast before cosine}

\begin{table}[H]
    \centering
    \caption{Evaluation Results on Euclidean}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 80.69 & 100 & 99.10 & 100 \\ 
        & Sweet              & 98.12 & 91.67 & 99.32 & 95.83 \\ 
        & Resize             & 77.08 & 79.17 & 70.78 & 86.67 \\ 
        & Resize No Contrast & 68.28 & 100 & 83.59 & 100 \\ 
        & Sweet No Contrast  & 99.06 & 97.50 & 9.78 & 99.17 \\ 
        \midrule
        \multirow{2}{*}{\textbf{After Projection Head}} 
        & Strong             & - & 92.5 & - & 95 \\  
        \bottomrule
    \end{tabular}% 
    }
\end{table}



\section{Direct day 7 to day 10 distance evaluation}\label{sec: distance}
calculate distance between day 7 untreated and its corresponding day 10 treated one.
Idea is:
1. it should give same distance between day 7 and its corresponding day 10
 even its changed in position/flipped. ie checking whether simlcr learned to 
 be invariant to position error in microscope error because of manual 
 handling/transporting.
2. it sould give same distance even if its blured/sharpened
3. it should give sma edistance even if its changed in brightness
4. main test for k means centroid approach. it should give different distance to single dose and exploded.


basic evaluations for ranking:
1.
whether it learned to be invariant the position change:
do flipps and calculate the cosine distance from control to treated flipped versions. ( i don't expect it learn to the change in center of position, 
if it learns good we can say center crop have some effect maybe? but not for the edge one?)
2. 
shape invariant:control to all single dose should be almost same cosine distance.

it also applicable to time prediction and reconstruction ranking evaluation and also from kmeans centriod approach.


\textcolor{red}{We will start with cosine distance and euclidean distance. other distcances based on time if we have}


\textcolor{red}{we are trying to find whether simclr learns something, from simclr side cond 7 to same cond7 augs should have high cosine similarity and within all
 gps foe example within sd higher cosine similarity since its gping this is what we have to check but we will do this only after ranking strategy depend on time}


 \textcolor{red}{then if have time we will do cond7 to cond10, sd stuuf}

 \begin{table}
 \centering
 \begin{tabular}{ll*{5}{l}}
 \toprule
 \multicolumn{2}{l}{Augmentation type} & Strong & Sweet & Resize & No contrast Resize & No contrast sweet \\ 
 \midrule
 & Flip and rotation & 0.1932 & 0.0109 & 0.134 & 0.0120 & 0.0108 \\
 Before & Blur and sharpness & 0.0061 & 0.0185 & 0.171 & 0.0024 & 0.0058 \\
 & Brightness change & 0.0003 & 0.0037 & 0.184 & 0.0013 & 0.0043 \\
 \midrule
 & Flip and rotation & 0.3435 & 0.0204 & 0.0747 & 0.0113 & 0.0212 \\
 After & Blur and sharpness & 0.0108 & 0.0388 & 0.2087 & 0.0047 & 0.0133 \\
 & Brightness change & 0.0005 & 0.0067 & 0.2864 & 0.0024 & 0.0078 \\
 \bottomrule
 \end{tabular}
 \end{table}
 ```
    

 \begin{table}[H]
    \centering
    \begin{tabular}{@{}lll@{}}
    \toprule
    \textbf{Flip and Rotation} & \textbf{Blur and Sharpness} & \textbf{Brightness Change} \\ \midrule
    0.0291                     & 0.0001                     & $5.73 \times 10^{-8}$      \\ \bottomrule
    \end{tabular}
    \caption{Metrics for different augmentation types.}
    \label{tab:augmentation_metrics}
\end{table}
    



  