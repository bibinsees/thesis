\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

\textcolor{red}{change the name to evaluatoin to check whether simclr learned something? instead of intermediate evaluatoin?}  

If we do kmeans with cosine distance distance then it only compare direction of the vector thats why we need to do normal kmeans with euclidean dist to show 
the magnitude similarity

Final evaluation of the SimCLR model depends on the  Ranking task, neverthless we can use other evaluation metrics, such as downstream task 
like classification  and clusetring to check if simclr atleast learned to differentiate between untreated , single dose and exploded images.

Distance measurement approach is used to whats the performance of simclr feats derived from  5 different aug pipelines  in terms of cosine distance between 2 augs derived 
from same image.

\section{Classification using Logistic Regression on the SimCLR features}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single, linear layer that maps these representations to class predictions, where the three categories, 'untreated' and 'single dose', 'explod'  
 serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. 
 Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
 

2. Baseline comparison to original images. We classify the original images to see if simclr feature vectors are better or worse than original image to classify.

The logistic regression model is implemented as a single linear layer, where the input dimension corresponds to the feature dimension of feature spits out from simclr, and the 
output dimension corresponds to the number of classes. Specifically, the model uses a feature dimension of \(512\) if the feature is before projection head or  \(20\) if the
 feature is after projection head and outputs predictions for \(3\) classes. The mathematical representation of the model is as follows:

\[
\hat{y} = xW^T + b
\]

where:

- \(x \in \mathbb{R}^{N \times d}\) is the input feature vector, where \(d = 512\) if the feature is extracted before the projection head, or \(d = 20\) if the feature is 
extracted after the projection head.  
- \(W \in \mathbb{R}^{3 \times d}\) represents the learnable weights.  
- \(b \in \mathbb{R}^3\) is the bias term,
- \(\hat{y} \in \mathbb{R}^{N \times 3}\) are the logits representing the unnormalized class scores for \(N\) samples.

PyTorch's CrossEntropyLoss combines the softmax operation with the log loss computation for numerical stability, which is why we pass the logits (unnormalized outputs) directly
to the loss function.

This model is trained to minimize the cross-entropy loss for multi-class classification.
3 classes trained for 250 epochs. The dataset was divided into a training set (80\%) and a validation set (20\%). Batch size = 8. loss function = 
cross entropy loss.  a learning rate of \( 5 \times 10^{-4} \). 

The learning rate scheduler used is the \texttt{MultiStepLR}, which reduces the learning rate at specific milestones during training. In this case, the learning rate is 
reduced by a factor of 0.1 at the epochs corresponding to 60\% and 80\% of the total training epochs. These milestones are defined as:
\[
\text{milestones} = \left[ \text{int}(T_{\text{max}} \times 0.6), \, \text{int}(T_{\text{max}} \times 0.8) \right]
\]
where \( T_{\text{max}} \) represents the total number of training epochs. The \texttt{gamma} parameter specifies the factor by which the learning rate is multiplied at each 
milestone, which in this case is 0.1.

\begin{table}[h!]
    \centering
    \caption{Dataset Summary from Drug Screening Experiments}
    \label{tab:dataset_summary}
    \begin{tabular}{lc}
    \toprule
    \textbf{Dataset}                              & \textbf{Total Number of Images} \\ 
    \midrule
    Untreated Dataset                             & 472                            \\ 
    Single Dose Dataset                           & 103                            \\ 
    Exploded Dataset from Drug Screening Experiments & 40                             \\ 
    \bottomrule
    \end{tabular}
\end{table}

I didn't add any other comibnation of drug screening experiments since we are not sure whether they belong to which group. Some of them have medium resemblence of the
 above 3 classes but we can't be sure.


\subsection{Comparison of Classification Accuracy and Epochs for Different Data Augmentations}

As explained in Data augmentation section we use  'strong', 'sweet', 'resize', 'sweet no contrast', 'resize no contrast' simclr features to compare their performance inbetween
 and also against original images which one classify these 3 classes better.

We calculate the train accuracy and the test accuracy for each data aug pipelines as well as for original images.

Train epoch in the table: The number of epochs required to reach the best training accuracy.
Test epoch in the table: The number of epochs required to reach the best test accuracy.

\textcolor{red}{how do we turn raw images to orig images to feats}
To use original images directly first we resized to 96*96 since we did that for simclr so that it could be fair comparison. Then we normalised each by dividing it by 65535 
(16 bit). Each image flatten into vectors of ([1,96*96*3]where 96 =H=W and 3 = no of channels) suitable for logistic regression model. and we use same parameters that we used 
for simclr feats classification lke learning rate , batch size etc for fair comparison.


\textcolor{red}{Table 8.1.1 - are you saying that you classification accuracy is 100?
 That seems far too good to be true. Why do the train runs have different number of epochs? And what do you mean by "Test Epoch"?!}


 \textcolor{red}{general comment to tables with result - after giving the tables you need to write a text which helps us to interpret it. What are the most important 
 number we shall look at? What shall we take out from the table as a message. For example something like (please do not use this directly. I have no clue 
 if this is the message you want us to take away from the table) "In Table 8.1. we show that when using the features Before Projection Head all 
 augmentation methods reached 100 train and test accuracy. The Resize approach needed the fewest epochs."}
    
    \begin{table}[h!]
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{llcccccc}
        \toprule
        \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
        \multirow{4}{*}{\textbf{After Projection Head}}  
            & Train Accuracy (\%) & 66.67 & 47.76 & 89.63 & 63.41 & 54.67 \\
            & Train Epoch         & 246   & 249   & 250   & 250   & 245   \\
            & Test Accuracy (\%)  & 68.29 & 55.28 & 90.24 & 63.41 & 56.10 \\
            & Validation Epoch          & 228   & 246   & 223   & 202   & 244   \\ \midrule
        \multirow{4}{*}{\textbf{Before Projection Head}} 
            & Train Accuracy (\%) & 100 & 100 & 100 & 100 & 100 \\
            & Train Epoch         & 190    & 3     & 13     & 12     & 21     \\
            & Validation Accuracy (\%)  & 100 & 100 & 100 & 100 & 100 \\
            & Validation Epoch          & 1      & 1      & 1      & 2      & 2      \\ 
        \bottomrule
        \end{tabular}%
        }
        \caption{Performance metrics for different augmentation strategies before and after the projection head.}
        \label{tab:augmentation_metric}
        \end{table}
        

        \begin{table}[h!]
            \centering
            \caption{Original Image Results}
            \label{tab:original_image_results}
            \begin{tabular}{lcccc}
            \toprule
            \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Validation Accuracy (\%)} & \textbf{Validation Epoch} \\ \midrule
            \textbf{Original Image} & 99.39                        & 249                  & 99.19                        & 17                  \\ 
            \bottomrule
            \end{tabular}
        \end{table}
           
Train loss vs Val loss . Train accurcay  vs val accuracy 
          



          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/test_class.png} 
            \caption{before}
            \label{fig:before}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/train_class.png} 
            \caption{before}
            \label{fig:before}
          \end{figure}
\subsection{Inference}

I choose my self 22 images that have close resemblence in my eyes to single dose images from drug screen for inference. ( I repeat these are not biology expert labeled drug 
screened image. so any inference result is irrelavant in the sense to actual ground truth )

For doing inferecne I choosed before head projection features since thats the one gave 100 perentage accuracy while train and validation.


\begin{table}[H]
    \centering
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{Type} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize no Contrast} & \textbf{Sweet no Contrast} \\ \midrule
    Before Projection Head & 90.91 & 100.00 & 100.00 & 100.00 & 95.45 \\ \midrule
    Original Images & & & & 68.18 & \\ \bottomrule
    \end{tabular}
    \caption{Performance Metrics Before and After the Projection Head}
    \label{tab:performance_metrics}
\end{table}



From the above table we can see that during the inference except 'strong' and 'sweet no contrast' all other augs still able to reach 100 percentage accuracy to classify the ds 
close to sd images to sd based on visual resemblence. Using raw Original images features it got  68.18 percentage accuracy while Simclr before projectoin head could classify atleast 90 percentage.


\textcolor{red}{table 8.2 can you somehow bring this into table 8.1 e.g. as another column? Would be easier to read. The first column 
"Augmentation Type" could be made smaller if you wrap the text so that it is on multiple lines.} 


In Table 8.1. we show that when using the features Before Projection Head, all augmentation methods reached 100 percent train and test accuracy. It is clear that the features extracted 
before the projection head perform far better than those extracted after the projection head, as seen in the original similar paper. Also, the number of epochs required
to reach the best accuracy is lower for the features extracted before the projection head than those extracted after the projection head.

While original images also classify with 99 percent accuracy, they perform better than the features we use after projection. Also, their almost indistinguishable performance 
from the features extracted before the projection head leads to the conclusion that we cannot make an informed decision based on this classification task.



\section{kmeans clustering}

Idea is whether the learned representation from simlcr outperforms the original images in clustering the images (unsupervised manner) For that we use simple kmeans 
clusetring. We will cluster them based on both euclidean distance as well as cosine distance.





\subsection{Evaluation}

Full dataset (unbalanced): contains all control(untreated): 472, all single dose: 103, all exploded: 40 images, all drug screen visualy similar/closer to single dose.

40 subset (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded): contains random but make sure it have exploded like 
controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

Curated Full dataset (unbalanced): contains all control except controls have debris: 280, all single dose: 103, all exploded: 40 images, all drug screen visualy 
similar/closer to single dose: 22.

Curated 40 subset  (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded but excluded explod look alike from control):
contains random but make sure it doesn't have exploded like controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/controlex.png} 
    \caption{orig}
    \label{fig:controlex.png}
\end{figure}

\begin{table}[H]
\centering
\caption{Summary of Datasets}
\label{tab:dataset_summary}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}             & \textbf{Control (C)} & \textbf{Single Dose (SD)} & \textbf{Exploded (E)} & \textbf{SD close for inference} \\ \midrule
Full Dataset (Unbalanced)    & 472                  & 103                       & 40                    & all 22                          \\ 
40 Subset (Balanced)         & (20 with debris + 20 without debris)                   & 40                        & 40                    &    20          \\ 
Curated Full Dataset         & 280                  & 103                       & 40                    & all 22                                \\ 
Curated 40 Subset (Balanced) & 40 (all without debris)                  & 40                        & 40                    & 20           \\ 
\bottomrule
\end{tabular}%
}
\end{table}

    

run 400 times for different random initilisation. Note: Drug screen images are visually similar to single dose treatment. All balanced datasets
 are normalized to the exploded group size (n=40).
 \textcolor{red}{the curated full data set doesn't give any infromation ? if no remove the column}
 \textcolor{red}{class imbalnce problem nn orappich parayanda , say this could be  because of weight imbalance problem.}

    \begin{table}[H]
        \centering
        \caption{Evaluation Results on Different Datasets and Augmentations with cosine distance}
        \label{tab:evaluation_results_cosine_distance}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
            \multirow{5}{*}{\textbf{Before}} 
            & Strong                & \textbf{83.90} & \textbf{100} & \textbf{100} & \textbf{100} \\
            & Sweet                 & \textbf{99.18} & \textbf{100} & \textbf{100} & \textbf{100} \\
            & Resize                & 65.7 & 97.5 & 97.87 & 100 \\
            & Resize No Contrast    & 69.92 & 90 & 86.76 & 100 \\
            & Sweet No Contrast     & \textbf{98.21} & \textbf{99.17} & \textbf{99.52}  & \textbf{99.17} \\ \midrule
            \multirow{5}{*}{\textbf{After}} 
            & Strong                & 72.68 & 95.83 & 74.00 & 100 \\
            & Sweet                 & 47.32 & 74.17 & 55.08 & 78.33 \\
            & Resize                & 60.32 & 92.50 & 74.23 & 100 \\
            & Resize No Contrast    & 44.72 & 64.17 & 47.28 & 66.67 \\
            & Sweet No Contrast     & 52.20 & 86.67 & 58.16 & 89.17 \\ \bottomrule
        \end{tabular}% 
        }
    \end{table}
    

cosine distance:

The results show a clear trend of superior clustering performance before the projection head compared to after the projection head.

Before: 

data augmentatoin ‘sweet’ performs the best over other data augmentations and achieved consistant accuracy over all different data set types. 

since sweet and strong performed 100 percentage accurcay across cured and uncured balanced dataset its evidant that the problem for having less accracy is nothing
 to do with exploeded look like images in control instead its weight unbalance class problem. 

comparing 'sweet' and 'strong' they both achieved 100 percenatage accuracy ie (same performance)over all different data set types except the full dataset.
for full dataset sweet performed far better than strong , sweeet even achieved 99.18 where strong reached only 83.90.

comparing all data augmentations 'resize' and 'reesize no contrast performed the least giving the insight that cropping is essential to get better cluster performance
 using cosine distance as distance metric.

 as a breif conclusion its pretty clear that cropping 0.35 to 1 percentage of whole image is crucial especially cropping have huge effect for cluster performance using cosine distance as distacne metric.

 One thing that need to keep in mind that all above explanations or achievements by different data augs are corresponds to clustering performance doesn't mean that
  it could work for rankin strategies too.
    
  \begin{table}[H]
    \centering
    \caption{Evaluation Results on Different Datasets and Augmentations with euclidean distance}
    \label{tab:evaluation_results_euclidean}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & \textbf{88.45} & \textbf{100} & \textbf{100} & \textbf{100} \\
        & Sweet                 & \textbf{98.21} & \textbf{91.67} & \textbf{99.29} & \textbf{95.83} \\
        & Resize                & 70.40 & 78.33 & 66.66 & 88.33 \\
        & Resize No Contrast    & 68.62 & 90.00 & 86.28 & 100 \\
        & Sweet No Contrast     & \textbf{99.02} & \textbf{97.50} & \textbf{99.76} & \textbf{99.17} \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & \textbf{75.45} & \textbf{100} & \textbf{99.53} & \textbf{100} \\
        & Sweet                 & 51.54 & 74.17 & 60.52 & 79.17 \\
        & Resize                & 43.25 & 83.33 & 53.90 & 87.5 \\
        & Resize No Contrast    & 47.8 & 63.33 & 47.04 & 65.83 \\
        & Sweet No Contrast     & 51.87 & 85.0 & 57.68 & 88.33 \\ \bottomrule
    \end{tabular}% 
    }
\end{table}

    
Euclidean distance:
Just like  cosine distance, with eulcidean distacne as distacne metric also established a superior performance trend for before the projection head compared to after the
 projection head.
 Just like  cosine distance, inside the before projection head, data augs with cropping have far better performance than without cropping dataaugs such as resize and 
 resize no contrast.

 Eventhough 'strong' achieved 100 percentage accuracy across all dataset types except the full dataset unbalanced. when it comes to full dataset, sweet and sweet nocontrast pperforms better than all others.
 and among all data augs we can see sweet no contrast' maintain the cosistancey in performance across all dataset types with more than 97 percentage accuracy and also perfromed better than just 'sweet' across all dataset.
 
 interesting thing to notice here is the after projection head  'strong' it achieved higher clusterig performance 99 and above across all dataset types except 
 the full dataset. which is a good indication that we should also consider the after projection head features for ranking task.

    \begin{table}[H]
        \centering
        \caption{Evaluation Results Using Different Distance Metrics for original images}
        \label{tab:distance_metrics}
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{@{}llcccc@{}}
            \toprule
            & \textbf{Metric} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset (Unbalanced)} & \textbf{Curated 40 Subset (Balanced)} \\ \midrule
            \multirow{2}{*}{\textbf{Original Images}} 
            & Cosine Distance    & 62.76 & 72.5 & 58.15 & 77..50 \\ 
            & Euclidean Distance & 55.28 & 76.67 & 53.66 & 77.50 \\ \bottomrule
        \end{tabular}%
        }
    \end{table}
    
Original images:
comparing original images feats to simclr feats we can see that entire data augs for  before head feats outperformed entirly for both cosine distacne and euclidean distacne.
while comparing original image to after projection head the performance is comparable except for the strong data aug where after head still have better perfromance.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cluster.png} 
    \caption{orig}
    \label{fig:cluster}
\end{figure}



\textcolor{red}{Include PCA 1 here? maybe in ranking. but we can add PCA 2 fugures.}
\textcolor{red}{remove inference data from table.}

\textcolor{red}{do sd VS others all? yes depend on time}

Inference:



Since before projection head for cosine distance gives far better performance, i do  inference on them. I choose 22 images that have close resemblence in my eyes 
to single dose images from drug screen for inference. We name those images as 'sd close'( I repeat these are not biology expert labeled drug screened image. so any inference result is irrelavant
 in the sense to actual ground truth )

we used all 22 sd close images for full dataset along with the 3 classes that describe in the table. But for balanced datasets except control and exploded
 we changed the  dataset setup described in the table as follows for keeping the dataset balancing: 20 sd and 20 sd close images. so that it will stil be 40 as other classes.



 so during 3 class clustering 'sweet' (99.18 ) perfromed better than sweet no contrast (98.21) for all full dataset unbalanced. and in others sweet achieved 100 percentage accuracy
 whihc makes clear that for 3 class problem sweet have higher perfromacne comparing to other data augs. but during inference sweet accuracy reduced to 95.76  with -3.42 
 change in reduction while sweet no contrast reduced to  97.33 with -0.88 reduction change. and sweet no contrast kept the highest accuracy during inference. figure 8.8 shows that 
 with sweet no contrast data aug it classified all 103 sd images correctly and during inference it clustered all sd close image images in the cluster of sd images.
 
 

 \begin{table}[H]
    \centering
    \caption{Evaluation Results on COSINE}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 85.09 (+1.19) & 100 (0) & 98.43 (-1.57) & 100 (0) \\ 
        & Sweet              & 95.76 (-3.42) & 100 (0) & 100 (0) & 100 (0) \\ 
        & Resize             & 67.03 (+1.33) & 97.50 (0) & 98.42 (+0.55) & 100 (0) \\ 
        & Resize No Contrast & 71.89 (+1.97) & 90.83 (+0.83) & 87.42 (+0.66) & 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{97.33 (-0.88)} & \textbf{98.33 (-0.84)} & \textbf{99.55 (+0.03)} & \textbf{99.17 (0)} \\ 
        \bottomrule
    \end{tabular}% 
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cconfusion.png} 
    \caption{orig}
    \label{fig:cconfusion}
\end{figure}

\textcolor{red}{How do you know in inference dsclose was the one confused? maybe confusion matrix?  sweet no contrast before cosine}

\begin{table}[H]
    \centering
    \caption{Evaluation Results on Euclidean}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Unbalanced)} & \textbf{Uncured Balanced} & \textbf{Curated Full Dataset} & \textbf{Curated Balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 84.61 (-3.84) & 100 (0) & 99.10 (-0.9)& 100 (0) \\ 
        & Sweet              & 98.27 (+0.06) & 91.67(0) & 99.32(+0.03) & 95.83 (0) \\ 
        & Resize             & 72.06 (+1.66) & 79.17 (+0.84) & 68.54 (+1.88) & 86.67(-1.66) \\ 
        & Resize No Contrast & 71.11 (+2.49) & 100 (+10) & 87.42 (1.14)& 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{99.06 (+.04)} & \textbf{97.50 (0)} & \textbf{99.78 (+.02)} & \textbf{99.17 (0)} \\ 
        \midrule
        \textbf{After Projection Head} & Strong & 75.51 (+0.06) & 92.5 (-7.5)& 72.58 (-26.95) & 95 (-5 )\\  
        \bottomrule
    \end{tabular}% 
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/econfuse.png} 
    \caption{orig}
    \label{fig:econfuse.png}
\end{figure}


For euclidean distance, during 3 class 'sweet no contrast' have he highest performance with 99.02 accuracy without any misclassification for sd images as we seen 
in figure 8.9. and during inference it clustered all sd close image and sd images together in the cluster of sd images corrrectly as we seen in figure 8.9.


From this we can conclude that 'sweet no contrast' data aug before projection head is the best data aug for clustering task for both
 cosine distance and euclidean distance as distance metric. which basically means that sweet cropping of 0.35 to 1 (less intense cropping is essential for clustering performance.)



