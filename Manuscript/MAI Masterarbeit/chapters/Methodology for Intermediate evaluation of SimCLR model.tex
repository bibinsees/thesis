\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

\textcolor{red}{change the name to evaluatoin to check whether simclr learned something? instead of intermediate evaluatoin?}  

If we do kmeans with cosine distance distance then it only compare direction of the vector thats why we need to do normal kmeans with euclidean dist to show 
the magnitude similarity

Final evaluation of the SimCLR model depends on the  Ranking task, neverthless we can use other evaluation metrics, such as downstream task 
like classification, clusetring, distance measurement approach to check if simclr atleast learned to differentiate between untreated , single dose and exploded images.

\section{Classification using Logistic Regression on the SimCLR features}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single, linear layer that maps these representations to class predictions, where the three categories, 'untreated' and 'single dose', 'explod'  
 serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. 
 Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
 

2. Baseline comparison to original images. We classify the original images to see if simclr feature vectors are better or worse than original image to classify.


3 classes trained for 250 epochs.
1.untreated dataset: total no of images: 472
2. single dose dataset :  total no of images: 103
3. exploded dataset from drug screening experiments:  total no of images:  40

i didn't add any other comibnation of drug screening experiments since we are not sure whether they belong to which gp. ( some of them have medium resemblence of the
 above dataset images but we can't be sure.)


\subsection{Comparison of Classification Accuracy and Epochs for Different Data Augmentations}
    
    \begin{table}[h!]
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabular}{llcccccc}
        \toprule
        \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
        \multirow{4}{*}{\textbf{After Projection Head}}  
            & Train Accuracy (\%) & 88 & 59.76 & 97.15 & 58.74 & 56.30 \\
            & Train Epoch         & 249   & 248   & 236   & 239   & 249   \\
            & Test Accuracy (\%)  & 86.18 & 62.60 & 96.75 & 53.66 & 51.22 \\
            & Test Epoch          & 240   & 235   & 239   & 248   & 237   \\ \midrule
        \multirow{4}{*}{\textbf{Before Projection Head}} 
            & Train Accuracy (\%) & 100 & 100 & 100 & 100 & 100 \\
            & Train Epoch         & 148    & 18     & 62     & 14     & 44     \\
            & Test Accuracy (\%)  & 100 & 100 & 100 & 100 & 100 \\
            & Test Epoch          & 1      & 1      & 2      & 2      & 2      \\ 
        \bottomrule
        \end{tabular}%
        }
        \caption{Performance metrics for different augmentation strategies before and after the projection head.}
        \label{tab:augmentation_metrics}
        \end{table}
        

        \begin{table}[h!]
            \centering
            \caption{Original Image Results}
            \label{tab:original_image_results}
            \begin{tabular}{lcccc}
            \toprule
            \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Test Accuracy (\%)} & \textbf{Test Epoch} \\ \midrule
            \textbf{Original Image} & 95.12                        & 18                   & 94.31                        & 15                  \\ 
            \bottomrule
            \end{tabular}
        \end{table}
            




\textcolor{red}{I used some package to automate the width of the table in main.tex: change if needed} 







\section{kmeans clustering}

Idea is whether the learned representation from simlcr outperforms the original images in clustering the images (unsupervised manner) For that we use simple kmeans 
clusetring. We will cluster them based on both euclidean distance as well as cosine distance.

Derivation of K-Means Clustering using Euclidean Distance and Mean

Objective Function
The k-means algorithm aims to minimize the total squared Euclidean distance between data points and their assigned cluster centroids. The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): The \( i \)-th data point,
    \item \( \boldsymbol{\mu}_k \): The centroid of the \( k \)-th cluster,
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\subsection*{Cluster Assignment Step}
For a fixed set of centroids \( \{ \boldsymbol{\mu}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest centroid. This minimizes:

\[
r_{ik} =
\begin{cases} 
1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
0, & \text{otherwise.}
\end{cases}
\]

\subsection*{Centroid Update Step}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \boldsymbol{\mu}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
\]

Focus on a single cluster \( k \). The term involving \( \boldsymbol{\mu}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2
= \sum_{i=1}^{n} r_{ik} \left( \mathbf{x}_i^\top \mathbf{x}_i - 2 \mathbf{x}_i^\top \boldsymbol{\mu}_k + \boldsymbol{\mu}_k^\top \boldsymbol{\mu}_k \right)
\]

Take the derivative with respect to \( \boldsymbol{\mu}_k \) and set it to zero:

\[
\frac{\partial}{\partial \boldsymbol{\mu}_k} \sum_{i=1}^{n} r_{ik} \| \mathbf{x}_i - \boldsymbol{\mu}_k \|^2 =
-2 \sum_{i=1}^{n} r_{ik} \mathbf{x}_i + 2 \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k = 0
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \boldsymbol{\mu}_k
\]

Factor out \( \boldsymbol{\mu}_k \):

\[
\boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the points in cluster \( k \).

\subsection*{Algorithm Summary}
The k-means algorithm alternates between the following two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the nearest cluster:
    \[
    r_{ik} =
    \begin{cases} 
    1, & \text{if } k = \arg\min_{j} \| \mathbf{x}_i - \boldsymbol{\mu}_j \|^2, \\
    0, & \text{otherwise.}
    \end{cases}
    \]
    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the mean of its assigned points:
    \[
    \boldsymbol{\mu}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
    \]
\end{enumerate}

This iterative process continues until the assignments \( r_{ik} \) and centroids \( \boldsymbol{\mu}_k \) no longer change or the change is below a threshold.

\subsection*{Cosine distance}

\section*{Normalization}

To calculate the cosine distance, we first \textbf{normalize} all data points and centroids. Suppose the normalized data points and centroids are \( \mathbf{x}_i \) and \( \mathbf{c}_k \), respectively, then:

\[
\|\mathbf{x}_i\| = 1 \quad \text{and} \quad \|\mathbf{c}_k\| = 1.
\]

This ensures all vectors are on the unit sphere. The cosine similarity between two vectors \( \mathbf{x}_i \) and \( \mathbf{c}_k \) is defined as:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|}
\]

Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we substitute these values into the equation:

\[
\text{cosine similarity} = \frac{\mathbf{x}_i^\top \mathbf{c}_k}{1 \times 1}
\]

This simplifies to:

\[
\text{cosine similarity} = \mathbf{x}_i^\top \mathbf{c}_k.
\]

Thus, the \textbf{cosine distance} becomes:
\[
\text{cosine distance} = 1 - \mathbf{x}_i^\top \mathbf{c}_k.
\]



\section*{Relating Cosine Distance to Euclidean Distance}

For normalized vectors, we derive the relationship between \textbf{Euclidean distance} and \textbf{cosine distance}. The squared Euclidean distance between a data point \( \mathbf{x}_i \) and a centroid \( \mathbf{c}_k \) is:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \sum_{j} (x_{ij} - c_{kj})^2.
\]
Expanding this:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Since \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we get:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k.
\]
Simplify:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k).
\]
Thus, for normalized vectors, the Euclidean distance is proportional to the cosine distance:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2 \cdot \text{cosine distance}.
\]
Rearranging to express the cosine distance:
\[
\text{cosine distance} = \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]




\section*{Objective Function}

The k-means algorithm with cosine distance aims to minimize the cosine distance between data points \( \mathbf{x}_i \) and their assigned cluster centroids \( \mathbf{c}_k \). The objective function is:

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \frac{\mathbf{x}_i^\top \mathbf{c}_k}{\|\mathbf{x}_i\| \|\mathbf{c}_k\|} \right),
\]
where:
\begin{itemize}
    \item \( n \): Number of data points,
    \item \( K \): Number of clusters,
    \item \( \mathbf{x}_i \): \( i \)-th data point,
    \item \( \mathbf{c}_k \): Centroid of cluster \( k \) (normalized to unit length),
    \item \( r_{ik} \): Binary indicator; \( r_{ik} = 1 \) if \( \mathbf{x}_i \) belongs to cluster \( k \), otherwise \( r_{ik} = 0 \).
\end{itemize}

\section*{Objective Function in Terms of Euclidean Distance}

Using the above result, the k-means objective function with cosine distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]
can be rewritten in terms of Euclidean distance:
\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}.
\]
Here, the factor \( \frac{1}{2} \) accounts for the scaling difference.

\section*{Cluster Assignment Step}

For a fixed set of centroids \( \{ \mathbf{c}_k \}_{k=1}^K \), assign each data point \( \mathbf{x}_i \) to the nearest cluster based on the \textbf{cosine similarity} (or equivalently, minimize cosine distance):
\[
r_{ik} =
\begin{cases}
1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
0, & \text{otherwise.}
\end{cases}
\]
\subsection*{Centroid Update Step for Cosine Distance}
For a fixed cluster assignment \( \{ r_{ik} \} \), minimize \( J \) with respect to the centroids \( \{ \mathbf{c}_k \} \):

\[
J = \sum_{i=1}^{n} \sum_{k=1}^{K} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
\]

Focus on a single cluster \( k \). The term involving \( \mathbf{c}_k \) is:

\[
\sum_{i=1}^{n} r_{ik} \frac{\|\mathbf{x}_i - \mathbf{c}_k\|^2}{2}
= \sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( \|\mathbf{x}_i\|^2 + \|\mathbf{c}_k\|^2 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Since the vectors are normalized, \( \|\mathbf{x}_i\| = 1 \) and \( \|\mathbf{c}_k\| = 1 \), we have:

\[
\sum_{i=1}^{n} r_{ik} \frac{1}{2} \left( 1 + 1 - 2 \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
\]

Now, take the derivative of the above with respect to \( \mathbf{c}_k \) and set it to zero:

\[
\frac{\partial}{\partial \mathbf{c}_k} \sum_{i=1}^{n} r_{ik} \left( 1 - \mathbf{x}_i^\top \mathbf{c}_k \right)
= \sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Simplify:

\[
\sum_{i=1}^{n} r_{ik} \mathbf{x}_i = \sum_{i=1}^{n} r_{ik} \mathbf{c}_k
\]

Factor out \( \mathbf{c}_k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\sum_{i=1}^{n} r_{ik}}
\]

This is the mean of the normalized points in cluster \( k \).

\section*{Centroid Update Step}

For a fixed cluster assignment \( \{ r_{ik} \} \), update the centroids \( \{ \mathbf{c}_k \} \) as the \textbf{normalized mean} of all data points assigned to cluster \( k \):

\[
\mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
\]

\section*{Algorithm Summary}

The k-means algorithm with cosine distance alternates between two steps until convergence:

\begin{enumerate}
    \item \textbf{Cluster Assignment Step}: Assign each point \( \mathbf{x}_i \) to the cluster with the highest cosine similarity:
    \[
    r_{ik} =
    \begin{cases}
    1, & \text{if } k = \arg\max_{j} \mathbf{x}_i^\top \mathbf{c}_j, \\
    0, & \text{otherwise.}
    \end{cases}
    \]

    \item \textbf{Centroid Update Step}: Update the centroid of each cluster as the normalized mean of its assigned points:
    \[
    \mathbf{c}_k = \frac{\sum_{i=1}^{n} r_{ik} \mathbf{x}_i}{\left\| \sum_{i=1}^{n} r_{ik} \mathbf{x}_i \right\|}.
    \]
\end{enumerate}

\section*{Conclusion}

By normalizing the data points and centroids, the k-means clustering objective can be expressed in terms of both cosine distance and Euclidean distance. The equivalence:
\[
\|\mathbf{x}_i - \mathbf{c}_k\|^2 = 2(1 - \mathbf{x}_i^\top \mathbf{c}_k)
\]
enables seamless interpretation and implementation in the algorithm.



\subsection{Evaluation}

Full dataset (unbalanced): contains all control(untreated): 472, all single dose: 103, all exploded: 40 images, all drug screen visualy similar/closer to single dose.

40 subset (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded): contains random but make sure it have exploded like 
controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

Curated Full dataset (unbalanced): contains all control except controls have debris: 280, all single dose: 103, all exploded: 40 images, all drug screen visualy 
similar/closer to single dose: 22.

Curated 40 subset  (Balanced to the minimum nof set in the group which is exploded. ie there is only total 40 of exploded but excluded explod look alike from control):
contains random but make sure it doesn't have exploded like controls total 40, and all exploded which is basically 40, 10 ds close to sd, 30 single dose.

run 100 times for different random initilisation.

\begin{table}[htbp]
    \centering
    \caption{Dataset Composition Overview}
    \begin{tabular}{lccccl}
    \toprule
    Dataset Type & Control & Single Dose & Exploded & Drug Screen & Notes \\
    \midrule
    \multicolumn{6}{l}{\textbf{Raw Datasets}} \\
    \midrule
    Full & 472 & 103 & 40 & -- & Unbalanced \\
    Balanced-40 & 40 & 30 & 40 & 10 & Minimum set balanced \\
    \midrule
    \multicolumn{6}{l}{\textbf{Curated Datasets}} \\
    \midrule
    Full & 280 & 103 & 40 & 22 & Debris-free controls \\
    Balanced-40 & 40 & 30 & 40 & 10 & No exploded-like controls \\
    \bottomrule
    \end{tabular}
    \begin{flushleft}
    \small
    Note: Drug screen images are visually similar to single dose treatment. All balanced datasets are normalized to the exploded group size (n=40).
    \end{flushleft}
    \end{table}

\begin{table}[H]
    \centering
    \caption{Evaluation Results on Different Datasets and Augmentations with cosine distance}
    \label{tab:evaluation_results_cosine_distance}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & 93.56 & 95.83 & 98.43 & 100 \\
        & Sweet                 & 93.72 & 100 & 100 & 100 \\
        & Resize                & - & - & - & - \\
        & Resize No Contrast    & - & - & - & - \\
        & Sweet No Contrast     & - & - & - & - \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & 86.19 & 94.17 & 89.89 & 95.83 \\
        & Sweet                 & 74.10 & 72.50 & 75.96 & 69.17 \\
        & Resize                & - & - & - & - \\
        & Resize No Contrast    & - & - & - & - \\
        & Sweet No Contrast     & - & - & - & - \\ \bottomrule
    \end{tabular}%
    }
\end{table}

\begin{table}[H]
    \centering
    \caption{Evaluation Results on Different Datasets and Augmentations with euclidean distance}
    \label{tab:evaluation_results_euclidean}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & 93.72 & 99.17 & 99.1 & 100 \\
        & Sweet                 & 98.12 & 93.33 & 99.30 & 100 \\
        & Resize                & - & - & - & - \\
        & Resize No Contrast    & - & - & - & - \\
        & Sweet No Contrast     & - & - & - & - \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & 92.15 & 95 & 89.89 & 95.83 \\
        & Sweet                 & 74.10 & 71.67 & 73.71 & 68.33 \\
        & Resize                & - & - & - & - \\
        & Resize No Contrast    & - & - & - & - \\
        & Sweet No Contrast     & - & - & - & - \\ \bottomrule
    \end{tabular}%
    }
\end{table}

\textcolor{red}{Should I need to add precision recall? yes to confirm control exploded lokk like is the one who reduces the accuracy. maybe for full dataset where accuracy lw and included control looks like explod}

\begin{table}[H]
    \centering
    \caption{Original Image Results}
    \label{tab:original_image_results}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Metric}            & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Test Accuracy (\%)} & \textbf{Test Epoch} \\ \hline
    \textbf{Original Image}    & -                      & -                    & -                        & -                   \\ \hline
    \end{tabular}
\end{table}




\section{Direct day 7 to day 10 distance evaluation}
calculate distance between day 7 untreated and its corresponding day 10 treated one.
Idea is:
1. it should give same distance between day 7 and its corresponding day 10
 even its changed in position/flipped. ie checking whether simlcr learned to 
 be invariant to position error in microscope error because of manual 
 handling/transporting.
2. it sould give same distance even if its blured/sharpened
3. it should give sma edistance even if its changed in brightness
4. main test for k means centroid approach. it should give different distance to single dose and exploded.


basic evaluations for ranking:
1.
whether it learned to be invariant the position change:
do flipps and calculate the cosine distance from control to treated flipped versions. ( i don't expect it learn to the change in center of position, 
if it learns good we can say center crop have some effect maybe? but not for the edge one?)
2. 
shape invariant:control to all single dose should be almost same cosine distance.

it also applicable to time prediction and reconstruction ranking evaluation and also from kmeans centriod approach.