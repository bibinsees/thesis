\chapter{Methodology for Intermediate evaluation of SimCLR model}\label{ch:Methodology for Intermediate evaluation of SimCLR model}

The final evaluation of the SimCLR model depends on the ranking task. Nevertheless, other evaluation metrics, such as downstream tasks like classification and clustering, can be used to verify if SimCLR has effectively learned to differentiate between control, single dose, and explode images.

\section{Classification using Logistic Regression on the SimCLR features and original image vectors}
1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features.
 In other words, we use a single linear layer that maps these representations to class predictions, where the three categories, 'control' and 'single dose', 'explode' serve as our classes. The Logistic regression model can only perform well if the learned representations capture all the relevant features necessary for the task. Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data.
 

2. Baseline comparison to original images. We classify the original image vectors to see if SimCLR feature vectors are better or worse than original image to classify.

The logistic regression model is implemented as a single linear layer, where the input dimension corresponds to the feature dimension of feature spits out from SimCLR, and the 
output dimension corresponds to the number of classes. Specifically, the model uses a feature dimension of \(512\) if the feature is before projection head or  \(20\) if the
 feature is after projection head and outputs predictions for \(3\) classes. The mathematical representation of the model is as follows:

\[
\hat{y} = xW^T + b
\]

where:

- \(x \in \mathbb{R}^{N \times d}\) is the input feature vector, where \(d = 512\) if the feature is extracted before the projection head, or \(d = 20\) if the feature is 
extracted after the projection head.  
- \(W \in \mathbb{R}^{3 \times d}\) represents the learnable weights.  
- \(b \in \mathbb{R}^3\) is the bias term,
- \(\hat{y} \in \mathbb{R}^{N \times 3}\) are the logits representing the unnormalized class scores for \(N\) samples.

This model is trained to minimize the cross-entropy loss for multi-class classification.
3 classes trained for 250 epochs. The dataset was divided into a training set (80\%) and a validation set (20\%). Batch size = 8.  A learning rate of \( 5 \times 10^{-4} \). 

The learning rate scheduler used is the \texttt{MultiStepLR}, which reduces the learning rate at specific milestones during training. In this case, the learning rate is 
reduced by a factor of 0.1 at the epochs corresponding to 60\% and 80\% of the total training epochs. These milestones are defined as:
\[
\text{milestones} = \left[ \text{int}(T_{\text{max}} \times 0.6), \, \text{int}(T_{\text{max}} \times 0.8) \right]
\]
where \( T_{\text{max}} \) represents the total number of training epochs. The \texttt{gamma} parameter specifies the factor by which the learning rate is multiplied at each 
milestone, which in this case is 0.1.

\begin{table}[h!]
    \centering
    \caption{Dataset Summary from Drug Screening Experiments}
    \label{tab:dataset_summary}
    \begin{tabular}{lc}
    \toprule
    \textbf{Dataset}                              & \textbf{Total Number of Images} \\ 
    \midrule
    Control Dataset                             & 472                            \\ 
    Single Dose Dataset                           & 103                            \\ 
    Explode dataset from Drug Screening Experiments & 40                             \\ 
    \bottomrule
    \end{tabular}
\end{table}
Table \ref{tab:dataset_summary} shows the total number of images in each dataset used for classification. Additional images from drug screening experiments were not included, as their class labels are uncertain. Some images exhibit low to medium resemblance to the three defined classes, while others differ significantly, making it difficult to use them for classification task with confidence.
\subsection{Comparison of Classification Accuracy and Epochs for Different Data Augmentations}

As explained in the data augmentation section, we use 'strong,' 'sweet,' 'resize,' 'sweet no contrast,' and 'resize no contrast' SimCLR features to compare their performance against each other and the original images in classifying the three defined classes. We calculate the training accuracy and testing accuracy for each data augmentation pipeline as well as for the original image vectors. Train epoch in the table: The number of epochs required to reach the best training accuracy.
Test epoch in the table: The number of epochs required to reach the best test accuracy.
To use original images directly first we resized to 96*96 since we did that for simclr so that it could be fair comparison. Then we normalised each by dividing it by 65535 
(16 bit). Each image flatten into vectors of ([1,96*96*3]where 96 =H=W and 3 = no of channels) suitable for logistic regression model. and we use same parameters that we used 
for simclr feats classification lke learning rate , batch size etc for fair comparison.
From table \ref{tab:augmentation_metric} and table \ref{tab:original_image_results} we show that when using the features Before Projection Head all augmentation methods reached 100 train and test accuracy. And when using the features original image vectors both accuracy reached upto 99 percentage. So comparing before projection head SimCLR features to original image vectors there is only subtle difference. Their almost indistinguishable performance leads to the conclusion that it is difficult to determine whether SimCLR features have been learned efficiently from classification task. But it is clear that the features extracted before the projection head perform far better than those extracted after the projection head, as seen in the SimCLR \Cite{chen2020simple}paper. Also, the number of training and test epochs required to reach the best accuracy is lower for the features extracted before the projection head than those extracted after the projection head as we seen in figures \ref{fig:test} and \ref{fig:train}. Interestingly, original image vectors performed better than the after projection head SimCLR features.

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llcccccc}
    \toprule
    \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
    \multirow{4}{*}{\textbf{After Projection Head}}  
        & Train Accuracy (\%) & 66.67 & 47.76 & 89.63 & 63.41 & 54.67 \\
        & Train Epoch         & 246   & 249   & 250   & 250   & 245   \\
        & Test Accuracy (\%)  & 68.29 & 55.28 & 90.24 & 63.41 & 56.10 \\
        & Validation Epoch          & 228   & 246   & 223   & 202   & 244   \\ \midrule
    \multirow{4}{*}{\textbf{Before Projection Head}} 
        & Train Accuracy (\%) & 100 & 100 & 100 & 100 & 100 \\
        & Train Epoch         & 190    & 3     & 13     & 12     & 21     \\
        & Validation Accuracy (\%)  & 100 & 100 & 100 & 100 & 100 \\
        & Validation Epoch          & 1      & 1      & 1      & 2      & 2      \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for different augmentation strategies before and after the projection head using SimCLR}
    \label{tab:augmentation_metric}
\end{table}

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccc}
    \toprule
    \textbf{Metric}         & \textbf{Train Accuracy (\%)} & \textbf{Train Epoch} & \textbf{Validation Accuracy (\%)} & \textbf{Validation Epoch} \\ \midrule
    \textbf{Original Image} & 99.39                        & 249                  & 99.19                        & 17                  \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for original image vectors}
    \label{tab:original_image_results}
\end{table}

Since the train and test accuracy are both 100 percent for before projection head features, which may indicate overfitting, I performed a 10-fold cross-validation as an additional check to ensure the model's performance generalizes well. The results are shown in Tables \ref{tab:aug_metric} and \ref{tab:original_results}. The train and test accuracy for the features before the projection head drops from 100 percent to around 99 percent, and for the original image vectors, it drops from 99 percent to approximately 96 percent. This shows that the trend of the features before the projection head being slightly superior, as observed in earlier results, is also reflected here. Additionally, the after-projection head features still perform considerably worse than the before-projection head and original image vectors. Comparing this with classification using the ResNet18 architecture shows that the before-projection head SimCLR feature vectors and ResNet18 feature vectors achieve similar performance. However, between the data augmentation pipelines when using the features before the projection head, it is difficult to evaluate which one is better, as all of them show subtle differences in accuracy.
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{llcccccc}
    \toprule
    \textbf{Augmentation Type}      & \textbf{Metric} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize No Contrast} & \textbf{Sweet No Contrast} \\ \midrule
    \multirow{2}{*}{\textbf{After Projection Head}}  
        & Average train Accuracy (\%) & 35.17 & 36.60 & 49.44 & 35.28 & 34.57 \\
        & Average test Accuracy (\%)  & 33.48 & 34.62 & 54.43 & 33.77 & 34.65 \\ \midrule
    \multirow{2}{*}{\textbf{Before Projection Head}} 
        & Average train Accuracy (\%) & 99.81 & 99.90 & 99.86 & 99.90 & 99.89 \\
        & Average validation Accuracy (\%)  & 99.71 & 99.77 & 99.49 & 99.72 & 99.61 \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for different augmentation strategies before and after the projection head using cross validation with 10 k folds}
    \label{tab:aug_metric}
\end{table}

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcc}
    \toprule
    \textbf{Metric}         & \textbf{Logistic Reg Average Accuracy (\%)} & \textbf{ResNet18 Average Accuracy (\%)} \\ \midrule
    \textbf{Train}          & 96.73                                                     & 99.86                                             \\ 
    \textbf{Validation}     & 96.50                                                     & 99.93                                              \\ 
    \bottomrule
    \end{tabular}%
    }
    \caption{Classification performance metrics for original image vectors using cross-validation with 10 k folds}
    \label{tab:original_results}
\end{table}
          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/test_class.png} 
            \caption{Test accuracy vs Test epochs of original image vectors and Simclr features}
            \label{fig:test}
          \end{figure}

          \begin{figure}[H]
            \centering
            \includegraphics[scale=0.5]{figures/train_class.png} 
            \caption{Train accuracy vs Train epochs of original image vectors and Simclr features }
            \label{fig:train}
          \end{figure}
\subsection{Inference}\label{subsec:inference}

Twenty-two images that closely resemble single-dose images from drug screening were manually selected for inference and refer them as 'inference images'. These images were not labeled by biology experts, and therefore, any inference results do not correspond to the actual ground truth. For the inference process, features before the projection head were utilized, as they achieved 100 percent accuracy during training and validation.
\begin{table}[H]
    \centering
    \begin{tabular}{@{}lcccccc@{}}
    \toprule
    \textbf{Type} & \textbf{Strong} & \textbf{Sweet} & \textbf{Resize} & \textbf{Resize no Contrast} & \textbf{Sweet no Contrast} \\ \midrule
    Before Projection Head & 90.91 & 100.00 & 100.00 & 100.00 & 95.45 \\ \midrule
    Original Images & & & & 68.18 & \\ \bottomrule
    \end{tabular}
    \caption{Performance Metrics Before and After the Projection Head}
    \label{tab:performance_metrics}
\end{table}

From the inference experiment table \ref{tab:performance_metrics}, it can be seen that, except for 'strong' and 'sweet no contrast', all other augmentations were able to achieve 100 percent accuracy in classifying the inference images as belonging to the single-dose image class. Using the original image features, the accuracy was 68.18 percent, while SimCLR features before the projection head achieved at least 90 percent accuracy. Note that this inference is from the first experiment of classification where cross validation training didn't used.
\section{K-means clustering}

The idea is to determine whether the learned representation from SimCLR outperforms the original images in clustering the images in an unsupervised manner. To achieve this, we use simple K-means clustering as explained in \cite{bishop:2006:PRML}. We use both euclidean distance and cosine distance to find the nearest data point to the cluster centroid as two different approach explained in appendix ~\ref{appendix}. When performing K-means with cosine distance, it compares only the direction of the vector. Therefore, we also perform standard K-means with Euclidean distance to evaluate the magnitude similarity. 
\subsection{Evaluation}
Since there is a class imbalance in the control, single dose and explode classes. Its also useful to cluster balanced subset. Additionally, there are some images in control, where debris is formulated, it can be attributed due to the environmental factors like cell medium. Which is noticed to be significantly less than the amount of debris formed around the treated spheroids as we seen in explode images. Assuming that these images can be misclustered as exploded images or vice versa, it is beneficial to cluster a dataset containing only control images without any debris. Concluding above assumptions, for clustering, we use below dataset categories.

Imbalance full dataset: contains all control: 472, all single dose: 103, all explode: 40 images.

Balance uncured 40 subset (Balanced to the minimum number of set in the class which is explode): control contains 20 explode look alike controls + 20 normal control images without any debris, all 40 explode image and 40 single dose images.

Balance curated 40 subset  (Balanced to the minimum number of set in the class which is explode):
contains randomly selected control 40 images without and debris, all 40 explode, and 40 single dose images. These category of dataset showed in the table \ref{tab:dataset_summary}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.25]{figures/controlex.png} 
    \caption{Example of control images which contains debris which may miscluster with explode or viseversa}
    \label{fig:controlex.png}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Summary of Datasets}
    \label{tab:dataset_summary}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccc}
    \toprule
    \textbf{Dataset}             & \textbf{Control (C)} & \textbf{Single Dose (SD)} & \textbf{Exploded (E)} \\ \midrule
    (Imbalance) uncurated Full Dataset       & 472                  & 103                       & 40                    \\ 
    Uncurated 40 Subset (Balanced)         & (20 with debris + 20 without debris)                   & 40                        & 40                    \\ 
    Curated 40 Subset (Balanced) & 40 (all without debris)                  & 40                        & 40                    \\ 
    \bottomrule
    \end{tabular}%
    }
\end{table}

    
    

K-means was run 400 times with different random initializations.

\section{Evaluation of cosine distance based K-means:}
\begin{table}[H]
    \centering
    \caption{Evaluation Results on different datasets and augmentation pipeline with cosine distance}
    \label{tab:evaluation_results_cosine_distance}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & \textbf{83.90} & \textbf{100} & \textbf{100} \\
        & Sweet                 & \textbf{99.18} & \textbf{100} & \textbf{100} \\
        & Resize                & 65.7 & 97.5 & 100 \\
        & Resize No Contrast    & 69.92 & 90 & 100 \\
        & Sweet No Contrast     & \textbf{98.21} & \textbf{99.17} & \textbf{99.17} \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & 72.68 & 95.83 & 100 \\
        & Sweet                 & 47.32 & 74.17 & 78.33 \\
        & Resize                & 60.32 & 92.50 & 100 \\
        & Resize No Contrast    & 44.72 & 64.17 & 66.67 \\
        & Sweet No Contrast     & 52.20 & 86.67 & 89.17 \\ \bottomrule
    \end{tabular}% 
    }
\end{table}

The results in table \ref{tab:evaluation_results_cosine_distance} show a clear trend of superior clustering performance before the projection head compared to after the projection head.

\textbf{Among before projection head evaluation:} 

Data augmentation ‘sweet’ performs the best over other data augmentations and achieved consistent accuracy over all different data set types. Since sweet and strong performed 100 percentage accuracy across cured and uncured balanced dataset its evident that the problem for having less accuracy is nothing to do with explode look alike images (with debris)  in control instead it could be imbalance class problem. Comparing 'sweet' and 'strong' they both achieved 100 percentage accuracy ie (same performance) over all different data set types except the full dataset.
for full dataset sweet performed far better than strong , sweet even achieved 99.18 where strong reached only 83.90. Comparing all data augmentations 'resize' and 'resize no contrast performed the least giving the insight that cropping is essential to get better cluster performance using cosine distance as distance metric. As a brief conclusion its pretty clear that cropping 0.35 to 1 percentage of whole image is crucial especially cropping have huge effect for cluster performance using cosine distance as distance metric. One thing that need to keep in mind that all above explanations or achievements by different data augmentations are corresponds to clustering performance doesn't mean that it could work for ranking strategies too.



    
\section{Evaluation of euclidean distance based K-means:}
Just like  cosine distance, with euclidean distance as distance metric also established a superior performance trend for before the projection head compared to after the projection head. Just like  cosine distance, between the before projection head, data augmentation pipeline with cropping have far better performance than without cropping data augmentations such as resize and resize no contrast. Even though 'strong' achieved 100 percentage accuracy across all dataset types except the full dataset imbalanced. when it comes to full dataset, sweet and sweet no contrast performs better than all others. And among all data augmentation we can see 'sweet no contrast' maintain the constance in performance across all dataset types with more than 97 percentage accuracy and also performed better than just 'sweet' across all dataset. Interestingly after projection head  'strong' achieved higher clustering performance 99 and above across all dataset types except the full dataset. which is a good indication that we should also consider the after projection head features for ranking task.
\begin{table}[H]
    \centering
    \caption{Evaluation Results on Different Datasets and Augmentations with Euclidean Distance}
    \label{tab:evaluation_results_euclidean}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llccc@{}}
        \toprule
        \textbf{Projection Head} & \textbf{Augmentation Type} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated 40 Subset} \\ \midrule
        \multirow{5}{*}{\textbf{Before}} 
        & Strong                & \textbf{88.45} & \textbf{100} & \textbf{100} \\
        & Sweet                 & \textbf{98.21} & \textbf{91.67} & \textbf{95.83} \\
        & Resize                & 70.40 & 78.33 & 88.33 \\
        & Resize No Contrast    & 68.62 & 90.00 & 100 \\
        & Sweet No Contrast     & \textbf{99.02} & \textbf{97.50} & \textbf{99.17} \\ \midrule
        \multirow{5}{*}{\textbf{After}} 
        & Strong                & \textbf{75.45} & \textbf{100} & \textbf{100} \\
        & Sweet                 & 51.54 & 74.17 & 79.17 \\
        & Resize                & 43.25 & 83.33 & 87.5 \\
        & Resize No Contrast    & 47.8 & 63.33 & 65.83 \\
        & Sweet No Contrast     & 51.87 & 85.0 & 88.33 \\ \bottomrule
    \end{tabular}% 
    }
\end{table}

\section{Evaluation of original images:}
Comparing original images features to SimCLR features, we can see that entire data augmentation for  before head features outperformed entirely for both cosine distance and euclidean distance based K-means.
While comparing original image vectors to after projection head SimCLR features the performance is comparable except for the strong data augmentations where after head still have better performance. 
Below figure \ref{fig:cluster} demonstrates the above findins in a nutshell. From the figure its evident that Before projection head SimCLR features perform better than after projections head SimCLR features.
\begin{table}[H]
    \centering
    \caption{Evaluation Results Using Different Distance Metrics for original images}
    \label{tab:distance_metrics}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llcccc@{}}
        \toprule
        & \textbf{Metric} & \textbf{Full Dataset (Unbalanced)} & \textbf{40 Subset (Balanced)} & \textbf{Curated Full Dataset (Unbalanced)} & \textbf{Curated 40 Subset (Balanced)} \\ \midrule
        \multirow{2}{*}{\textbf{Original Images}} 
        & Cosine Distance    & 62.76 & 72.5 & 58.15 & 77..50 \\ 
        & Euclidean Distance & 55.28 & 76.67 & 53.66 & 77.50 \\ \bottomrule
    \end{tabular}%
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cluster.png} 
    \caption{Comparison of clustering performance between SimCLR features and original image features}
    \label{fig:cluster}
\end{figure}

\section{Inference for cosine distance based K-means:}
Since before projection head SimCLR features have far better performance, I do  inference on them. I choose 22 images that have close resemblance in my eyes to single dose images from drug screen for inference referred as 'inference image' just like we did in classification inference \ref{subsec:inference}. ( I repeat these are not biology expert labeled drug screen image. so any inference result is irrelevant in the sense to actual ground truth ). We used all inference images for full dataset along with the 3 classes refer as Full dataset (imbalanced). But for balanced datasets except control and explode
 we changed the  dataset setup as follows: for keeping the dataset balanced: 20 single dose and 20 inference images. so that it will still be 40 as other classes. everything else stays the same as previous experiment.

So during the 3 class ( for the uncurated imbalance full dataset set up explained in table \ref{tab:dataset_summary} ) clustering 'sweet' (99.18 percent accuracy) performed better than sweet no contrast (98.21 percent accuracy) for all full dataset imbalanced. and in other dataset types, 'sweet' achieved 100 percentage accuracy which makes clear that for 3 class problem sweet have higher performance comparing to other data augmentations. but during inference sweet accuracy reduced to 95.76  with -3.42 change in reduction while sweet no contrast reduced to  97.33 with -0.88 reduction change. and sweet no contrast kept the highest accuracy during inference. figure \ref{fig:cconfusion} shows that with 'sweet no contrast' data augmentation (in left side confusion matrix) it classified all 103 single dose images correctly and during inference figure \ref{fig:cconfusion} (right side confusion matrix) it clustered all inference images in the cluster of single dose images.

\begin{table}[H]
    \centering
    \caption{Inference evaluation results on cosine distance based K-means}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Imbalanced)} & \textbf{Uncured subset balanced} & \textbf{Curated subset balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 85.09 (+1.19) & 100 (0) & 100 (0) \\ 
        & Sweet              & 95.76 (-3.42) & 100 (0) & 100 (0) \\ 
        & Resize             & 67.03 (+1.33) & 97.50 (0) & 100 (0) \\ 
        & Resize No Contrast & 71.89 (+1.97) & 90.83 (+0.83) & 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{97.33 (-0.88)} & \textbf{98.33 (-0.84)} & \textbf{99.17 (0)} \\ 
        \bottomrule
    \end{tabular}% 
    }
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/cconfusion.png} 
    \caption{Confusion matrix for K-means clustering based on cosine distance for both 3 class clustering (left side) and inference (right side) for uncurated imbalance full data set. }
    \label{fig:cconfusion}
\end{figure}



\section{Inference for euclidean distance based K-means:}
For euclidean distance, during 3 class ( for the uncurated imbalance full dataset set up explained in table \ref{tab:dataset_summary} ) clustering 'sweet no contrast' have the highest performance with 99.02 accuracy without any misclassification for single dose images as we seen 
in figure \ref{fig:econfuse} (left side confusion matrix). and during inference it clustered all inference images and single dose images together in the cluster of single dose images correctly as we seen in figure \ref{fig:econfuse} (right side confusion matrix).


From this we can conclude that 'sweet no contrast' data augmentation before projection head is the best data augmentation for clustering task for both cosine distance and euclidean distance as distance metric. which basically means that sweet cropping of 0.35 to 1 area of original image (less intense cropping is essential for clustering performance.)

\begin{table}[H]
    \centering
    \caption{Inference evaluation results on euclidean distance based on K-means}
    \label{tab:professional_table}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llccc@{}}
        \toprule
        \textbf{Type} & \textbf{Augmentation} & \textbf{Full Dataset (Imbalanced)} & \textbf{Uncured subset balanced} & \textbf{Curated subset balanced} \\ 
        \midrule
        \multirow{5}{*}{\textbf{Before Projection Head}} 
        & Strong             & 84.61 (-3.84) & 100 (0) & 100 (0) \\ 
        & Sweet              & 98.27 (+0.06) & 91.67(0) & 95.83 (0) \\ 
        & Resize             & 72.06 (+1.66) & 79.17 (+0.84) & 86.67(-1.66) \\ 
        & Resize No Contrast & 71.11 (+2.49) & 100 (+10) & 100 (0)\\ 
        & \textbf{Sweet No Contrast}  & \textbf{99.06 (+.04)} & \textbf{97.50 (0)} & \textbf{99.17 (0)} \\ 
        \midrule
        \textbf{After Projection Head} & Strong & 75.51 (+0.06) & 92.5 (-7.5)& 95 (-5 )\\  
        \bottomrule
    \end{tabular}% 
    }
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.37]{figures/econfuse.png} 
    \caption{Confusion matrix for K-means clustering based on euclidean distance for both 3 class clustering (left side) and inference (right side)}
    \label{fig:econfuse}
\end{figure}

