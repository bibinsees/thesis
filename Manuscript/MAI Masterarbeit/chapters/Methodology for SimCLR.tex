\chapter{Methodology for SimCLR}\label{ch: Methodology for SimCLR}
\section{Data preprocessing} \label{sec:data preprocessing}
\begin{enumerate}
  \item \textbf{No of channel per image selection}:
   For 3 reasons, I decided to use 3 layers per image as 3 channels of the image for SimCLR training:
   \begin{enumerate}[itemsep=0pt]
    \item Considering each 3 layers per image may capture depth information at 3 different focal planes of a 3D tumor tissue model, providing richer information compared to only single-channel data.
    \item For ease of integration with our pretrained architecture, ResNet18, which was trained on 3-channel images.
    \item The majority of the images in the dataset have exactly 3 layers, making this a practical choice.
  \end{enumerate}
  
   Images with only 1 channel will have their single channel duplicated to form 3 identical channels. For images with 5 channels, the top 3 sharpest
   layers are filtered and selected to reduce the image to 3 layers (Process is explained below). This ensures consistency across the dataset while maintaining maximum texture and edge information. The concept of sharpness is central to selecting the most informative layers. Layers with higher sharpness are retained because they carry more texture and edge information, while layers with lower sharpness tend to have less detail and appear blurred. Sharpness is calculated intuitively as follows (Dalia Mahdy's code used for the implementation): 
  \begin{enumerate}
      \item Sharpness is related to how quickly pixel intensities change in an image. These changes are measured using gradients, which calculate how intensity changes between neighboring pixels. The gradient magnitude \[g_{\text{norm}}\] is computed as:
      \[
      g_{\text{norm}} = \sqrt{g_x^2 + g_y^2}
      \]
      where \(g_x\) and \(g_y\) are the gradients in the horizontal and vertical directions, respectively.
      \item After calculating the gradients for all pixels in a layer, their magnitudes are averaged:
      \[
      \text{Sharpness Score} = \frac{1}{N} \sum_{i=1}^{N} g_{\text{norm}, i}
      \]
      where \(N\) is the total number of pixels in the layer. A higher sharpness score corresponds to layers with more edges, transitions, and details, which are likely to be in focus.
      \item Layers are ranked based on their sharpness scores, and the top 3 layers are selected. This ensures that the dataset retains layers that are in focus and have the most texture and edge information.
  \end{enumerate}

  
  \item  \textbf{Croping the image to make whole image from rectangular to square ie Hight = Width (H=W) of the iamge}
  
    Advantage of croping to H=W are:
    \begin{enumerate}
      \item  Remove unnecessary background which contains no information
      \item  There will be no shear during resize to 96*96 augmentation like as shown in the figure \ref{fig:elong}
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: Cropped to 2054x2054 and then resized to 96x96, with no shear or elongation in one dimension. Second row: Original image (2456x2054) resized to 96x96, resulting in shear or elongation in one dimension}
        \label{fig:elong}
      \end{figure}
      
      \item Augmentations such as rotation by 90° and 270°, as well as combinations of horizontal flip and rotations by 90° or 270°, are easily applied when the image is square. If the rotation involves angles other than multiples of 90°, black borders will appear, requiring additional interpolation or other techniques. Therefore, since the image is square, rotations by multiples of 90° can be applied without any additional tasks, as explained in the figure \ref{fig:rotation}.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.3]{figures/rotation.png} 
        \caption{A: Cropped to 2054x2054 and then resized to 96x96: No black borders. B: Original image (2456x2054) resized to 96x96: Black borders appeared.}
        \label{fig:rotation}
      \end{figure}
    \end{enumerate}
  We cannot rely on a simple center crop for our images because some tissue models are not located centrally within the image; instead, they are closer to the edges. To ensure that the tissue models are completely included in the cropped region, we must identify the boundaries of the tissue models, including any debris, obtain the corresponding bounding box, and crop the image accordingly to guarantee that the cropped image contains the full tumor tissue models. The original image dimensions are 2456×2054 (Height × Width). It turns out that setting H = W = 2054 ensures that the tumor tissue model is fully contained in all images processed with computer vision libraries (to be explained soon). Any tumor tissue model that exceeds this size is either elongated in a single dimension (e.g., horizontally), indicating invalid cultivation by the robotic system, and will be discarded by Dalia's machine learning model.
For implementing this procedure, the following steps were employed using the \texttt{skimage}, \texttt{cv2}, and \texttt{numpy} libraries:
\begin{enumerate}[label=\arabic*.]
  \item The image is converted to grayscale by averaging across the three layers and then normalized to an 8-bit format using the \texttt{cv2.normalize} function.
  \item The Otsu thresholding method (\texttt{skimage.filters.threshold\_otsu}) is used to create a binary mask, followed by inversion to highlight the tumor tissue model region.
  \item Contours are extracted from the binary mask using \texttt{cv2.findContours}. The largest contour, corresponding to the tumor tissue model or debris, is identified by sorting the contours based on area.
  \item A bounding rectangle is determined around the largest contour using \texttt{cv2.boundingRect}. The rectangle is then adjusted to ensure the crop is centered around the detected region and fits within the  \(2054 \times 2054\) dimensions. The image is then cropped, ensuring that the tumor tissue model and debris are fully included in the crop. Figure \ref{fig:crop} demonstrates the whole procedure.
\end{enumerate}


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/crop.png} 
  \caption{Pipeline for careful cropping of the image.}
  \label{fig:crop}
\end{figure}

  \item  \textbf{Normalize the 16-bit image to [0, 1]}
  
  \item \textbf{Perform data augmentations} which detaily explained in the section \ref{sec:data augmentation}

  \item \textbf{Perform Z-score normalization} after data augmentation since:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}

\section{Data augmentation} \label{sec:data augmentation}
I started with best data augmentation pipeline that proposed by SimCLR \cite{chen2020simple} paper did,
because it gave best performance on natural image dataset such as Imagenet and CIFAR for downstream task like classification. 

Which follows a sequential of:

\begin{enumerate}  
  \item Randomly crop and resize to a specific smaller size. In my case, I chose to crop and resize to $96 \times 96$. I selected this small height and width for the image size to train our model because achieving good performance with these small image sizes (i.e., fewer pixel details compared to $2054 \times 2056$) may indicate that we can improve performance with larger image sizes. Additionally, it's computationally efficient. The image is randomly cropped uniformly from 0.08 to 1.0 in area of the original image and maintains a random aspect ratio (default: from 3/4 to 4/3) of the original aspect ratio. This crop is then resized to its original size.

  \item Apply a horizontal flip (the SimCLR paper only used horizontal flip because it doesn't make sense to have vertical flip/rotation in natural images). This is the default policy in SimCLR as it improves 1 percentage accuracy for downstream tasks in the SimCLR paper. We can also apply a vertical flip (as flipping the tumor tissue model vertically still results in valid samples that belong to our image distribution.), combination of horizontal and vertical flip, rotation of (90, 270), or a combination of horizontal flip + rotation (90, 270) with a probability of 50 percent. We include these additional augmentations because they still fit within our distribution samples. Other flip and rotation combinations are restricted due to their repetitive pattern, as shown in figure \ref{fig:repeat}, and the black cut problem explained in figure \ref{fig:rotation}. We set this as our default for all data augmentation.

  \item Randomly change the brightness and contrast up to lower=1-0.8, upper=1+0.8, with a probability of 80 percent. I removed saturation and hue since they do not affect grayscale images. Grayscale images are neutral, and the saturation and hue of a grayscale image are zero. Thus, when we try to adjust the values, we essentially have no color to modify as it is achromatic.
   
  \item Apply Gaussian blur and increase sharpness. Blur augmentation is also the default policy in SimCLR because it has been found to improve performance for downstream linear classification tasks by around 2 percent. Like them, we randomly sample sigma from 0.1 to 2. We also keep the kernel size to be 10 percent of the image height/width, which is 5 in our case. Some images are blurred (probably due to a microscopic error). To mimic the original version, I added an increase in sharpness, which doubles the sharpness factor. We set this as our default for all data augmentation. Figure \ref{fig:bs} shows the original and corresponding sharpened image.  
\end{enumerate}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/bs.png} 
  \caption{Applying blurriness and sharpness to the original images: The higher the sharpness, the more texture and edge details are enhanced, while blurriness results in the opposite effect}
  \label{fig:bs}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/repeat.png} 
  \caption{Orange box consist of implemented flips and rotations. Blue box consist of avoided flips and rotation combinations because of their repeated pattern to
   the orange box augmentations.}
  \label{fig:repeat}
\end{figure}

I refer to the above chain of data augmentation parameters as 'strong' data augmentation, as illustrated in the first row of Figure \ref{fig:strong_aug}.

Since there is a significant intensity change in brightness and contrast for 'strong' augmentation, I decided to analyze the effect of only the brightness and contrast changes. For 16-bit images with 3 channels, there was an increase in the number of unique pixel values—by a maximum of 258,757 percent in one of the changes in the combination of brightness and contrast. This is due to the fact that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift toward the two extremes, such as 0 or 65535, deviating significantly from the original image distribution, as shown in figure \ref{fig:16bit_three_v1}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image in one of the brightness and contrast changes according to 'strong' augmentation}
    \label{fig:16bit_three_v1}
  \end{figure}
   

As we seen in the figures  \ref{fig:16bit_three_v1}, I found that those 'strong' combination of  brightness and contrast augmentations transformed to distribution out of the original distribution. Hence its a good idea to experiment with less intensity change instead of using SimCLR best performed data augmentation parameters. Also original simclr random crop resizing to 96*96 is first done by random croping of random size (uniform from 0.08 to 1 in area). that means even really small crops without a cancer cell in it is train to have high cosine simililartiy to these same image smaller crop patch of dark cancer cell from center due to the design
 of  loss function in SimCLR. Which doesn't make sense especially when we have commom as background inner well for every of our images (or natural images atleast cars are only seen in roads, so 
 relating cars or bikes to road make sense). Hence the next data augmentation sequence designed to have random crop with scale 0.4 to 1 ( so that 
 assuming crop patches will have atleast small portion of cancer cell) and brightness and contrast variations reduced to lower=1-0.2, upper=1+0.2 and lower=1-0.35,
  upper=1+0.35 consequently as showed in figures  \ref{fig:bright} and  \ref{fig:contra}. I named this augmentation as 'sweet'. which illustrated in \ref{fig:strong_aug} second row.

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/bright.png} 
    \caption{contrast increased or decreased}
    \label{fig:bright}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/contra.png} 
    \caption{Brightness increased or decreased}
    \label{fig:contra}
  \end{figure}

 In some drug screening group of images, when we look at the day 10 image, they are exploded ( assuming due to the overdose). exploded in the sense, round 
 small cancer cell surronding lot of debris around it as we seen in figure \ref{fig:originals}. so if we apply the above random crop and resize aug: smaller crop from the round cancer cell part (without 
 debris) have high cosine similarity with some other small crop from the round cancer cell part (without debris) of the same image like in figure \ref{fig:expo}. which doesn't connect anything
  about the debris surrounded by for exploed images. In general model will also learn to map high cosine similarity between dark area to gray area which can happen
to generally all images except single dose images, meaning it doesn't learn any specific feature or certain debris patterns of explod images.
Maybe its better to not to crop and just resize to 96*96 all the time so that model learn to map high cosine similarity by seeing the whole picture (Big picture).
I understand this is not good argument because then dog and cat have same color and maybe similar shape but still learns to differentiate using strong crop in simclr.
But maybe for time predictoin ranking model it helps. because model have to predict the change from day 7 to day 10. and most of the time the size/shape changes. 
especially to the category which are exploded, they produce debris. so maybe data augmentation seeing the whole picture is good to learn the model better for time 
change.  With this assumption in mind, I decided to do data aug pipeline with just resizing.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/expo.png} 
  \caption{5 Data augmentations we explored}
  \label{fig:expo}
\end{figure}

I named this augmentation as 'Resize', which illustrated in \ref{fig:strong_aug} third row.


All of the above augmentations includes contrast change. With this augmentation it is possible that model learned to have both dark cancer cell and gray cancer
cell of same image have same feature map ( because 2 data aug from original image can be one with increased contrast which makes darker and one with less contrast 
which makes it gray as in the fugure \ref{fig:ohne} ) which is infact not good for our downstream task. because one of our goal is to differentiate/have different feature representation
for  untreated images (gray color) and single dose (darker color). 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/ohne.png} 
  \caption{contrast increased or decreased}
  \label{fig:ohne}
\end{figure}

 
Hence I removed contrast from 'Resize' and 'Sweet' and named them as 'Resize ohne contrast' and 'Sweet ohne contrast' correspondingly. 
which illustrated in \ref{fig:strong_aug} third row and fourth row correspondingly. 

Eventhough we know that these kind of strong augmentations is sensitive to our dataset, I was speculate that strong augmentation may
 have better learned latent space representations because I feel like the purpose of ths strong data augmentation explained by the authorsof simclr is not to
 include/increase the possible distribution that can happen in real life scenarios, but it is to learn the features that are invariant to these augmentations.
 \textcolor{red}{ie if contrast change is there it learns to have high cosine sim for same shape. if croping happens it learns to have high cosine sim for same pixel 
 color. if shape is different but color is same it learns to have high cosine sim. if shape is same but color is different it learns to have high cosine sim. if  color
  is different it learns to have high cosine sim for shape something like this}

Hence Its worth to experiment with this strong augmentation for downstream tasks.


5 Different data augmentation pipe lines explained above illlustrated as nutshell in \ref{fig:strong_aug}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/strong_aug.png} 
  \caption{5 Data augmentations we explored}
  \label{fig:strong_aug}
\end{figure}



\section{Train SimCLR as SSL model}

 As explained in chapter \ref{ch: Methodology for SimCLR}, SimCLR was used as the  model for self-supervised learning (SSL). 




\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter.
The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.



Intituivly given a set of \( \{ z_k \} \) with the batch size 2N, that includes a positive pair of examples \( z_i \) and \( z_j \), the loss function aims 
to identify \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \).

  
Above loss function we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]



\[
\ell_{i, j} = -\log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) + \log \left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right)
\]




2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation gives us:  
\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\label{eq:loss}
\end{equation}

Equation~\ref{eq:loss} is implemented as the loss function in our experiments.

The SimCLR framework was originally implemented in TensorFlow by the authors. In this work, I adopted the PyTorch implementation of SimCLR and utilized the parameters provided 
in
 the PyTorch SimCLR tutorial \textcolor{red}{add reference}, as they demonstrated optimal performance for the STL10 dataset. The STL10 dataset is still natural image dataset,
  with an image resolution of \( 96 \times 96 \),  matches the dimensionality of the our images to feeded into the model. 
 I used the following parameters for training from them: a learning rate of \( 5 \times 10^{-4} \), a temperature of 0.07, a weight decay of \( 1 \times 10^{-4} \), a cosine learning 
 rate schedule with a minimum learning rate of \( \frac{\text{lr}}{50} \) and \( T_{\text{max}} \) set to the maximum number of epochs where 
 \( T_{\text{max}} \) refers to the maximum number of iterations or epochs for which the cosine learning rate schedule is defined. It determines the period of the cosine function used to anneal the learning rate, the ADAM optimizer, and 4 hidden layers
with a hidden dimension of 128 in the projection head. Additionally I used max epoch as 245 and Batch size as 64. I didn't finetuned specifically for
 downstream ranking task because we don't know the ground truth label. I could have been fine tune for the intermediate evaluations, but my focus was on completing the pipeline. 

The following explains how we process a 3-channel image as if it were a standard RGB image and apply augmentations to generate two augmented versions.

\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([64, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([64, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([64, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([64, 20])  (Batch size, no of values in feature vector)  
\end{itemize}

Both output feature before and after projection head will be used for further downstream task to check the performance.
\textcolor{red}{why after not only before}
\section{Evaluation of SimCLR training} \label{sec:Evaluation result of SImclr}

\textbf{Top-1 Accuracy}:

Top-1 accuracy measures how often the loss function correctly identifies \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \). Specifically, it evaluates how often the model
 ranks \( z_j \) and \( z_i \) as having the highest cosine similarity in a given set \( \{ z_k \} \), where the batch size is \( 2N \) while the set includes a positive pair of 
 examples \( z_i \) and \( z_j \).  
If the positive pair \( z_j \) and \( z_i \) is ranked first (i.e., has the highest cosine similarity), it is counted as correct. Top-1 accuracy is calculated as the mean of these correct
 counts over all samples in the batch.  

Purpose: Indicates how often the model correctly identifies the positive example as the most similar, reflecting the model's precision at a fine-grained level.


\textbf{Top-5 Accuracy}:

Top-5 accuracy measures how often the loss function identifies \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \) within the top 5 ranked pairs. Specifically, it evaluates whether \( z_j \) is ranked among the 5 highest cosine similarity scores in \( \{ z_k \} \), where the batch size is \( 2N \), and the set includes a positive pair \( z_i \) and \( z_j \).  
If the positive pair \( z_j \) and \( z_i \) is ranked within the top 5, it is counted as correct. Top-5 accuracy is the mean of these correct counts over all samples in the batch.  

Purpose: Evaluates the model's ability to identify the true positive within a broader range of candidates, providing a softer measure of precision compared to Top-1 accuracy.

\textbf{Mean Position}: 
For each pair, the ranking is calculated based on its cosine similarity relative to all other \( z_k \) in \( \{ z_k \}_{k \neq i} \). Calculate the ranking position of the positive 
pair \( z_j \) and  \( z_i \) across all sample pairs in the batch. The mean position is the average of these ranking positions.

Purpose: Provides a quantitative metric for how far down the ranked list the  positive pairs typically appears, offering insight into the quality of the model's ranking performance.

While training  and validation images are applied by data augmentation pipelines such as 'strong' , 'sweet', 'resize', 'sweet no contrast', 'resize no contrast' inference 
images are only resized to 96*96 before the training as data augmentation. that means while calculating the ranking position or top 1 accuracy or top 5 accurcay or loss, model do inference on pairs 
of images which are always full image without any cropping.

If we compare the strong aug vs others we can see loss or top accuracies or mean position for training and validation dataset 
are better for others than strong. This is because 'strong' have to learn harder than other augmentations since 2 different 
augs of same image in strong are completely different. For example one is highly increased contrasted and other is highly 
decreased contrast. or another example: one have very small portion of background other have small portion of cancer which is
 just pure black.  which doesn't have any certain pattern or feature relation of same image. These pure small portion of black can be also be present in the batch as  from another image as aug
 so it have to learn to map high cosine sim for these two different augs. Specifically Top one accuracy struugles to get close to 100  comparing to others.

 Another interesting thing to notice is: top accuracy and mean position for inference dataset. From the beginning of the epoch inference top accuracies or mean position are the 
 close to 100 or 1 respectively. This is because since inference image augs are just resized and not applied any kind of brightness or contrast or blur/sharp or flip/rotation,
  its just original images but with reduced size 96*96. so while ranking or calculating the similarity between pairs model sees the full picture to determine the similarity. and as we are gonna seen in section \ref{sec: distance} if its full image  without any additional data augs other than reduced to size 96*96 cosine similarity between augs
   of same image is higher than any other data aug pipelines which includes crop(strong, sweet etc).  in another words for model its so easy. this leads to the question whether model learn some effective features for downstream tasks.

Thats also refelects why 'Resize' and 'resize no contrast' data aug pipelines have better performance in terms of training and val loss or top accuracies or mean position 
than data aug pipeline which includes cropping.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/strong.png} 
  \caption{strong}
  \label{fig:strong_64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/sweet_bs64.png} 
  \caption{sweet}
  \label{fig:sweet_bs64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/sweet_no_contra.png} 
  \caption{sweet no contra}
  \label{fig:sweet_no_contra}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/Resize_No_contra.png} 
  \caption{Resize no contra}
  \label{fig:Resize_No_contra_b64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/Resize.png} 
  \caption{Resize}
  \label{fig:Resize_b64}
\end{figure}


\textcolor{red}{add tsne i prefer pca plot for the cluster vs original vs befroe and after}