\chapter{Methodology for SimCLR}\label{ch: Methodology for SimCLR}

\section{Data preprocessing} \label{sec:data preprocessing}
\textcolor{red}{add croping image below?} 
\begin{enumerate}
  
  \item  We can't center crop it because for some of cancer cells are not centered in the image instead they are close to egdes. so we have to make 
  sure that when we crop it it should include the cacne cell fully. the solution is find the boundary of cancer cell and get the bounding box of cancer 
  cell then make crop to required size. original image width size is: 2456*2054 (H*W). crop the original image to have H = W. since the cancer cell debris
   spread across the width, we didn't change the width to include the debris,
   instead we reduced the hight to same size of width. hence we get H = W = 2054. if cancer cell is formed for instance, the cell it self spread acroos
    one diemsion that means its not valid cultvation by robot so we can digard it. ie maximum area of cancer cell should be included in this square 
    2054*2054 size image. advantage of croping like this are:
    \begin{enumerate}
      \item  remove unnecessary background which contains no information
      \item  there will be no shear during resize to 96*96 augmentation like in the figure.
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: croped to 2054*2054 then resized to 96*96: no shear change/elongation in one dimension happened, Second row: 
       original image 2456*2054 then resized to 96*96: shear change/elongation in one dimension happened}
        \label{fig:elong}
      \end{figure}
      
      \item rotation = 90 and 270, Horiflip+rotation 90, Horiflip+rotation270 this is only possible because of square image. if its other 
      angles except multiples of 90 then images will have black part for that we need additional careful interpolation or something like that.
       so my point is since the image is square we could take rotations of 90 mulitiples without any additional tasks. explained in \ref{fig:rotation}.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/rotation.png} 
        \caption{A: croped to 2054*2054 then resized to 96*96: No black cuts. B: 
        original image 2456*2054 then resized to 96*96: black cuts happened}
        \label{fig:rotation}
      \end{figure}
    \end{enumerate}

  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform data augmentations which detaily explained in the \ref{sec:data augmentation}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\section{Data augmentation} \label{sec:data augmentation}

I started with best data augmentation pipeline that proposed by simclr \cite{chen2020simple} paper did,
because it gave best performance on natural image dataset like Imagenet and CIFAR for downstream task like classification. 

Which follows a sequential of:

\begin{enumerate}
  \item Randomly crop  and resize to specific smaller size . In my case I choosed to crop and resize to $96 \times 96$. i choosed this small H and W as 
  image size to feed train our model because of two reasons: 1. computational fast, so that we can complete the pipeline in time. 2. if we can get good performance in ranking with this small image sizes (ie less pixel details
compared to 2054 * 2056 ) that means we can improve the performance with larger image sizes. The crop of random size (uniform from 0.08 to 1.0 in area) of the 
   original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to the original size.

  \item Apply a horizontal flip, (simclr paper only used horizontal flip because it doesn't make sense to have vertical flip/rotation in natural images) vertical 
  flip, (Horizontal+Vertical flip), Rotation = (90, 270), Horizontal flip + Rotation = (90, 270) with the probability 50 percentage. This is default policy as it
   improve 1 percentage accuracy for downstream task in simclr paper. Other flip and rotation combinations are restrained due to its repeatable pattern to the above as shown in \ref{fig:repeat} and due to the black cut problem explained in 
  \ref{fig:rotation}.

  \item Randomly change the brightness, contrast, saturation  upto lower=1-0.8, upper=1+0.8 and hue upto 0.2 times with the probability of 80 percentage.
  
  \item Gaussian blur. This augmentation is default policy in SimCLR, because they find it helpful as  it improves the performance for linear classification task
   by around 2 percentage. as they do we randomly sample sigma from 0.1 to 2 . We also kept the kernal size to be 10 percentage of image height/width, in our case 5.
\end{enumerate}


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/repeat.png} 
  \caption{Orange box consist of implemented flips and rotations. Blue box consist of avoided flips and rotation combinations because of their repeated pattern to
   the orange box augmentations.}
  \label{fig:repeat}
\end{figure}

I call this color jitterness parameters as strong data augmentation. with this strong augmentation interestingly, for 16-bit images with 3 channels, instead of a reduction, there was an 
increase in the number of unique pixel valuesâ€”by a maximum of 258,757 percentage. The issue with this increase is that after data augmentation, the new pixel
 values are not distributed similarly to the original image. Instead, they shift to
 the two extremes,such as 0 or 1 which deviate significantly from the original image distribution, as shown in 
 Figures \ref{fig:16bit_three_v1}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v1}
  \end{figure}

  Comparing to three channel, single channel after 3000 epochs even with strong augmentation only had $49\%$ maximum reduction  as observed in figure 
  \ref{fig:16bit_single_channel}. It is also not ideal, as it remove the gradual spread of darker regions and  debris around the cells, compare to three channel it
   have less effect on getting out of original distribution. thats why we need to experiment the downstream tasks with  single channel.
  
  \textbf{16-bit single-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 1058
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_onen.png} 
    \caption{16-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
    \label{fig:16bit_single_channel}
  \end{figure}

As we seen before, I found that those strong augmentations transformed to distribution out of 
the original distribution. Hence we need to experiment with less intensity change  within the color 
jitter transform instead of using SimCLR values.



then I removed contrast, saturation and hue. because if I tried to make 
those augs as invariant that means I'm making control and treated as 
similar because if I increase or decrease contrast or 
saturation or hue it increase/decrease the darkness of cancer cell to 
have similarity.
which doesn't make sense because we want exact opposite of it.

but if I use the same croping percentage as the original simclr does, 
that means I crop 0.1  to 1 percentage which doesn't make sense because that 
menas we make smilarity to small background to cancer which is unnecessary. 
so i chaneg the percentage to 0.4-1.

Questionable below one:

lasty it may be possible to learn efficient feature extraction by not 
applying cropping, ie learn by seeing full picture (big picture). 
I understand this is not good argument because then dog and cat have same 
color and maybe similar shape but still learns to differentiate using simclr.
But maybe for time predictoin ranking model it helps. because model have to 
predict the change from day 7 to day 10. and most of the ime the size/shape
 changes. especially to the category which are exploded, they produce debris. 
 so maybe data augmentation seeing the time change. 

 Eventhough we know that these kind of strong augmentations is sensitive to our dataset, I was speculate that strong augmentation may
 have better learned latent space representations because I feel like the purpose of ths strong data augmentation is not to include/increase 
 the possible distribution that can happen in real life scenarios, but it is to learn the features that are invariant to these augmentations.
Hence I decided to experiment with this strong augmentation for downstream tasks.
\section{Train SimCLR as SSL model}
For step 1, as explained in chapter \ref{ch: Methodology for SimCLR}, SimCLR was used as the first model for self-supervised learning (SSL). 
Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Explore different data augmentation combinations by researching the best augmentations for medical grayscale images, and applying intuitive approaches beyond the standard SimCLR \cite{chen2020simple} data augmentation combinations as explained in section \ref{sec:data preprocessing}.
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream task.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for the our downstream task.
  \item Supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for medical grayscale images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations). This resembles to triplet loss (not sure, need to be studied)  
\end{enumerate}
For variations 5 and 7 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}




