\chapter{Methodology for SimCLR}\label{ch: Methodology for SimCLR}

\textcolor{red}{why after not only before}
\section{Data preprocessing} \label{sec:data preprocessing}
\begin{enumerate}
  \item \textbf{No of channel per image selection}:

   For 3 reasons, I decided to use 3 layers per image as 3 channels of the image for SimCLR training:
  \begin{enumerate}
      \item Considering each 3 layers per image may  capture depth information at 3 different focal planes of a 3D tumor tissue model give more richer information
      comparing to only single channel information.
      \item For ease of integration with our pretrained architecture ResNet18, which was trained on 3-channel images.
      \item The majority of the images in the dataset have exactly 3 layers, making this a practical choice.
  \end{enumerate}
   Images with only 1 channel will have their single channel duplicated to form 3 identical channels. For images with 5 channels, the top 3 sharpest
   layers are filtered and selected to reduce the image to 3 layers. This ensures consistency across the dataset while maintaining maximum texture and edge 
   information.

   The concept of sharpness is central to selecting the most informative layers. Layers with higher sharpness are retained because they carry more 
  texture and edge information, while layers with lower sharpness tend to have less detail and appear blurred. Sharpness is calculated intuitively as follows:
  (Code is from Dalia)
  \begin{enumerate}
      \item Sharpness is related to how quickly pixel intensities change in an image. These changes are measured using gradients, which calculate how intensity changes between neighboring pixels. The gradient magnitude is computed as:
      \[
      g_{\text{norm}} = \sqrt{g_x^2 + g_y^2}
      \]
      where \(g_x\) and \(g_y\) are the gradients in the horizontal and vertical directions, respectively.
      \item After calculating the gradients for all pixels in a layer, their magnitudes are averaged:
      \[
      \text{Sharpness Score} = \frac{1}{N} \sum_{i=1}^{N} g_{\text{norm}, i}
      \]
      where \(N\) is the total number of pixels in the layer. A higher sharpness score corresponds to layers with more edges, transitions, and details, which are likely to be in focus.
      \item Layers are ranked based on their sharpness scores, and the top 3 layers are selected. This ensures that the dataset retains layers that are in focus and have the most texture and edge information.
  \end{enumerate}
  
  
  \item  \textbf{Croping the image to make whole image from rectangular to square ie H= W}
  
    Advantage of croping to H=W are:
    \begin{enumerate}
      \item  Remove unnecessary background which contains no information
      \item  There will be no shear during resize to 96*96 augmentation like as shown in the figure \ref{fig:elong}
      \item minor changes in positions of day 10 image can be reduce with this crop, but itis not a solution if the cancer cell is too edge of the image.
      \item \textcolor{red}{only state if you can show fig}
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: croped to 2054*2054 then resized to 96*96: no shear change/elongation in one dimension happened, Second row: 
       original image 2456*2054 then resized to 96*96: shear change/elongation in one dimension happened}
        \label{fig:elong}
      \end{figure}
      
      \item Augmentations such as rotation = 90 and 270, Horiflip+rotation 90, Horiflip+rotation270 this is only possible because of square image. if its other 
      angles except multiples of 90 then images will have black part for that we need additional careful interpolation or something like that.
       so my point is since the image is square we could take rotations of 90 mulitiples without any additional tasks. explained in \ref{fig:rotation}.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/rotation.png} 
        \caption{A: croped to 2054*2054 then resized to 96*96: No black cuts. B: 
        original image 2456*2054 then resized to 96*96: black cuts happened}
        \label{fig:rotation}
      \end{figure}
    \end{enumerate}

    We cannot rely on a simple center crop for our images because some cancer cells are not centrally located within the image but are instead closer to the 
edges. To ensure the cancer cells are fully included in the cropped region, the solution involves identifying the boundaries of the cancer cell including the
 debris, obtaining the corresponding bounding box, and cropping the image, wich ensures that croped image contain the full cancer cell. 
 The original image dimensions are 2456×2054 (Height × Width). It turns out to be H=W= 2054 ensures that the cancer cell is fully inside for all of the 
 image using computer vision lybraries (will explain below).
  and any cancer cell that have more than this are either elongated in single dimension (eg. Horizontal) this indicates an invalid cultivation by the robotic system,  which will be disgard in the first place through Dalias machine learning model 
  or have have very few amount of debris which I neglected since its very small.


For implementing this procedure, the following steps were employed using the \texttt{skimage}, \texttt{cv2}, and \texttt{numpy} libraries:
\begin{enumerate}
    \item The image is converted to grayscale by averaging across the three layers and then normalized to an 8-bit format using the \texttt{cv2.normalize} function.
    \item The Otsu thresholding method (\texttt{skimage.filters.threshold\_otsu}) is used to create a binary mask, followed by inversion to highlight the cancer cell region.
    \item Contours are extracted from the binary mask using \texttt{cv2.findContours}. The largest contour, corresponding to the cancer cell or debris, is identified by sorting the contours based on area.
    \item A bounding rectangle is determined around the largest contour using \texttt{cv2.boundingRect}. The rectangle is then adjusted to ensure the crop is 
    centered around the detected region and fits within the  \(2054 \times 2054\) dimensions. The image is then cropped ensuring that the cancer cell and debris 
    are fully included in the crop. Figure \ref{fig:crop} demonstrates the whole procedure.
\end{enumerate}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/crop.png} 
  \caption{crop}
  \label{fig:crop}
\end{figure}

  \item  \textbf{Normalize the 16-bit image to [0, 1]}
  
  \item \textbf{Perform data augmentations} which detaily explained in the \ref{sec:data augmentation}

  \item \textbf{Perform Z-score normalization} after data augmentation since:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}

\section{Data augmentation} \label{sec:data augmentation}

\textcolor{red}{ figure add data aug for sweet , resize, ohne figure 7.7 why is anchor the last image and not the first in the left? }

I started with best data augmentation pipeline that proposed by simclr \cite{chen2020simple} paper did,
because it gave best performance on natural image dataset such as Imagenet and CIFAR for downstream task like classification. 

Which follows a sequential of:

\begin{enumerate}
  \item Randomly crop  and resize to specific smaller size. In my case I choosed to crop and resize to $96 \times 96$. i choosed this small H and W as 
  image size to feed train our model because of two reasons: 1. computational fast, so that we can complete the pipeline in time. 2. if we can get good performance
   in ranking with this small image sizes (ie less pixel details compared to 2054 * 2056 ) that means we may can improve the performance with larger image sizes. The
   crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is 
   made. This crop is finally resized to the original size.

  \item Apply a horizontal flip, (simclr paper only used horizontal flip because it doesn't make sense to have vertical flip/rotation in natural images. This is 
  default policy in SimCLR as it improve 1 percentage accuracy for downstream task in simclr paper) vertical flip, (Horizontal+Vertical flip), Rotation = (90, 270), 
  Horizontal flip + Rotation = (90, 270) with the probability 50 percentage. We can have these aditional augs because it will  still fall into our distribution
  samples. Other flip and rotation combinations are restrained due to its repeatable pattern to the above as shown in \ref{fig:repeat} and due to the black cut 
  problem explained in \ref{fig:rotation}. We set this as our default for all data augmentation. Figure \ref{fig:bs} shows the original and corresponding blured image.

  \item Randomly change the brightness and contrast  upto lower=1-0.8, upper=1+0.8  with the probability of 80 percentage. I removed saturation and hue since it 
  doesn't have effect on gray scale images since gray scale images are  nuetral and saturation and hue of a gray scale image is zero. ie when we try
   to adjust the value, we have essentially have no color to modify as it is achromatic.
   
  
  \item Gaussian blur and increase in sharpness. Blur augmentation is also default policy in SimCLR, because they find it helpful as  it improves the performance 
  for downstream linear classification task by around 2 percentage. as they do we randomly sample sigma from 0.1 to 2 . We also kept the kernal size to be 10 percentage of 
  image height/width, in our case 5. Some images are blured ( Probabily due to the microscopic error). So inorder to mimic original version, I added sharpness 
 increase. It increaases the sharpness by a factor of 2. We set this also as our default for all data augmentation.  Figure \ref{fig:bs} shows the original
  and corresponding sharpened image
\end{enumerate}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/bs.png} 
  \caption{Blur and sharped. explain what happens when its blured or sarpened. edge details}
  \label{fig:bs}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/repeat.png} 
  \caption{Orange box consist of implemented flips and rotations. Blue box consist of avoided flips and rotation combinations because of their repeated pattern to
   the orange box augmentations.}
  \label{fig:repeat}
\end{figure}

I call the above chain of data augmentation pipeline parameters as 'strong' data augmentation which illustrated in \ref{fig:strong_aug} first row.

Since there is a strong intensity change in color jitter \textcolor{red}{ explain whats color jitter or directly use brightness contrast word} in the above data augmentation I decided to analyse the effect of only that color jitter. For 16-bit images
 with 3 channels, instead of a reduction, there was an increase in the number of unique pixel values—by a maximum of 258,757 percentage. The issue with this
  increase is that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift to
 the two extremes,such as 0 or 1 which deviate significantly from the original image distribution, as shown in Figures \ref{fig:16bit_three_v1}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v1}
  \end{figure}
   

As we seen in the figures  \ref{fig:16bit_three_v1}, I found that those strong color jitter augmentations transformed to 
distribution out of the original distribution. Hence its a good idea to experiment with less intensity change instead of using SimCLR best performed data aug parameters.

Also original simclr random crop resizing to 96*96 is first done by random croping of random size (uniform from 0.08 to 1 in area). that means even really small 
crops without a cancer cell in it is train to have high cosine simililartiy to these same image smaller crop patch of dark cancer cell from center due to the design
 of  loss function in SimCLR. Which doesn't make sense especially when we have commom as background inner well for every of our images (or natural images atleast cars are only seen in roads, so 
 relating cars or bikes to road make sense). Hence the next data augmentation sequence designed to have random crop with scale 0.4 to 1 ( so that 
 assuming crop patches will have atleast small portion of cancer cell) and brightness and contrast variations reduced to lower=1-0.2, upper=1+0.2 and lower=1-0.35,
  upper=1+0.35 consequently as showed in figures  \ref{fig:bright} and  \ref{fig:contra}. I named this augmentation as 'sweet'. which illustrated in \ref{fig:strong_aug} second row.

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{figures/bright.png} 
    \caption{contrast increased or decreased}
    \label{fig:bright}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/contra.png} 
    \caption{Brightness increased or decreased}
    \label{fig:contra}
  \end{figure}

 In some drug screening group of images, when we look at the day 10 image, they are exploded ( assuming due to the overdose). exploded in the sense, round 
 small cancer cell surronding lot of debris around it as we seen in figure \ref{fig:originals}. so if we apply the above random crop and resize aug: smaller crop from the round cancer cell part (without 
 debris) have high cosine similarity with some other small crop from the round cancer cell part (without debris) of the same image like in figure \ref{fig:expo}. which doesn't connect anything
  about the debris surrounded by for exploed images. In general model will also learn to map high cosine similarity between dark area to gray area which can happen
to generally all images except single dose images, meaning it doesn't learn any specific feature or certain debris patterns of explod images.
Maybe its better to not to crop and just resize to 96*96 all the time so that model learn to map high cosine similarity by seeing the whole picture (Big picture).
I understand this is not good argument because then dog and cat have same color and maybe similar shape but still learns to differentiate using strong crop in simclr.
But maybe for time predictoin ranking model it helps. because model have to predict the change from day 7 to day 10. and most of the time the size/shape changes. 
especially to the category which are exploded, they produce debris. so maybe data augmentation seeing the whole picture is good to learn the model better for time 
change.  With this assumption in mind, I decided to do data aug pipeline with just resizing.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/expo.png} 
  \caption{5 Data augmentations we explored}
  \label{fig:expo}
\end{figure}

I named this augmentation as 'Resize', which illustrated in \ref{fig:strong_aug} third row.


All of the above augmentations includes contrast change. With this augmentation it is possible that model learned to have both dark cancer cell and gray cancer
cell of same image have same feature map ( because 2 data aug from original image can be one with increased contrast which makes darker and one with less contrast 
which makes it gray as in the fugure \ref{fig:ohne} ) which is infact not good for our downstream task. because one of our goal is to differentiate/have different feature representation
for  untreated images (gray color) and single dose (darker color). 

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/ohne.png} 
  \caption{contrast increased or decreased}
  \label{fig:ohne}
\end{figure}

 
Hence I removed contrast from 'Resize' and 'Sweet' and named them as 'Resize ohne contrast' and 'Sweet ohne contrast' correspondingly. 
which illustrated in \ref{fig:strong_aug} third row and fourth row correspondingly. 

Eventhough we know that these kind of strong augmentations is sensitive to our dataset, I was speculate that strong augmentation may
 have better learned latent space representations because I feel like the purpose of ths strong data augmentation explained by the authorsof simclr is not to
 include/increase the possible distribution that can happen in real life scenarios, but it is to learn the features that are invariant to these augmentations.
 \textcolor{red}{ie if contrast change is there it learns to have high cosine sim for same shape. if croping happens it learns to have high cosine sim for same pixel 
 color. if shape is different but color is same it learns to have high cosine sim. if shape is same but color is different it learns to have high cosine sim. if  color
  is different it learns to have high cosine sim for shape something like this}

Hence Its worth to experiment with this strong augmentation for downstream tasks.


5 Different data augmentation pipe lines explained above illlustrated as nutshell in \ref{fig:strong_aug}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/strong_aug.png} 
  \caption{5 Data augmentations we explored}
  \label{fig:strong_aug}
\end{figure}



\section{Train SimCLR as SSL model}

 As explained in chapter \ref{ch: Methodology for SimCLR}, SimCLR was used as the  model for self-supervised learning (SSL). 




\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter.
The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.



Intituivly given a set of \( \{ z_k \} \) with the batch size 2N, that includes a positive pair of examples \( z_i \) and \( z_j \), the loss function aims 
to identify \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \).

  
Above loss function we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]



\[
\ell_{i, j} = -\log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) + \log \left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right)
\]




2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation gives us:  
\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\label{eq:loss}
\end{equation}

Equation~\ref{eq:loss} is implemented as the loss function in our experiments.

The SimCLR framework was originally implemented in TensorFlow by the authors. In this work, I adopted the PyTorch implementation of SimCLR and utilized the parameters provided 
in
 the PyTorch SimCLR tutorial \textcolor{red}{add reference}, as they demonstrated optimal performance for the STL10 dataset. The STL10 dataset is still natural image dataset,
  with an image resolution of \( 96 \times 96 \),  matches the dimensionality of the our images to feeded into the model. 
 I used the following parameters for training from them: a learning rate of \( 5 \times 10^{-4} \), a temperature of 0.07, a weight decay of \( 1 \times 10^{-4} \), a cosine learning 
 rate schedule with a minimum learning rate of \( \frac{\text{lr}}{50} \) and \( T_{\text{max}} \) set to the maximum number of epochs where 
 \( T_{\text{max}} \) refers to the maximum number of iterations or epochs for which the cosine learning rate schedule is defined. It determines the period of the cosine function used to anneal the learning rate, the ADAM optimizer, and 4 hidden layers
with a hidden dimension of 128 in the projection head. Additionally I used max epoch as 245 and Batch size as 64. I didn't finetuned specifically for
 downstream ranking task because we don't know the ground truth label. I could have been fine tune for the intermediate evaluations, but my focus was on completing the pipeline. 

The following explains how we process a 3-channel image as if it were a standard RGB image and apply augmentations to generate two augmented versions.

\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([64, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([64, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([64, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([64, 20])  (Batch size, no of values in feature vector)  
\end{itemize}

Both output feature before and after projection head will be used for further downstream task to check the performance.

\section{Evaluation of SimCLR training} \label{sec:Evaluation result of SImclr}

\textbf{Top-1 Accuracy}:

Top-1 accuracy measures how often the loss function correctly identifies \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \). Specifically, it evaluates how often the model
 ranks \( z_j \) and \( z_i \) as having the highest cosine similarity in a given set \( \{ z_k \} \), where the batch size is \( 2N \) while the set includes a positive pair of 
 examples \( z_i \) and \( z_j \).  
If the positive pair \( z_j \) and \( z_i \) is ranked first (i.e., has the highest cosine similarity), it is counted as correct. Top-1 accuracy is calculated as the mean of these correct
 counts over all samples in the batch.  

Purpose: Indicates how often the model correctly identifies the positive example as the most similar, reflecting the model's precision at a fine-grained level.


\textbf{Top-5 Accuracy}:

Top-5 accuracy measures how often the loss function identifies \( z_j \) in \( \{ z_k \}_{k \neq i} \) for a given \( z_i \) within the top 5 ranked pairs. Specifically, it evaluates whether \( z_j \) is ranked among the 5 highest cosine similarity scores in \( \{ z_k \} \), where the batch size is \( 2N \), and the set includes a positive pair \( z_i \) and \( z_j \).  
If the positive pair \( z_j \) and \( z_i \) is ranked within the top 5, it is counted as correct. Top-5 accuracy is the mean of these correct counts over all samples in the batch.  

Purpose: Evaluates the model's ability to identify the true positive within a broader range of candidates, providing a softer measure of precision compared to Top-1 accuracy.

\textbf{Mean Position}: 
For each pair, the ranking is calculated based on its cosine similarity relative to all other \( z_k \) in \( \{ z_k \}_{k \neq i} \). Calculate the ranking position of the positive 
pair \( z_j \) and  \( z_i \) across all sample pairs in the batch. The mean position is the average of these ranking positions.

Purpose: Provides a quantitative metric for how far down the ranked list the  positive pairs typically appears, offering insight into the quality of the model's ranking performance.

While training  and validation images are applied by data augmentation pipelines such as 'strong' , 'sweet', 'resize', 'sweet no contrast', 'resize no contrast' inference 
images are only resized to 96*96 before the training as data augmentation. that means while calculating the ranking position or top 1 accuracy or top 5 accurcay or loss, model do inference on pairs 
of images which are always full image without any cropping.

If we compare the strong aug vs others we can see loss or top accuracies or mean position for training and validation dataset 
are better for others than strong. This is because 'strong' have to learn harder than other augmentations since 2 different 
augs of same image in strong are completely different. For example one is highly increased contrasted and other is highly 
decreased contrast. or another example: one have very small portion of background other have small portion of cancer which is
 just pure black.  which doesn't have any certain pattern or feature relation of same image. These pure small portion of black can be also be present in the batch as  from another image as aug
 so it have to learn to map high cosine sim for these two different augs. Specifically Top one accuracy struugles to get close to 100  comparing to others.

 Another interesting thing to notice is: top accuracy and mean position for inference dataset. From the beginning of the epoch inference top accuracies or mean position are the 
 close to 100 or 1 respectively. This is because since inference image augs are just resized and not applied any kind of brightness or contrast or blur/sharp or flip/rotation,
  its just original images but with reduced size 96*96. so while ranking or calculating the similarity between pairs model sees the full picture to determine the similarity. and as we are gonna seen in section \ref{sec: distance} if its full image  without any additional data augs other than reduced to size 96*96 cosine similarity between augs
   of same image is higher than any other data aug pipelines which includes crop(strong, sweet etc).  in another words for model its so easy. this leads to the question whether model learn some effective features for downstream tasks.

Thats also refelects why 'Resize' and 'resize no contrast' data aug pipelines have better performance in terms of training and val loss or top accuracies or mean position 
than data aug pipeline which includes cropping.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/strong.png} 
  \caption{strong}
  \label{fig:strong_64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/sweet_bs64.png} 
  \caption{sweet}
  \label{fig:sweet_bs64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/sweet_no_contra.png} 
  \caption{sweet no contra}
  \label{fig:sweet_no_contra}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/Resize_No_contra.png} 
  \caption{Resize no contra}
  \label{fig:Resize_No_contra_b64}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.3]{figures/Resize.png} 
  \caption{Resize}
  \label{fig:Resize_b64}
\end{figure}


\textcolor{red}{add tsne i prefer pca plot for the cluster vs original vs befroe and after}