
 
 
 
 
 
 
 since we don't have ground truth labels to rank the images except control images.



My strategy was to simplify the current ranking problem to
 only scale/order/rank images 
using only control, single dose and exploded images.
The reason to pick these groups is that controls we know that there is no drug applied which means that there is no effect of drug at all. 
single dose images are the category which is clinically recommended at the moment (eventhough we don't know how much drug effected or how 
much it killed the cancer) 
and exploded are visually exploded from the original cancer cell meansing we can visually see debris around the cancer cell potentially which
 may potentially harm the surrounding goof cells.
once if we can order those small subset of entire images, we can add the other image as inference to see where they plotted relative to 
control 
or single dose or exloded in this scale.


\chapter{Methodology for SimCLR}\label{ch: Methodology for SimCLR}

\section{Data preprocessing} \label{sec:data preprocessing}

Detailed study/research/experiments on data augmentation and image preprocessing techniques sepcifically for our 16 bit gray scale image are still need to be done.
Currently, as the focus is on creating the complete pipeline, the standard data augmentation combination (which showed high performance for SimCLR downstream tasks) from the SimCLR \cite{chen2020simple} paper is being used, as shown below.

\begin{enumerate}
  \item  We can't center crop it because for some of cancer cells are not centered in the image instead they are close to egdes. so we have to make 
  sure that when we crop it it should include the cacne cell fully. the solution is find the boundary of cancer cell and get the bounding box of cancer 
  cell then make crop to required size. original image width size is: 2456*2054 (H*W). crop the original image to have H = W. since the cancer cell debris
   spread across the width, we didn't change the width to include the debris,
   instead we reduced the hight to same size of width. hence we get H = W = 2054. if cancer cell is formed for instance, the cell it self spread acroos
    one diemsion that means its not valid cultvation by robot so we can digard it. ie maximum area of cancer cell should be included in this square 
    2054*2054 size image. advantage of croping like this are:
    \begin{enumerate}
      \item  remove unnecessary background which contains no information
      \item  there will be no shear during resize to 96*96 augmentation like in the figure.
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: croped to 2054*2054 then resized to 96*96: no shear change/elongation in one dimension happened, Second row: 
       original image 2456*2054 then resized to 96*96: shear change/elongation in one dimension happened}
        \label{fig:elong}
      \end{figure}
      
      \item rotation = 90 and 270, Horiflip+rotation 90, Horiflip+rotation270 this is only possible because of square image. if its other 
      angles except multiples of 90 then images will have black part for that we need additional careful interpolation or something like that.
       so my point is since the image is square we could take rotations of 90 mulitiples without any additional tasks.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/rotation.png} 
        \caption{A: croped to 2054*2054 then resized to 96*96: No black cuts. B: 
        original image 2456*2054 then resized to 96*96: black cuts happened}
        \label{fig:rotation}
      \end{figure}
    \end{enumerate}

  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform the following augmentations:
  \begin{enumerate}
      \item Apply a horizontal flip.
      \item Randomly crop the image and resize it to $96 \times 96$. i choosed this small H and W as image size to feed train our model because of two reasons: 1. 
      computational fast, so that we can complete the pipeline in time. 2. if we can get good performance in ranking with this small image sizes (ie less pixel details
       compared to 2054 * 2056 ) that means we can improve the performance with larger image sizes.
      \item Randomly change the brightness, contrast, saturation, and hue of the cropped patch.
  \end{enumerate}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\section{Data augmentation} \label{sec:data augmentation}
\textcolor{red}{add what kind of rotations and flips are possible without repetion of image} 
I started with data augmentation just like simclr paper did that is strong data augmentation, just to get 
to play around , beacuse it was easy to do , since I can directly adapt code 
from tutorial just to have complete pipeline.

I found that those strong augmentations transformed to distribution out of 
the original distribution like we see in the figures below. specifically color jitterness including brightness, 
contrast, hue, saturation.

Interestingly, for 16-bit images with 3 channels, instead of a reduction, there was an increase in the number of unique pixel valuesâ€”by a maximum of 258,757 percentage. 
The issue with this increase is that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift to
 the two extremes,
 such as 0 or 1, or sometimes pushing values to both 0 and 1, which deviate significantly from the original image distribution, as shown in Figures
  \ref{fig:16bit_three_v1} and \ref{fig:16bit_three_v2}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
    \item Original Image - Minimum pixel value: 0.13064774870872498, Maximum pixel value: 0.6874189376831055
    \item Augmented Image - Minimum pixel value: 0.022128667682409286, Maximum pixel value: 0.11041323840618134
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v1}
  \end{figure}
  
  \textbf{Another example of a 16-bit three-channel image before and after data augmentation:}
  
  \begin{itemize}
    \item Original Image - Unique pixel counts per channel: 2137
    \item Augmented Image - Unique pixel counts per channel: 1686717
    \item Original Image - Minimum pixel value: 0.1306, Maximum pixel value: 0.6874
    \item Augmented Image -  Minimum pixel value: 0.1970, Maximum pixel value: 0.3748
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bithree2.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v2}
  \end{figure}
  
  \textbf{16-bit single-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 1058
    \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
    \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_onen.png} 
    \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
    \label{fig:16bit_single_channel}
  \end{figure}

  $49\%$ maximum reduction for 16-bit single-channel data augmentation with color jitter is also not ideal, as it diminishes the gradual spread of 
  darker regions 
as happened in original image, as  as observed in figure \ref{fig:16bit_single_channel}. One potential solution is to experiment with specific parameters
 within the color 
jitter transform instead of using random values, ensuring that the reduction in the number of unique pixel values does not exceed, for example, $30\%$. 
Another 
option would be to write a custom Python function, depending on the available time. Other augmentations from PyTorch work fine in this experiment.

then I removed contrast, saturation and hue. because if I tried to make 
those augs as invariant that means I'm making control and treated as 
similar because if I increase or decrease contrast or 
saturation or hue it increase/decrease the darkness of cancer cell to 
have similarity.
which doesn't make sense because we want exact opposite of it.

but if I use the same croping percentage as the original simclr does, 
that means I crop 0.1  to 1 percentage which doesn't make sense because that 
menas we make smilarity to small background to cancer which is unnecessary. 
so i chaneg the percentage to 0.4-1.

Questionable below one:

lasty it may be possible to learn efficient feature extraction by not 
applying cropping, ie learn by seeing full picture (big picture). 
I understand this is not good argument because then dog and cat have same 
color and maybe similar shape but still learns to differentiate using simclr.
But maybe for time predictoin ranking model it helps. because model have to 
predict the change from day 7 to day 10. and most of the ime the size/shape
 changes. especially to the category which are exploded, they produce debris. 
 so maybe data augmentation seeing the time change. 





\section{Train SimCLR as SSL model}
For step 1, as explained in chapter \ref{ch: Methodology}, SimCLR was used as the first model for self-supervised learning (SSL). 
Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Explore different data augmentation combinations by researching the best augmentations for medical grayscale images, and applying intuitive approaches beyond the standard SimCLR \cite{chen2020simple} data augmentation combinations as explained in section \ref{sec:data preprocessing}.
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream task.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for the our downstream task.
  \item Supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for medical grayscale images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations). This resembles to triplet loss (not sure, need to be studied)  
\end{enumerate}
For variations 5 and 7 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}




