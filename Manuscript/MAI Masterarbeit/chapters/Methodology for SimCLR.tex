\chapter{Methodology for SimCLR}\label{ch: Methodology for SimCLR}

\section{Data preprocessing} \label{sec:data preprocessing}

However, for the ease of use to integrate with our pretarined architecture resnet18 as it trained on 3 channels,
I determined to start with 3 layers per image. Hence from now on each layers of the tiff image are refered as channels.
what matters is  most sharped because 
most sharpest channels have more texture/edge information information than less sharpness 
less information (less texture/edge information) 
\textcolor{red}{add the details of no of channels/image and how does it calcualtes the sharpness intuiton behind
it: https://chatgpt.com/share/675f440d-3868-8010-ae82-ad6cdca13c5d last explanation}

the images with one channel will duplicate the channels to make upto 3.

\textcolor{red}{add croping image below?} 
\begin{enumerate}
  
  \item  We can't center crop it because for some of cancer cells are not centered in the image instead they are close to egdes. so we have to make 
  sure that when we crop it it should include the cacne cell fully. the solution is find the boundary of cancer cell and get the bounding box of cancer 
  cell then make crop to required size. original image width size is: 2456*2054 (H*W). crop the original image to have H = W. since the cancer cell debris
   spread across the width, we didn't change the width to include the debris,
   instead we reduced the hight to same size of width. hence we get H = W = 2054. if cancer cell is formed for instance, the cell it self spread acroos
    one diemsion that means its not valid cultvation by robot so we can digard it. ie maximum area of cancer cell should be included in this square 
    2054*2054 size image. advantage of croping like this are:
    \begin{enumerate}
      \item  remove unnecessary background which contains no information
      \item  there will be no shear during resize to 96*96 augmentation like in the figure.
      \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/long.png} 
        \caption{First row: croped to 2054*2054 then resized to 96*96: no shear change/elongation in one dimension happened, Second row: 
       original image 2456*2054 then resized to 96*96: shear change/elongation in one dimension happened}
        \label{fig:elong}
      \end{figure}
      
      \item rotation = 90 and 270, Horiflip+rotation 90, Horiflip+rotation270 this is only possible because of square image. if its other 
      angles except multiples of 90 then images will have black part for that we need additional careful interpolation or something like that.
       so my point is since the image is square we could take rotations of 90 mulitiples without any additional tasks. explained in \ref{fig:rotation}.
       \begin{figure}[H]
        \centering
        \includegraphics[scale=0.46]{figures/rotation.png} 
        \caption{A: croped to 2054*2054 then resized to 96*96: No black cuts. B: 
        original image 2456*2054 then resized to 96*96: black cuts happened}
        \label{fig:rotation}
      \end{figure}
    \end{enumerate}

  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform data augmentations which detaily explained in the \ref{sec:data augmentation}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\section{Data augmentation} \label{sec:data augmentation}

I started with best data augmentation pipeline that proposed by simclr \cite{chen2020simple} paper did,
because it gave best performance on natural image dataset like Imagenet and CIFAR for downstream task like classification. 

Which follows a sequential of:

\begin{enumerate}
  \item Randomly crop  and resize to specific smaller size . In my case I choosed to crop and resize to $96 \times 96$. i choosed this small H and W as 
  image size to feed train our model because of two reasons: 1. computational fast, so that we can complete the pipeline in time. 2. if we can get good performance
   in ranking with this small image sizes (ie less pixel details compared to 2054 * 2056 ) that means we can improve the performance with larger image sizes. The
   crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is 
   made. This crop is finally resized to the original size.

  \item Apply a horizontal flip, (simclr paper only used horizontal flip because it doesn't make sense to have vertical flip/rotation in natural images. This is 
  default policy in SimCLR as it improve 1 percentage accuracy for downstream task in simclr paper) vertical flip, (Horizontal+Vertical flip), Rotation = (90, 270), 
  Horizontal flip + Rotation = (90, 270) with the probability 50 percentage. We can have these aditional augs because it will  still fall into our distribution
  samples. Other flip and rotation combinations are restrained due to its repeatable pattern to the above as shown in \ref{fig:repeat} and due to the black cut 
  problem explained in \ref{fig:rotation}. We set this as our default for all data augmentation.

  \item Randomly change the brightness and contrast  upto lower=1-0.8, upper=1+0.8  with the probability of 80 percentage. I removed saturation and hue since it 
  doesn't have effect on gray scale images since gray scale images are  nuetral and saturation and hue of a gray scale image is zero. ie when we try
   to adjust the value, we have essentially have no color to modify as it is achromatic and  grayscale images contain only intensity information.
   
  
  \item Gaussian blur and increase in sharpness. Blur augmentation is also default policy in SimCLR, because they find it helpful as  it improves the performance 
  for linear classification task by around 2 percentage. as they do we randomly sample sigma from 0.1 to 2 . We also kept the kernal size to be 10 percentage of 
  image height/width, in our case 5. Some images are blured ( Probabily due to the microscopic error). So inorder to mimic original version, I added sharpness 
 increase. It increaases the sharpness by a factor of 2. We set this also as our default for all data augmentation. \textcolor{red}{sharpeness increase what happens
  to the image? figure explaination? edge details increase?} 
\end{enumerate}


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{figures/repeat.png} 
  \caption{Orange box consist of implemented flips and rotations. Blue box consist of avoided flips and rotation combinations because of their repeated pattern to
   the orange box augmentations.}
  \label{fig:repeat}
\end{figure}

I call the above chain of data augmentation pipeline parameters as 'strong' data augmentation which illustrated in \ref{fig:strong_aug} first row.

Since there is a strong intensity change in color jitter in the above data augmentation I decided to analyse the effect of only that color jitter. For 16-bit images
 with 3 channels, instead of a reduction, there was an increase in the number of unique pixel valuesâ€”by a maximum of 258,757 percentage. The issue with this
  increase is that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift to
 the two extremes,such as 0 or 1 which deviate significantly from the original image distribution, as shown in Figures \ref{fig:16bit_three_v1}.

  \textbf{16-bit three-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 5044624
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
    \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch.}
    \label{fig:16bit_three_v1}
  \end{figure}

  Comparing to three channel, single channel after 3000 epochs even with 'strong' augmentation only had $49\%$ maximum reduction  as observed in figure 
  \ref{fig:16bit_single_channel}. It is also not ideal, as it remove the gradual spread of darker regions and  debris around the cells. But compare to three channel
   it have less effect on getting out of original distribution. thats why we need to experiment the downstream tasks with  single channel.

  \textcolor{red}{Should we add the 3 layer position change issue? remove both if we are not showing comparison of sinlge and three }
  
  \textbf{16-bit single-channel image before and after data augmentation:}
  \begin{itemize}
    \item Number of unique pixel values in the original image: 2111
    \item Number of unique pixel values in the augmented image: 1058
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{figures/16bit_onen.png} 
    \caption{16-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
    \label{fig:16bit_single_channel}
  \end{figure}

As we seen in the figures  \ref{fig:16bit_three_v1} and \ref{fig:16bit_single_channel}, I found that those strong color jitter augmentations transformed to 
distribution out of the original distribution. Hence we need to experiment with less intensity change instead of using SimCLR values.

Also original simclr random crop resizing to 96*96 is first done by random croping of random size (uniform from 0.08 to 1 in area). that means even really small 
crops without a cancer cell in it is train to have high cosine simililartiy to these same image smaller crop patch of dark cancer cell from center due to the design
 of  loss function in SimCLR. Which doesn't make sense. Hence the next data augmentation sequence designed to have random crop with scale 0.4 to 1 ( so that 
 assuming crop patches will have atleast small portion of cancer cell) and brightness and contrast variations changed to lower=1-0.2, upper=1+0.2 and lower=1-0.35,
  upper=1+0.35 consequently. I named this augmentation as 'sweet'. which illustrated in \ref{fig:strong_aug} second row.

 In some drug screening group of images, when we look at the day 10 image, they are exploded ( assuming due to the overdose). exploded in the sense, broke round 
 small cancer cell surronding lot of debris around it. so if we apply the above random crop and resize aug: smaller crop from the round cancer cell part (without 
 debris) have high cosine similarity with some other small crop from the round cancer cell part (without debris) of the same image. which doesn't connect anything
  about the debris surrounded by for exploed images. In general model will also learn to map high cosine similarity between dark area to gray area which can happen
to generally all images except single dose images, meaning it doesn't learn any specific feature about certain patterns.
Maybe its better to not to crop and just resize to 96*96 all the time so that model learn to map high cosine similarity by seeing the whole picture (Big picture).
I understand this is not good argument because then dog and cat have same color and maybe similar shape but still learns to differentiate using strong crop in simclr.
But maybe for time predictoin ranking model it helps. because model have to predict the change from day 7 to day 10. and most of the time the size/shape changes. 
especially to the category which are exploded, they produce debris. so maybe data augmentation seeing the whole picture is good to learn the model better for time 
change.  With this assumption in mind, I decided to do data aug pipeline with just resizing in the begning.

\textcolor{red}{Include figure to explain better}

I named this augmentation as 'Resize', which illustrated in \ref{fig:strong_aug} third row.


All of the above augmentations includes contrast change. With this augmentation it is possible that model learned to have both dark cancer cell and gray cancer
cell of same image have same feature map ( because 2 dta aug from original image can be one with increased contrast which makes darker and one with less contrast 
which makes it gray) which is infact not good for our downstream task. because one of our goal is to differentiate/have diferent feature representation
for  untreated images (gray color) and single dose (darker color). 

\textcolor{red}{Include figure to explain better }

 
Hence I removed contrast from 'Resize' and 'Sweet' and named them as 'Resize ohne contrast' and 'Sweet ohne contrast' correspondingly. 
which illustrated in \ref{fig:strong_aug} third row and fourth row correspondingly. 

Eventhough we know that these kind of strong augmentations is sensitive to our dataset, I was speculate that strong augmentation may
 have better learned latent space representations because I feel like the purpose of ths strong data augmentation explained by the authorsof simclr is not to
 include/increase the possible distribution that can happen in real life scenarios, but it is to learn the features that are invariant to these augmentations.
 \textcolor{red}{ie if contrast change is there it learns to have high cosine sim for same shape. if croping happens it learns to have high cosine sim for same pixel 
 color. if shape is different but color is same it learns to have high cosine sim. if shape is same but color is different it learns to have high cosine sim. if  color
  is different it learns to have high cosine sim for shape something like this}

as in figure when we did strong data aug background went white so it learns about only the dark matter.

Hence Its worth to experiment with this strong augmentation for downstream tasks.

\textcolor{red}{add data augs without brightness too  if we have time, and add our improvements to strong like sharpness rot flips} 


5 Different data augmentation pipe lines explained above illlustrated as nutshell in \ref{fig:strong_aug}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/strong_aug.png} 
  \caption{5 Data augmentations we explored}
  \label{fig:strong_aug}
\end{figure}



\section{Train SimCLR as SSL model}

\textcolor{red}{add explanation that tere is no pushing in the loss function} 

For step 1, as explained in chapter \ref{ch: Methodology for SimCLR}, SimCLR was used as the first model for self-supervised learning (SSL). 
Future work: Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster 
similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{sec:data preprocessing} in the batch, 
    resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss
     function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream 
  task.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for 
  the our downstream task.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for
   medical grayscale 
  images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
 
\end{enumerate}
For variations 5 and 7 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{sec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}


\section{Evaluation result of SImclr} \label{sec:Evaluation result of SImclr}


\textcolor{red}{add figures of SImclr train and val and inference loss} 