%%%%%%%%%%%%%%%%%%%
%% imports
%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,twoside,a4paper,parskip]{scrbook} % oneside for writing, twoside for submission (space for comments on the print)
\setcounter{tocdepth}{1} % depth of table of contents, for debugging purposes
\usepackage[utf8]{inputenc} % define the encoding type for LateX, may be obsolete in newer versions of LateX
\usepackage{csquotes} % helper package for quotes
\usepackage[english]{babel} % language support, multiple languages can also be listed
\usepackage{floatflt} % wrap text around tables and figures, useful for some templates
\usepackage[pdftex]{graphicx} % /includegraphics command with more options
\usepackage[hidelinks]{hyperref} % used for cross-referencing in LateX, hidelinks is an option to better format links
\usepackage{xcolor} % color management
\usepackage{amssymb} % symbol support
\usepackage{textcomp} % text symbol support
\usepackage{nicefrac} % better formatting for fractions
\usepackage{scrhack} % definition of macros 
\usepackage{pdfpages} % include pdf pages in LaTeX
\usepackage{float} % floating objects (i.e. figures, tables)
\usepackage{pdflscape} % better pdf support with more options
\usepackage[verbose]{placeins} % checks that floats are within bounds - verbose adds extra logs
\usepackage[markcase=ignoreuppercase,headsepline,plainfootsepline]{scrlayer-scrpage} % manage page styles, markcase: automatic typesetting in heads, headsepline: line underneath the headerm, plainfootsepline: rule above the footer
\usepackage[ruled,vlined]{algorithm2e} % algorithm environment, ruled: algorithm with line at the top and bottom. Caption is not centered but at the beginning of the algorithm, vlined: vertical line followed by a small horizontal line between the start and the end of each block
\usepackage{listings} % used to display nicely formatted source code 
\usepackage{caption} % adds captions to figures, tables, equations... with correct number assignments
\usepackage{subcaption} % adds subcaptions to subfigures, useful for more complex figures, has to be imported after the caption package
\usepackage{epstopdf} % needed to use .eps 
\usepackage{longtable} % enables long tables over page boundaries
\usepackage{setspace} % sets line spacing
\usepackage{booktabs} % better line management and line separation within tables
\usepackage[sortcites,style=numeric,backend=biber,doi=false,isbn=false,url=false,eprint=false]{biblatex} % good package for bibliography support in LaTeX with biber backend, sortcites: sorting bibliography (name, title, year), style=numeric: standard numeric citation style (recommended!), doi/isbn/url/eprint=false: hide links in bibliography (recommended for clean bibliography!)
\usepackage{xcolor}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\usepackage{ulem}
\usepackage{tikz}
\usepackage{amssymb}



\usepackage{makecell} % tabular column heads and multilined cells
\usepackage{amsmath} % math support 
\bibliography{bibliography} % included biblatex formatted citations from the file bibliography.bib
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{lipsum} % only used for the template generation, remove this usepackage command and all \lipsum calls
%%%%%%%%%%%%%%%%%%%
%% definitions
%%%%%%%%%%%%%%%%%%%
\def\BaAuthor{Bibin Babu}
\def\BaAuthorStudyProgram{Master of Artificial intelligence} 
\def\BaType{Master Thesis}
\def\BaTitle{Determination of  Drug Efficacy  
on Pancreatic Tumor 3D Spheroidal Tissues  }
\def\BaSupervisorOne{Prof. Dr. Magda Gregorová}
\def\BaSupervisorTwo{Prof. Dr. Jan Hansmann}
\def\BaDeadline{\SubmitDate}
\def\SubmitDate{13.05.2024}
\def\git{https://github.com/.../...}

% option to generate anonymous submission for plagiarism scan!
\ifdefined\iswithfullname
\def\ShowBaAuthor{\BaAuthor}
\else
\def\ShowBaAuthor{BIBIN BABU}
\fi

\hypersetup{
pdfauthor={\ShowBaAuthor},
pdftitle={\BaTitle},
pdfsubject={Subject},
pdfkeywords={Keywords}
}

%%%%%%%%%%%%%%%%%%%
%% configs to include
%%%%%%%%%%%%%%%%%%%
\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\definecolor{pblue}{rgb}{0.13,0.13,1}
\definecolor{pgreen}{rgb}{0,0.5,0}
\definecolor{pred}{rgb}{0.9,0,0}
\definecolor{pgrey}{rgb}{0.46,0.45,0.48}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
  linewidth=\textwidth
}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}

\lstset{language=xml,
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  backgroundcolor=\color{background},
  morekeywords={xmlns,version,type}% list your attributes here
}

\lstset{language=Java,
  showspaces=false,
  showtabs=false,
  tabsize=4,
  breaklines=true,
  keepspaces=true,
  numbers=left,
  numberstyle=\scriptsize,
  stepnumber=1,
  numbersep=8pt,
  showstringspaces=false,
  breakatwhitespace=true,
  commentstyle=\color{pgreen},
  keywordstyle=\color{pblue},
  stringstyle=\color{pred},
  basicstyle=\ttfamily,
  backgroundcolor=\color{background},
%  moredelim=[il][\textcolor{pgrey}]{$$},
%  moredelim=[is][\textcolor{pgrey}]{\%\%}{\%\%}
}
\newcommand*{\forcetwoside}[1][1]{%
 \begingroup
   \cleardoubleoddpage
   \KOMAoptions{titlepage=true}% useful e.g. for scrartcl
   \csname @twosidetrue\endcsname
   \maketitle[{#1}]
 \endgroup
}
\def\c#1{\mathcal{#1}}
\DeclareUnicodeCharacter{2212}{-}

\begin{document}

%%%%%%%%%%%%%%%%%%%
%% Title page
%%%%%%%%%%%%%%%%%%%
\ifdefined\print
\newgeometry{centering}    %%% make the page centered on paper
\begin{titlepage}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	\centering
	\begin{center}
		\vspace*{.06\textheight}	
		\HRule \\[0.4cm] % Horizontal line
		{\huge \bfseries \BaTitle \par}\vspace{0.4cm} % Thesis title
		\HRule \\[1.5cm] % Horizontal line
		\vspace{4.5cm}
		\begin{minipage}[t]{0.4\textwidth}

		\centering
		{\large \BaAuthor} 

		\end{minipage}
		\vfill
		\vspace*{.1\textheight}
		{\large \BaType}\\[0.5cm] % Thesis type
		{\large University of Applied Sciences Würzburg-Schweinfurt}\\[0.5cm]
		{\large \SubmitDate}\\[4cm] % Date
		\vfill
	\end{center}
\end{titlepage}
\restoregeometry
\fi

\frontmatter
\titlehead{%
  \centering
  Technical University of Applied Sciences Würzburg-Schweinfurt (THWS)\\
  Faculty of Computer Science and Business Information Systems\\
}

\subject{\BaType}
\title{\BaTitle\\[15mm]}
\subtitle{\normalsize{submitted to the Technical University of Applied Sciences W\"{u}rzburg-Schweinfurt in the Faculty of Computer Science and Business Information Systems to complete a course of studies in \BaAuthorStudyProgram}}
\author{\ShowBaAuthor}
\date{\normalsize{Submitted on: \BaDeadline}}
\publishers{
  \normalsize{Initial examiner: \BaSupervisorOne}\\
  \normalsize{Secondary examiner: \BaSupervisorTwo}\\
}

\forcetwoside


%%%%%%%%%%%%%%%%%%%
%% abstract
%%%%%%%%%%%%%%%%%%%

\section*{Abstract (en)}
Pancreatic tumor treatment is hindered by the intricate nature of tumors and their diverse microenvironments. This complexity necessitates an exploration into identifying optimal drug combinations and concentrations tailored to each patient's specific tumor characteristics. This thesis aims to assess drug efficacy by ranking these various drug combinations and concentrations. The ranking is based on features extracted from bright-field microscopy images of three-dimensional tumor tissue models using representation learning. The core challenge is to learn robust features that accurately characterize alterations in these tumor tissue models induced by drug application over time. This research seeks to develop a standardized and effective approach for evaluating drug efficacy, potentially improving treatment outcomes for pancreatic tumor patients.



% cSpell:disable
\section*{Abstract (de)}
Die Behandlung von Bauchspeicheldrüsentumoren wird durch die komplexe Natur der Tumore und ihre vielfältigen Mikroumgebungen behindert. Diese Komplexität erfordert eine Untersuchung zur Identifizierung optimaler Medikamentenkombinationen und -konzentrationen, die auf die spezifischen Tumoreigenschaften jedes Patienten zugeschnitten sind. Diese Masterarbeit zielt darauf ab, die Wirksamkeit von Medikamenten zu bewerten, indem sie diese verschiedenen Medikamentenkombinationen und -konzentrationen einstuft. Die Bewertung basiert auf Merkmalen, die aus Helligkeitsmikroskopiebildern dreidimensionaler Tumorgewebsmodelle mittels Repräsentationslernen extrahiert werden. Die zentrale Herausforderung besteht darin, robuste Merkmale zu erlernen, die Veränderungen in diesen Tumorgewebsmodellen genau charakterisieren, die durch die Anwendung von Medikamenten über Zeit induziert werden. Diese Forschung zielt darauf ab, einen standardisierten und effektiven Ansatz zur Bewertung der Medikamenteneffizienz zu entwickeln, der möglicherweise die Behandlungsergebnisse für Patienten mit Bauchspeicheldrüsentumoren verbessert.
%%%%%%%%%%%%%%%%%%%
%% Main part of the thesis
%%%%%%%%%%%%%%%%%%%
\mainmatter
\let\cleardoublepage\clearpage
\chapter{Introduction}\label{ch:intro}

Pancreatic tumor presents a significant challenge in terms of treatment due to its heterogeneous nature and the mutations that occur during its progression within the human body. Clinicians rely on case studies, human trials, and their own expertise gained from past patient treatments to select drugs for new patients. However, this approach is often based on trial and error, with varying outcomes. Patients may experience either successful treatment or severe side effects such as hair loss and damage to other organs. Since each patient's tumor cells exhibit unique characteristics influenced by factors such as age and genetics, treatments that have worked for one patient may not be effective for another. Consequently, clinicians may need to change the prescribed drugs or try different combinations, which can lead to delays and increased risks for the patient, including mortality.

In light of these challenges researchers at Fraunhofer Translational Center for Regenerative Therapy TLZ-RT Wuerzburg,  propose a vision for the future: cultivating multiple three-dimensional tumor tissue models for each patients in the lab using biopsy samples and studying the efficacy of drugs on these three-dimensional tumor tissue models first.\textit{(Note: In this thesis, "3D tumor tissue models or tumor tissue models" refers to physical, lab-grown tissues and not computational or AI models.)} By conducting drug development experiments and analyses on these tissue models, they aim to find the optimum or best drug combination tailored to each patient's specific tumor characteristics. This approach can not only minimize direct side effects on human patients and reduce the time needed to select the most effective personalized treatment, thereby decreasing the risk of that patient's mortality, but also significantly reduce the cost and time of preclinical testing in the drug development process. Ultimately, these information obtained from drug efficacy assessment experiments can inform clinicians' decisions, enabling them to select the most effective drug combination before administering it to the patient.

As a proof of concept, The Fraunhofer TLZ-RT Wuerzburg laboratory utilized a modular dual-arm robot-based system \cite{Dembski2023Establishing}, equipped with incubators and bioreactors (see Figure \ref{fig:platform} and Figure \ref{fig:Robot}) under physiological conditions to study drug efficacy for the long-term culture of these three-dimensional tumor tissue models. One advantage of this platform is its ability to capture bright-field microscopy images of 3D tumor tissue models using a customized microscope setup integrated into the robotic platform, offering flexibility in image acquisition according to experimental needs.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/platform setup.png} % Adjust the width as needed
  \caption{Robo platform}
  \label{fig:platform}
\end{figure}



\begin{figure}[H]
  \centering
  \includegraphics[scale=0.4]{figures/robot.png} % Adjust the scale factor as needed
  \caption{Dual-arm robot}
  \label{fig:Robot}
\end{figure}

Although the vision for the future is to simulate the identical interaction environment of drugs with tumor cells as it occurs in the human body, current technology has not yet achieved this. The current three-dimensional tumor tissue models developed in the lab do not fully resemble real pancreatic tumor cells found in the human body. These 3D tumor tissue models only contain pure tumor tissues, whereas real human pancreatic tumor cells exist within a complex microenvironment comprising tumor cells, blood vessels, other tissues, and various cell types. Despite this limitation, our work serves as a valuable starting point for studying drug efficacy in a controlled environment. Fortunately, if we are able to replicate human body tumor cells in the lab in the future, the techniques currently used to study the bright-field microscopy images will still be applicable. However, the fact that bright-field microscopy images are two-dimensional limits the ability to perform a comprehensive analysis of the drug's impact on the entire 3D structure of the cultivated tumor tissue models.

Alternatives to bright-field microscopy images include 3D fluorescence microscopy and luminescent cytotoxicity assays. However, both methods are invasive. Fluorescent molecules tend to generate reactive chemical species under illumination, enhancing phototoxic effects. This chemical reaction with the 3D tumor tissue model may alter its structure, making it not suitable to isolate the drug's effect over time. Similarly, luminescent cytotoxicity assays result in a dead culture, rendering them unsuitable for longitudinal studies. Additionally, both methods require removing the well plate from the isolated culture environment for extended periods, making the samples susceptible to external environmental factors. For instance, in fluorescence microscopy, cells are particularly vulnerable to phototoxicity from short wavelength light. In contrast, bright-field microscopy images are non-invasive, allowing continuous culture and the possibility of creating time series of images to study dynamic changes. Therefore, we rely on bright-field microscopy images to study the time-evolutionary effects of drugs.

\section{Laboratory Setup}
\label{sec:lab-setup}
3D tumor tissue models were cultured in well plates containing 96 wells, each providing a nutrient medium that allows them to maintain their tissue-specific functions in vitro. Although each plate can yield 96 pure 3D tumor tissue models, the edge effect is accounted for, where outer wells may be exposed to variable conditions such as temperature fluctuations, increased evaporation rates, and other environmental factors. Consequently, we restrict our analysis to the 60 inner wells per plate as in figure \ref{fig:Wellplate}, adhering to standard procedures to ensure consistent and reliable experimental data. 

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/WellPlate.png} % Adjust the width as needed
  \caption{A well plate containing 96 wells where rows A, H and columns 1, 2 are excluded due to edge effects.}
  \label{fig:Wellplate}
\end{figure}





Based on the drug concentration applied to 3D tumor tissue models, the bright-field microscopy images we capture can be categorized into three:

Images of

\begin{enumerate}
  \item Control (0 percentage drug applied)
        \begin{itemize}
            \item For easiness, we refer to this category as ``Untreated''
        \end{itemize}
  
  \item Single concentration (theoretically recommended single concentration of drug treatment)
        \begin{itemize}
            \item For easiness, we refer to this category as ``Single dose''
        \end{itemize}
  \item Drug screening: different drug combinations and concentrations used for experimental study of drug efficacy, which may or may not result in the killing of surrounding non-tumor cells in the human body with potential side effects.
        \begin{itemize}
            \item For easiness, we refer to this category as ``Drug screened''
        \end{itemize}
\end{enumerate}

The 60 wells are divided into sections to provide these three type of tumor tissues shown in figure \ref{fig:Single dose wellplate} and figure \ref{fig:Drug screen wellplate}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/singledose.png} % Adjust the width as needed
  \caption{Well plate setup for the single-dose experiment where the left half remains untreated and the right half is treated with a single drug concentration. This image was taken three days after drug application, i.e., on day 10.}
  \label{fig:Single dose wellplate}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1.2\linewidth]{figures/drug_screened.png} % Adjust the width as needed
  \caption{Well plate setup for the drug screening experiment where the majority of tumor tissues are treated with different combinations of drug concentrations (multi-colored wells), while some are left untreated (white wells bounded by boxes).}
  \label{fig:Drug screen wellplate}
\end{figure}

The 3D tumor tissue models develop in the wellplate progressively from day 1 to day 7, reaching their maximum cancerous state by day 7, at which point the drug is administered. By day 10, the drug's effect on the cancerous tissue is expected to peak, as nutrient availability gradually decreases and the tumor begins to diminish. To isolate the drug's effects, changes in tumor tissue deterioration are assessed on day 3 post-drug administration (day 10),  in accordance with established medical protocols and previous research findings.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.9]{figures/timed.pdf} 
  \caption{Illustrates the flow chart of time evolutoion of 3D tumor tissues.}
  \label{fig:time}
\end{figure}

We assess the efficacy of the drug by comparing the changes it induces in the untreated bright-field microscopy images over a period of time. The current methods to differentiate these changes involve studying the alterations from day 7 (after applying the drug) to Day 10. These changes are typically observed in three main parameters: 

\begin{enumerate}
  \item Size/Area
  \item Circularity/Diameter/Perimeter
  \item Pixel intensity or color change
\end{enumerate}

These parameters serve as human-interpretable metrics for assessing the efficacy of the drug. However, there may be other hidden information or patterns within these bright-field microscopy images that are not human-interpretable. This potential can be explored using representation learning techniques. Additionally, this method can provide more standardization compared to manual assessment.
\let\cleardoublepage\clearpage
\chapter{Objective}\label{ch:Objective}
This thesis aims to assess drug efficacy by ranking different drug combinations and concentrations. The ranking is based on features extracted from bright-field microscopy images of  three-dimensional tumor tissue models using representation learning. The primary challenge lies in learning the efficient features of alterations induced in these tumor tissue models by the impact of drug application over a period of time.
\let\cleardoublepage\clearpage
\chapter{Research questions}\label{ch:Research questions}

1. Can we learn latent features that effectively establish a ranking of drug efficacy from bright-field microscopy images, specifically features that capture the alterations induced in three-dimensional tumor tissue models by drug application over a period of time?

2. What methodologies and frameworks can be employed to extract and learn these hidden representations efficiently?

3. What could be reasonable metrics, such as L2 loss or cosine similarity, for supporting the relative assessment of drug efficacy?

\chapter{Related works}\label{ch:Related works}
\textbf{Base neural network architecture for representation learning.} Learning visual representations of medical images, such as X-rays (radiographic images) and bright-field microscopy images, is crucial for medical image understanding. However, progress in this area has been hindered by the heterogeneity and complexity of subtle features in these images, especially when they don't have labels. Existing work often relies on fine-tuning weights transferred from ImageNet pretraining (Wang et al., 2017 \cite{8099852} ; Esteva et al., 2017 \cite{Esteva2017Dermatologist} ; Irvin et al., 2019 \cite{irvin2019chexpert} ), which is suboptimal due to the drastically different characteristics of medical images. Recent studies have shown promising results using unsupervised contrastive learning on natural images, but these methods have limited effectiveness on medical images because of their high inter-class similarity.

To address these challenges, researchers have proposed various innovative approaches. ConVIRT \cite{zhang2022contrastive} offers an alternative unsupervised strategy for learning medical visual representations by exploiting naturally occurring paired descriptive text. This method introduces a new approach to pretraining medical image encoders using paired text data via a bidirectional contrastive objective between the two modalities. It is domain-agnostic and requires no additional expert input.  However, given the absence of specific paired text data for our image dataset, ConVIRT does not offer a solution tailored to our specific problem.

The contrastive loss used in ConVIRT is derived from the SimCLR \cite{chen2020simple} self supervised learning framework. SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. The framework consists of a neural network base encoder that extracts representation vectors from augmented data examples. The framework allows for various choices of network architecture without any constraints. The authors opt for simplicity and adopt ResNet, introducing a learnable nonlinear transformation between the representation and the contrastive loss to substantially improve the quality of the learned representations. However, these methods require careful treatment of negative pairs, typically relying on large batch sizes to retrieve them. Additionally, their performance is highly dependent on the choice of image augmentations. BYOL (Bootstrap Your Own Latent) \cite{grill2020bootstrap} addresses these limitations by using an architecture with online and target neural networks, which does not require negative pairs and is more robust to the choice of image augmentations compared to contrastive methods.

While SimCLR has achieved impressive success in the computer vision field, directly applying it to the time series domain often yields poor performance due to its data augmentation and feature extractor not being tailored to the temporal dependencies inherent in time series data. To address this limitation and to obtain high-quality representations of univariate time series, \cite{YANG2022108606} proposed TimeCLR, a framework that combines the strengths of Dynamic Time Warping (DTW) and InceptionTime. Drawing inspiration from the DTW-based k-nearest neighbor classifier, they introduced DTW data augmentation. This technique generates phase shifts and amplitude changes targeted by DTW, preserving the time series structure and feature information. By integrating the advantages of DTW data augmentation and InceptionTime, TimeCLR method extends SimCLR and adapts it effectively to the time series domain.
\cite{liu2024selfsupervised} conducted a comprehensive comparative analysis between contrastive and generative self-supervised learning methods for time series data, focusing specifically on SimCLR and MAE (Masked Autoencoder). They observed that, overall, MAE tends to converge more rapidly and delivers impressive performance, particularly when the fine-tuning dataset is relatively small (around 100 samples). However, in scenarios with larger datasets, SimCLR demonstrates a slight but consistent outperformance over its generative counterparts.

Another recent alternative study for self-supervised visual representation is DINO \cite{caron2021emerging}, which can be also interpreted as a form of self-distillation with no labels. DINO provides new properties to Vision Transformers that stand out compared to convolutional networks.

SupCon \cite{khosla2021supervised} extends the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. The drug applied to tumor samples could be used as labels for the bright-field microscopy images.

\textbf{Coupling engineered features and learned representation.}  
Due to its specificity, fluorescence microscopy has become a quintessential imaging tool in cell biology. However, photobleaching, phototoxicity and related artifacts continue to limit its utility. Recently, it has been shown that artificial intelligence (AI) can transform one form of contrast into another. Mikhail et al.\cite{Kandel_2020} present phase imaging with computational specificity (PICS), a combination of quantitative phase imaging (QPI) and AI, which provides information about unlabeled live cells with high specificity. By applying the computed fluorescence maps back to the QPI data, they measured the growth of both nuclei and cytoplasm independently over many days without loss of viability. This work could provide valuable insights for coupling fluorescent data with learned representations.

\let\cleardoublepage\clearpage
\chapter{Methodology}\label{ch: Methodology}
The goal is to leverage representation learning of bright-field microscopy images to develop a ranking/ordering scale (1 to n) for these images. 

\begin{enumerate}
  \item \textbf{Step 1:} Create a latent space representation of each image using contrastive learning techniques such as SimCLR, masked autoencoder, or any other self-supervised architectures such as DINO that can effectively help learn the efficient features of alterations induced in three-dimensional tumor tissue models by the impact of drug application over a period of time.
  
  \item \textbf{Step 2:} Train a time series prediction model exclusively on the representations of untreated images from Day 7 to Day 10 to predict the representation of the Day 10 image.
  
  \item \textbf{Step 3:} Perform inference on the representations of test images, which include untreated, clinically recommended, and drug screening images.
\end{enumerate}

Since the time series model is trained solely on the representations of untreated images, the inference loss/metric (i.e., the difference between the predicted and actual Day 10 image representations) will be very small for untreated images. Conversely, the inference loss/metric will increase for treated images as their representations deviate from those of untreated images. This inference loss/metric will be used as the feature for the ranking/order scale, where the initial images will start with untreated images that have very small inference loss/metric, and the scale will end with images having high inference loss/metric in ascending order. Determining a reasonable inference loss/metric will be one of the research problems to tackle.

\chapter{Experiments}\label{ch:Experiments}
\section{Data set}
\label{sec:Data set}

The original images are approximately 2500×2500 pixels in size, in 16-bit grayscale, and consist of multiple channels. These channels come from taking images at different focal planes in brightfield microscopy. The number of channels can vary, as you can take images at any number of focal planes. However, for time efficiency, the current data we have collected contains 3 channels per image.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/threes.png} 
  \caption{Illustration of three layers per image: A, B, and C. The three layers looks visually similar, with slight differences in focal planes. In this figure, A is the sharpest/focused layer.}
  \label{fig:Threes}
\end{figure}

Figure \ref{fig:Transition} illustrates that, even with the application of the same drug at the same concentration, the morphology of 3D tumor tissues changes differently.
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.46]{figures/transition.png} 
  \caption{C06, F11 and G04 are well names in the well plate.}
  \label{fig:Transition}
\end{figure}



The table below shows the division of three different types of image datasets, as  explained in the section ~\ref{sec:lab-setup}.
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
  \hline
  \textbf{Class} & \textbf{Drug Screened} & \textbf{Single Dose} & \textbf{Untreated} & \textbf{Total} \\ \hline
  \textbf{No. of Images (\%)}  & 12 (3\%) & 204 (60\%) & 150 (37\%) & 366 \\ \hline
  \end{tabular}
  \caption{Dataset Class Overview}
  \label{tab:dataset}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.6]{figures/originals.png} 
  \caption{Three different types of images: Drug Screened, Single Dose, and Untreated as mentioned in section ~\ref{sec:lab-setup}.}
  \label{fig:originals}
\end{figure}

An 8-bit image encompasses 256 color tones (ranging from 0 to 255) per channel, whereas a 16-bit image accommodates 65,536 color tones (ranging from 0 to 65,535) per channel, in our case 65,536 shades of gray. Retaining the original 16-bit depth is crucial for two primary reasons:

\begin{enumerate}
  \item Converting it to an 8-bit image for faster and more efficient computation can lead to significant information loss in intensity details. Since 8-bit images only allow 256 possible values, the finer variations in intensity that are present in 16-bit images become compressed. For example, two distinct values in 16-bit (such as 30,000 and 30,001) could map to the same 8-bit value (for instance, both might be mapped to 117). This results in the loss of subtle intensity differences. 

  \item During data augmentation processes that involve substantial alterations in brightness, contrast, or color, an 8-bit image—already limited to 256 tones—could lose up to 50 percentage of these tones, leaving only 128 levels of color and tone. This reduction can lead to "banding," where areas with smooth transitions in tone exhibit visible stripes with jagged edges. In contrast, a 16-bit image, even with a 50 percentage reduction in tones, would retain over 32,000 levels. This higher tonal range allows for smoother transitions, better edge preservation, and enhanced accuracy in color and hue representation. As a result, the dynamic range—the difference between the lightest and darkest areas of the image—remains much more effectively preserved in 16-bit images than in 8-bit images.
\end{enumerate}
In our case, the maximum reduction in unique pixel values for the 8-bit images, regardless number of channels was found to be 99.27 percentage after 3000 epochs of random
color jitter applied  using \texttt{torch.transforms.RandomApply([transform.ColorJitter(brightness=1, contrast=1, saturation=1, hue=0)], p=1)} as shown in figure \ref{fig:8bit_nThree} and \ref{fig:8bit_n one}, whereas for the 16-bit single-channel images (where one sharp layer was extracted from all three layers and considered as input for data augmentation), the reduction in unique pixel values was only 49 percentage after 3,000 epochs of random color jitter, as shown in Figure \ref{fig:16bit_n one}.

Interestingly, for 16-bit images with 3 channels, instead of a reduction, there was an increase in the number of unique pixel values—by a maximum of 258,757 percentage. The issue with this increase is that after data augmentation, the new pixel values are not distributed similarly to the original image. Instead, they shift to the two extremes, such as 0 or 1, or sometimes pushing values to both 0 and 1, which deviate significantly from the original image distribution, as shown in Figures \ref{fig:16bit_three_version1} and \ref{fig:16bit_three_version2}.

\textbf{8-bit three-channel image example before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 2
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nThree.png} 
  \caption{8-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 97.81\%}
  \label{fig:8bit_nThree}
\end{figure}

\textbf{8-bit single-channel (sharp layer) image before and after data augmentation:}

\begin{itemize}
  \item Number of unique pixel values in the original image: 137
  \item Number of unique pixel values in the augmented image: 3
  \item Original Image - Minimum pixel value: 33, Maximum pixel value: 170
  \item Augmented Image - Minimum pixel value: 3, Maximum pixel value: 3
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/8bit_nOne.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 99.27\%}
  \label{fig:8bit_n one}
\end{figure}

\textbf{16-bit three-channel image before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 2111
  \item Number of unique pixel values in the augmented image: 5044624
  \item Original Image - Minimum pixel value: 0.13064774870872498, Maximum pixel value: 0.6874189376831055
  \item Augmented Image - Minimum pixel value: 0.022128667682409286, Maximum pixel value: 0.11041323840618134
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bit_three_1.png} 
  \caption{sixteen bit three layer after 3000 epoch random torch color jitterness apply}
  \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Increase in percentage of unique pixel values: 2388\%}
  \label{fig:16bit_three_version1}
\end{figure}

\textbf{Another example of a 16-bit three-channel image before and after data augmentation:}

\begin{itemize}
  \item Original Image - Unique pixel counts per channel: 2137
  \item Augmented Image - Unique pixel counts per channel: 1686717
  \item Original Image - Minimum pixel value: 0.1306, Maximum pixel value: 0.6874
  \item Augmented Image -  Minimum pixel value: 0.1970, Maximum pixel value: 0.3748
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bithree2.png} 
  \caption{16-bit three-channel image after 3000 epochs of random color jitter applied using PyTorch. Increase in percentage of unique pixel values: 78\%}
  \label{fig:16bit_three_version2}
\end{figure}

\textbf{16-bit single-channel image before and after data augmentation:}
\begin{itemize}
  \item Number of unique pixel values in the original image: 2111
  \item Number of unique pixel values in the augmented image: 1058
  \item Original Image - Minimum pixel value:  0.13064774870872498, Maximum pixel value: 0.6666666865348816
  \item Augmented Image - Minimum pixel value: 0, Maximum pixel value: 1
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{figures/16bit_onen.png} 
  \caption{8-bit single-channel image after 3000 epochs of random color jitter applied using PyTorch. Reduction percentage in unique pixel values: 49.88\%}
  \label{fig:16bit_n one}
\end{figure}

$49\%$ maximum reduction for 16-bit single-channel data augmentation with color jitter is also not ideal, as it diminishes the gradual spread of darker regions as happened in original image, as  as observed in figure \ref{fig:16bit_n one}. One potential solution is to experiment with specific parameters within the color jitter transform instead of using random values, ensuring that the reduction in the number of unique pixel values does not exceed, for example, $30\%$. Another option would be to write a custom Python function, depending on the available time. Other augmentations from PyTorch work fine in this experiment.

\section{Training SSL model}

For step 1, as explained in chapter \ref{ch: Methodology}, SimCLR was used as the first model for self-supervised learning (SSL). Later, other models such as masked autoencoders and DINO will be explored, depending on the available time.
Why we would like to try other models? Because SimCLR demands larger batch size and more data for better performance which we don't have.



\subsection{Using SimCLR as SSL model}
\subsubsection{Data preprocessing}
\label{subsec:data preprocessing}

Detailed study/research/experiments on data augmentation and image preprocessing techniques sepcifically for our 16 bit gray scale image are still need to be done.
Currently, as the focus is on creating the complete pipeline, the standard data augmentation combination (which showed high performance for SimCLR downstream tasks) from the SimCLR \cite{chen2020simple} paper is being used, as shown below.

\begin{enumerate}
  \item Normalize the 16-bit image to [0, 1] for the following reasons:
  \begin{enumerate}
      \item Ideally, normalization should be done at the end after augmentations to ensure scaled input to the neural network, but in our case, we have to normalize first since the augmentation with \texttt{torch.transform.ColorJitter} didn't work without scaled data.
      \item \texttt{torch.transform.ToTensor()} didn't scale the data points to the [0, 1] range.
  \end{enumerate}
  
  \item Perform the following augmentations:
  \begin{enumerate}
      \item Apply a horizontal flip.
      \item Randomly crop the image and resize it to $96 \times 96$.
      \item Randomly change the brightness, contrast, saturation, and hue of the cropped patch.
  \end{enumerate}

  \item Perform Z-score normalization after data augmentation for the following reasons:
  \begin{enumerate}
      \item Pretrained models require this preprocessing.
      \item It ensures that the data is still normalized even after data augmentation tweaks, allowing for effective feeding into the neural network.
  \end{enumerate}

  \item For each original image, repeat step 2 twice to obtain two augmented images.
\end{enumerate}


Visualisation of before and after preprocessing of image shown in figures 6.5 to 6.11.

\subsubsection{Model}
The Resnet18 \cite{he2015deepresiduallearningimage} model processes a single image to produce a latent representation of the input, aiming to cluster similar images together in a latent space. 

\subsubsection{Training }
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $N$.
    
    \item Our dataset class returns two augmented versions for each original image as explained in section \ref{subsec:data preprocessing} in the batch, resulting in $2N$ images as input.

    \item The model produces $2N$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative pairs.

    \item We calculate the cosine similarities between the positive and negative pairs. These cosine similarities are then used as input to the loss function described below equation \ref{eq:loss}
\end{enumerate}
The original loss function for each pair from SimCLR paper \cite{chen2020simple} is defined as:

\begin{equation}
\ell_{i, j} = -\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right)}
\label{eq:original}
\end{equation}
  
which we can reformualte as:

1. Apply the logarithm: The negative log of a fraction can be separated into the difference of the logarithms:
\[
\ell_{i, j} = -\left( \log \left(\exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) - \log\left( \sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_k\right) / \tau\right) \right) \right)
\]

2. Simplifying the first term: The logarithm of an exponential function simplifies as follows:
\[
-\log\left(\exp\left(\operatorname{sim}\left(\boldsymbol{z}_i, \boldsymbol{z}_j\right) / \tau\right)\right) = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau}
\]

Substituting that back into the equation:
\[
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} - \log\left(\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right)
\]

This gives us:

\begin{equation}
\ell_{i, j} = -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log\left[\sum_{k=1}^{2 N} \mathbf{1}_{[k \neq i]} \exp\left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
\label{eq:loss}
\end{equation}

where \(\mathbf{1}_{[k \neq i]} \in \{0, 1\}\) is an indicator function evaluating to 1 iff \(k \neq i\), and \(\tau\) denotes a temperature parameter. The final loss is computed across all positive pairs, both \((i, j)\) and \((j, i)\), in a mini-batch with $z_i, z_k$ representing negative pairs.

Equation \ref{eq:loss} implemented as the loss function in our experiments.

The above standard SimCLR loss function and data augmentation combination  with the ResNet18 model will be used as the initial benchmark for experiments, and in the future, the following variations will be explored.

\textbf{Variation ideas:}

\begin{enumerate}
  \item Explore different data augmentation combinations by researching the best augmentations for medical grayscale images, and applying intuitive approaches beyond the standard SimCLR \cite{chen2020simple} data augmentation combinations as explained in section \ref{subsec:data preprocessing}.
  \item Each image is treated as an RGB image with 3 channels, and two of the best-performing data augmentations, which yielded high performance for our downstream task.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.
  \item One channel is considered as the anchor (the most sharpened layer), and two of the best-performing data augmentations, which yielded high performance for the our downstream task.

  \item Apply more than two standard augmentations to meet the large batch size requirement of SimCLR \cite{chen2020simple}.

  \item Remove the positive sample \( j \) from the denominator of the loss function. Since \( j \) is the only image as a positive sample in the sum of the denominator softmax, its contribution will be less.

  \item Supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.
  \item Since SimCLR architecture \cite{chen2020simple} allows for flexibility in model selection, and explore other pretrained models  than Resnet18  suitable for medical grayscale images. For example pretrained U-Net \cite{ronneberger2015unetconvolutionalnetworksbiomedical} model for MRI brain images from PyTorch.  
  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations). This resembles to triplet loss (not sure, need to be studied)  
\end{enumerate}
For variations 5 and 9 we need to modify the loss function since it includes more than 2 augmentations.

\textbf{Variations implementations:}  \label{sec:variations_implementations}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H,W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{subsec:data preprocessing}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/3_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed original image) with 3 channels and its augmentations}
  \label{fig:augmentation}
\end{figure}


  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3_2fine.png} % Adjust the width as needed
    \caption{Sample 2: Anchor (the preprocessed original image) with 3 channels and its augmentations}
    \label{fig:augmentations}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512, 1, 1]) (Batch size, standard resnet18 output dimension after avg pooling, H, W)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

The figure below shows the anchor and its two augmented versions as explained in section \ref{subsec:data preprocessing}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_1.png} % Adjust the width as needed
  \caption{Sample 1: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1_2.png} % Adjust the width as needed
  \caption{Sample 2: Anchor (the preprocessed sharpest layer amoung all 3 layers) with one channel and its augmentations}
  \label{fig:1doutput3}
\end{figure}

\section{Intermediate evaluation of SSL model}
**suppose I have a vector of image. if I normalise it unit length, willl I loose critical information?**

**https://chatgpt.com/share/671a6a63-ca7c-8010-92a0-23b6bd25ef05**

**if we do kmeans withcosine distance distance then it only compare cosine sim of whole image thats why we need to do normal kmeans with euclidean dist to show the magnitude similarity** 

**thats why we need to do instance segmantation and classifiacation to see if it able to learn more than magnitude similarity and cosine similarity like as texture, contrast, and brightness**

Evaluation of the SSL model depends on the  time series inference loss/accuracy metric, neverthless we can use other evaluation metrics, such as downstream task like classification. 

1. A common approach to verify whether the SSL model has learned generalized representations is to perform Logistic Regression on the learned features. In other words, we use a single, linear layer that maps these representations to class predictions, where the two categories, 'untreated' and 'single dose,' serve as our classes. The Logistic Regression model can only perform well if the learned representations capture all the relevant features necessary for the task. Moreover, we don't need to worry much about overfitting since only a few parameters are trained. Therefore, we expect the model to perform well even with limited data. We implemented a simple pipeline for a Logistic Regression setup, where the images are encoded into their feature vectors.

2. Baseline comparison: As a baseline for comparison  to our results above in the section \ref{sec:variations_implementations}, we will train a standard ResNet-18 with random initialization on the labeled training set, consisting of the 'untreated' and 'single dose' categories. The results will help us assess the advantages of contrastive learning on unlabeled data compared to purely supervised training. It is evident that ResNet-18 easily overfits the training data since its parameter count is over 1,000 times larger than the dataset size. To ensure a fair comparison with the contrastive learning models, we apply similar data augmentations as before, including crop-and-resize and color jittering.


\chapter{Furture works}\label{ch:Furture works}
\section{Time series prediction model}
\section{Integrated/ensembled model: SSL+Time series model}
\chapter{Conclusion}\label{ch:Conclusion}

These methodological steps collectively form the framework for addressing critical research questions and challenges throughout the course of this thesis. By exploring the dataset, assessing models, and developing custom architecture, we aim to to assess drug efficacy by ranking different drug combinations and concentrations.

\let\cleardoublepage\clearpage

\chapter{Proposed timeline}\label{ch:Proposed timeline}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/Thesis timeline.png} % Adjust the width as needed
    \caption{Proposed Master Thesis timeline}
    \label{fig:enter-label}
\end{figure}

\FloatBarrier

\printbibliography

\vspace{20pt}
\begin{flushright}
$\overline{~~~~~~~~~~~~~~~~~\mbox{\ShowBaAuthor, \SubmitDate}~~~~~~~~~~~~~~~~~}$
\end{flushright}
\end{document}
experimental scheme 1:
 On day 7, the drug treatment is applied to half of these, while the other half is left untreated to used as benchmark. This setup enables us to conduct a comparative study between the untreated 3D tumor tissue models and those treated with the specific drug combination to assess it's efficacy. However, live 3D tumor tissue models tend to deteriorate after a few days due to nutrient depletion in the well plate medium. 

 experimental scheme 2:
 drug screening not half  just look at dalia image