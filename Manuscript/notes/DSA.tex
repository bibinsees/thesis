\documentclass[a4paper,12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}

\title{DSA}
\author{Bibin Babu}

\begin{document}

\maketitle

\chapter{Experiments}\label{ch:Experiments}

Below figure shows the flow chart of whole time evolution of cancer cell.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.9]{figures/time.pdf} 
  \caption{time progress}
  \label{fig:time}
\end{figure}

The current dataset we are working with only includes data from day 10 (ie after 3 days of drug application). 

\vspace{1em} % Adds vertical space (you can adjust the amount)

\subsubsection{Dataset class overview}
\begin{table}[ht!]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
  \hline
  \textbf{Dataset Type} & \textbf{Drug Screened} & \textbf{Single Dose} & \textbf{Untreated} & \textbf{Total} \\ \hline
  \textbf{Combined Unlabeled} & 12 (3\%) & 204 (60\%) & 150 (37\%) & 366 \\ \hline
  \textbf{Supervised}         & 12 (16.66\%) & 30 (41.66\%)  & 30 (41.66\%) & 72  \\ \hline
  \end{tabular}
  \caption{Dataset Overview}
  \label{tab:dataset}
  \end{table}

The original images \ref{fig:originals} are approximately 2500×2500 pixels in size, in 16-bit grayscale, and consist of multiple channels. These channels come from taking images at different focal planes in brightfield microscopy. The number of channels can vary, as you can take images at any number of focal planes. However, for time efficiency, the current data we have collected contains 3 channels per image.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.8]{figures/originals.pdf} 
  \caption{originals}
  \label{fig:originals}
\end{figure}

It’s important to keep the original color depth, aka bit depth. An 8-bit image contains 256 color tones (0-255) per channel, while a 16-bit image contains 65,536 color tones (0-65,535) per channel—in our case, 65,536 shades of gray.

When data augmentation involves significant changes in brightness, contrast, or color, if we have an 8-bit image and lose, let’s say, 50 percentage of the tones, we’re left with only 128 levels of color and tone. This loss of information will typically manifest as banding, especially in areas of smooth color transitions. This is where we see visible stripes of color with slightly jagged edges forming across the image.

In contrast, a 16-bit image contains 65,536 levels of color and tone. If we lose 50 percentage of those colors and tones, we’d still have over 32,000 levels remaining. This means we’d retain smoother color transitions, preserve edge details, and maintain color and hue accuracy much better. The dynamic range (the difference between the lightest and darkest parts of the image) would be preserved much longer compared to a typical 8-bit image.



\subsubsection{Data preprocessing}

Detailed study/research/experiments on data augmentation and image preprocessing techniques sepcifically for our 16 bit gray scale image are still need to be done.
At the moment since we are trying to create the full pipe line first we used standard data augmentations from SimCLR paper.

\begin{enumerate}
  \item Normalize the 16-bit image to [0, 1].
  \item Do the following augmentations:
  \begin{enumerate}
      \item Crop the image randomly, and resize it to $96 \times 96$.
      \item Randomly change the brightness, contrast, saturation, and hue of the cropped patch.
  \end{enumerate}
  \item For each original image, perform step 2 twice to obtain two augmented images.
\end{enumerate}


\subsubsection{Model}
The model takes one image and produce a latent representation of the input. The aim of the model is to cluster similar images together in latent space.
\subsubsection{Training}
The training process follows these steps:

\begin{enumerate}
    \item We take a batch of images with batch size $B$.
    
    \item Our dataset class returns two augmented versions for each original image in the batch, resulting in $2B$ images as input.

    \item The model produces $2B$ latent representations, independently for each augmented image.

    \item For each batch, the two augmentations of the same image are treated as positive pairs, while all others are considered negative samples.

    \item We calculate the cosine similarities between the positive and negative pairs. Positive pairs consist of augmentations of the same image, 
    while negative pairs are augmentations of different images. These cosine similarities are then used as input to the loss function described below equation \ref{loss}
\end{enumerate}




\begin{equation}
  -\frac{\operatorname{sim}\left(z_{i}, z_{j}\right)}{\tau} + \log \left[\sum_{k=1}^{2 N} \mathbb{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)\right]
  \label{loss}
\end{equation}
  
where $z_i, z_j$ are positive pairs and $z_i, z_k$ are negative pairs.

\vspace{1em}

Visualisation of before and after preprocessing of image shown in figures \ref{fig:out_2} to \ref{fig:1doutput3}


\subsubsection{Variation ideas}

\begin{enumerate}
  \item Each image is considered as RGB since we have 3 channels, and we apply 2 standard augmentations.

  \item One channel is considered as the anchor (the most sharpened layer), and the others are treated as the two augmentations.

  \item Each image is considered as RGB since we have 3 channels, and we apply more than 2 standard augmentations (research for medical grayscale images).

  \item Include the anchor as a positive sample, i.e., 3 augmentations in total (1 anchor as augmentation and the other 2 layers as augmentations).

  \item Remove the positive sample \( j \) from the denominator of the loss function. Since \( j \) is the only image as a positive sample in the sum of the denominator softmax, its contribution will be less.

  \item For supervised SimCLR: Ensure that no images from the same breed/class are included in the negative samples.

  \item Try another loss function, such as Triplet loss, as shown in the original paper.
\end{enumerate}


\subsubsection{Variations implementations:}

The two variations tried so far differ only in how they handle the image for data augmentation. 

In the first variation, we take a 3-channel image and treat it like a standard RGB image, applying SimCLR-style augmentations to create two augmented versions.

In the second variation, we take a 3-channel image and compute the sharpness of each layer by calculating the magnitude of the gradient of pixel intensities in the x and y directions, which indicates edge strength and provides a measure of how sharp the transitions between pixel values are. The sharpest layer is used as the anchor, while the other two layers are treated as augmentations. 

\subsubsection{Variation 1:}
\textbf{Input to model (train loader dimension) :} 

\begin{itemize}
  \item aug1: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W)
  \item aug2: torch.Size([16, 3, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize} \vspace{1em}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512]) (Batch size, standard resnet18 output dimension after avg pooling)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}
Below images after data augmentation and normalisation of 3 channel version:

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/op1.png} % Adjust the width as needed
    \caption{op1}
    \label{fig:out_2}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/output3.png} % Adjust the width as needed
    \caption{op3}
    \label{fig:output3}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/op4.png} % Adjust the width as needed
    \caption{Blured augs}
    \label{fig:output4}
  \end{figure}
\subsubsection{Variation 2:}

\textbf{Input to model (train loader dimensions) :} 
\begin{itemize}
   \item aug1: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W)
   \item aug2: torch.Size([16, 1, 96, 96])        (batch size, no of channels, H, W) \vspace{1em}
\end{itemize}
\textbf{Model output just after convolution layers: (before applying projetion head)} 
\begin{itemize}
  \item torch.Size([16, 512]) (Batch size, standard resnet18 output dimension after avg pooling)   
  \item This output feature will be used for further downstream task.  \vspace{1em}
\end{itemize}

\textbf{Model output after projection head:}
\begin{itemize}
  \item torch.Size([16, 20])  (Batch size, no of values in feature vector)  
  \item No of values in feature vector is a variable which we can change and experiment which will give better accuracy.
\end{itemize}

Below images after data augmentation and normalisation of 1 channel version:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1dop1.png} % Adjust the width as needed
  \caption{1dop1}
  \label{fig:1doutput1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1dop2.png} % Adjust the width as needed
  \caption{1dop2}
  \label{fig:1doutput2}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/1dop3.png} % Adjust the width as needed
  \caption{1dop3}
  \label{fig:1doutput3}
\end{figure}
\end{document}